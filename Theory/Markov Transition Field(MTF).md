# Markov Transition Field(MTF)

Markov Transition Field(MTF)는 시계열(Time Series) 데이터를 이미지 데이터로 변환하여, 컴퓨터 비전 기술(CNN 등)을 시계열 분석에 활용할 수 있게 돕는 알고리즘입니다.

## 1. 핵심 개념
MTF의 핵심은 시계열 데이터의 '상태 변화(전이 확률)' 정보를 2차원 행렬(이미지)로 시각화하는 것입니다. 
- 이산화 (Quantization): 시계열 데이터의 값 범위(Min-Max)를 $\(Q\)$ 개의 구간(Bin)으로 나눕니다. 각 데이터 포인트는 해당되는 구간인 '상태(State)'에 할당됩니다.
- 마르코프 전이 행렬 (Markov Transition Matrix, W): 각 상태에서 다른 상태로 이동할 확률을 계산하여 $\(Q\times Q\)$ 크기의 행렬을 만듭니다. $\(w_{ij}\)$ 는 '상태 $\(i\)$ 다음에 상태 $\(j\)$ 가 올 확률'을 의미합니다.
- 필드 생성 (Field Generation): 시계열의 시간 흐름을 보존하기 위해, 각 시간 $\(t\)$ 에서의 상태 전이 확률을 $\(n\times n\)$ 크기의 행렬(여기서 $\(n\)$ 은 시계열 길이)에 배치합니다.

## 2. 작동 원리 (수식적 관점) 
MTF의 핵심은 "일차원 시계열의 동역학(Dynamics)을 2차원 조건부 확률 행렬로 인코딩하는 것"에 있습니다.

### 1. 상태 공간의 정의 (Binning) 
주어진 시계열 데이터 $\(X=\{x_{1},x_{2},...,x_{n}\}\)$ 에 대해, 데이터의 최솟값과 최댓값을 기준으로 $\(Q\)$ 개의 구간(Bin)을 나눕니다.  
각 구간을 상태(State) $\(q_{j}(j\in \{1,...,Q\})\)$ 라고 정의합니다.  
각 시점의 데이터 $\(x_{t}\)$ 는 해당 값의 크기에 따라 특정 상태 $\(q\)$ 에 할당됩니다.

### 2. 마르코프 전이 행렬 (Markov Transition Matrix) 구성 
시계열의 1차 마르코프 특성(현재 상태는 바로 직전 상태에만 의존함)을 가정하여, 상태 간의 전이 확률을 계산합니다.  
$\(Q\times Q\)$ 크기의 전이 행렬 $\(W\)$ 를 다음과 같이 정의합니다.

$$\(W_{ij}=P(x_{t}\in q_{j}|x_{t-1}\in q_{i})\)$$ 

- $\(W_{ij}\)$ : 상태 $\(q_{i}\)$ 다음에 상태 $\(q_{j}\)$ 가 나타날 조건부 확률입니다.
- 실제 계산 시에는 상태 $\(q_{i}\)$ 에서 $\(q_{j}\)$ 로 이동한 횟수를 카운트한 후, 각 행의 합이 1이 되도록 정규화(Normalization)합니다.

<details>

### 1. 데이터 준비 (이산화) 
예를 들어, 6개의 시계열 데이터가 있고 이를 2개의 상태(Bin)로 나눴다고 가정합시다. 
- 시계열: $\(X=\{1,2,1,1,2,2\}\)$
- 상태 정의: $\(1=q_{1}\)$ (낮음), $\(2=q_{2}\)$ (높음)
- 총 상태 수 $(\(Q\))$ : 2

### 2. 전이 횟수 카운트 (Adjacency Matrix 생성) 
시간 순서대로 이동 경로를 추적하여 횟수를 세어봅니다. $(\(t\rightarrow t+1\))$ 
- $\(1\rightarrow 2\)$ (상태 $\(q_{1}\rightarrow q_{2}\)$ ) : 1회
- $\(2\rightarrow 1\)$ (상태 $\(q_{2}\rightarrow q_{1}\)$ ) : 1회
- $\(1\rightarrow 1\)$ (상태 $\(q_{1}\rightarrow q_{1}\)$ ) : 1회
- $\(1\rightarrow 2\)$ (상태 $\(q_{1}\rightarrow q_{2}\)$ ) : 1회
- $\(2\rightarrow 2\)$ (상태 $\(q_{2}\rightarrow q_{2}\)$ ) : 1회 

카운트 결과 표: 
- $\(q_{1}\)$ 에서 시작한 경우: $\(q_{1}\)$ 으로 1번, $\(q_{2}\)$ 로 2번 이동함.
- $\(q_{2}\)$ 에서 시작한 경우: $\(q_{1}\)$ 으로 1번, $\(q_{2}\)$로 1번 이동함.

이를 행렬로 나타내면:

$`\text{Count\ Matrix}=\left(\begin{matrix}1&2\\ 1&1\end{matrix}\right)`$

### 3. 행 정규화 (Row Normalization) 
이제 각 행의 합이 1이 되도록 만듭니다.  
이는 "특정 상태 $\(i\)$ 에 있을 때, 다음에 어떤 상태가 올 확률의 총합은 100%다"라는 확률의 기본 원리를 따르기 위함입니다. 
- 1행 합 (Row 1 sum): $\(1+2=3\)$
- 2행 합 (Row 2 sum): $\(1+1=2\)$

각 원소를 해당 행의 합으로 나눕니다. 

- $\(W_{11}=1/3=0.33\)$
- $\(W_{12}=2/3=0.67\)$
- $\(W_{21}=1/2=0.5\)$
- $\(W_{22}=1/2=0.5\)$

최종 마르코프 전이 행렬 $\(W\)$ :

$`W=\left(\begin{matrix}0.33&0.67\\ 0.5&0.5\end{matrix}\right)`$

### 4. 왜 이렇게 계산하나요? (수학적 의미) 
이 계산식은 통계학의 최대 우도 추정법(Maximum Likelihood Estimation, MLE)에 기반합니다.

마르코프 전이 필드(MTF)에서 전이 확률 $\(W_{ij}\)$ 를 '횟수 기반의 비율'로 계산하는 것은 통계학의 최대 우도 추정법(MLE)에 따른 수학적 최적해를 구하는 과정입니다.

$\(W_{ij}=\frac{n_{ij}}{\sum_{k=1}^{Q}n_{ik}}\)$ 

- $\(n_{ij}\)$ : 상태 $\(i\)$ 에서 $\(j\)$ 로 이동한 총 횟수
- 분모: 상태 $\(i\)$에서 출발한 모든 이동의 총합

이렇게 얻어진 $\(W\)$ 행렬의 값들을 시계열의 원래 시간 순서 $(\(t,s\))$ 에 맞춰 2차원 평면에 뿌려주면(Mapping), 그것이 바로 MTF(Markov Transition Field) 이미지가 됩니다.

파이썬 시계열 라이브러리인 pyts의 가이드에서 확인하실 수 있습니다.

#### 왜 MLE 와 관련되어 있을까요?

##### 1. 우도 함수(Likelihood Function)의 설정 
우리는 관측된 시계열 데이터(상태의 나열)가 발생할 확률을 최대화하는 전이 확률 값을 찾고자 합니다.  
시계열에서 상태 $\(q_{i}\)$ 에서 $\(q_{j}\)$ 로 이동한 횟수를 $\(n_{ij}\)$ 라고 할 때, 전체 시계열이 나타날 확률(우도) $\(L\)$ 은 다음과 같이 정의됩니다.

$$\(L(W)=\prod_{i=1}^{Q}\prod_{j=1}^{Q}W_{ij}^{n_{ij}}\)$$

여기서 $\(W_{ij}\)$는 우리가 추정해야 할 매개변수(Parameter)입니다. 

##### 2. 제약 조건 하의 최적화 
전이 확률은 확률의 정의에 따라 각 행의 합이 1이 되어야 한다는 제약 조건이 있습니다.

$$\(\sum_{j=1}^{Q}W_{ij}=1\)$$

이 제약 조건을 만족하면서 우도 함수 $\(L\)$ 을 최대화하는 $\(W_{ij}\)$ 를 찾기 위해 라그랑주 승수법(Lagrange Multipliers)을 사용합니다.  
로그 우도(Log-likelihood)를 취하여 계산하면, 결과적으로 다음과 같은 목적 함수를 얻습니다.

$$\(J=\sum_{i,j}n_{ij}\ln (W_{ij})-\sum_{i}\lambda_{i}(\sum_{j}W_{ij}-1)\)$$

##### 3. MLE를 통한 최적의 $\(W_{ij}\)$ 도출 
위 식을 $\(W_{ij}\)$ 에 대해 편미분하여 0이 되는 지점을 찾으면, 우리가 익히 아는 비율 계산식이 도출됩니다.

$$\(\frac{\partial J}{\partial W_{ij}}=\frac{n_{ij}}{W_{ij}}-\lambda_{i}=0\implies W_{ij}=\frac{n_{ij}}{\lambda_{i}}\)$$

여기서 $\(\lambda_{i}\)$ 는 정규화 상수가 되어, 결국 상태 $\(q_{i}\)$ 에서 출발한 모든 전이 횟수의 합 $(\(\sum_{k}n_{ik}\))$ 이 됩니다.  

따라서 최종 식은 다음과 같습니다.

$$\(W_{ij}^{MLE}=\frac{n_{ij}}{\sum_{k=1}^{Q}n_{ik}}\)$$

##### 왜 MLE인가? 
- 통계적 정당성: 단순히 횟수를 세어 나누는 방식은 "현재 데이터에서 이 패턴이 나타날 확률이 가장 높다"라고 보장하는 가장 객관적인 추정 방식입니다.
- 패턴의 특징화: MLE를 통해 계산된 $\(W_{ij}\)$ 는 시계열의 동역학적 특성을 가장 잘 대변하는 수치가 되며, 이를 이미지화한 MTF는 해당 시계열의 고유한 '통계적 지문' 역할을 수행하게 됩니다.

###### 왜 여기서 $\(\lambda _{i}\)$ 는 정규화 상수가 되나요?
이전 단계에서 목적 함수를 $\(W_{ij}\)$ 로 편미분하여 얻은 식은 다음과 같았습니다.

$$\(\frac{n_{ij}}{W_{ij}}-\lambda_{i}=0\)$$

이를 $\(n_{ij}\)$ 에 대해 정리하면: $\(n_{ij}=\lambda_{i}\cdot W_{ij}\)$

우리가 알고 있는 제약 조건은 "특정 상태 $\(i\)$ 에서 출발하는 모든 확률의 합은 1이다"라는 것입니다. : $\(\sum_{j=1}^{Q}W_{ij}=1\)$

이제 위 1번 식의 양변에 모든 $\(j\)$ 에 대한 합 $(\(\sum_{j=1}^{Q}\))$ 을 취해봅니다. : $\(\sum_{j=1}^{Q}n_{ij}=\sum_{j=1}^{Q}(\lambda_{i}\cdot W_{ij})\)$

이때, 괄호 안의 값 $(\(\sum W_{ij}\))$ 은 제약 조건에 의해 1이 됩니다. 따라서: $\(\sum_{j=1}^{Q}n_{ij}=\lambda_{i}\cdot 1\)$

$$\(\lambda_{i}=\sum_{j=1}^{Q}n_{ij}\)$$

결과적으로 $\(\lambda_{i}\)$ 는 상태 $\(q_{i}\)$ 에서 출발한 모든 전이 횟수의 총합과 같아집니다.  
다시 원래 식 $\(W_{ij}=\frac{n_{ij}}{\lambda_{i}}\)$ 에 이 값을 대입하면:

$$\(W_{ij}=\frac{n_{ij}}{\sum_{k=1}^{Q}n_{ik}}\)$$

이 식은 (특정 이동 횟수) / (전체 이동 횟수)를 의미하므로, 전체 합을 1로 만드는 정규화(Normalization) 역할을 수행하게 되는 것입니다.  
요약하자면: 라그랑주 승수 $\(\lambda _{i}\)$ 는 수학적으로 "확률의 합이 1이 되어야 한다"는 물리적 제약을 수치화한 것이며, 계산 결과 그 상태에서 발생한 모든 사건의 총 횟수가 되어 자연스럽게 분모(정규화 상수) 자리에 위치하게 됩니다.

### 3. 필드(Field)로의 확장 (Mapping) 
전이 행렬 $\(W\)$ 는 상태 간의 관계(어떤 값이 나온 후 어떤 값이 나올지)만 나타낼 뿐, 시간적 순서(Temporal Information)를 직접적으로 표현하지 못합니다.  
이를 해결하기 위해 $\(n\times n\)$ 크기의 MTF 행렬 $\(M\)$ 을 생성합니다.  
시점 $\(i\)$의 데이터가 상태 $\(q_{\alpha }\)$ 에 속하고, 시점 $\(j\)$ 의 데이터가 상태 $\(q_{\beta }\)$ 에 속할 때, 행렬 $\(M\)$ 의 원소는 다음과 같이 배치됩니다.

$$\(M_{ij}=W_{\alpha \beta }\)$$ 

- 의미: $\(M_{ij}\)$ 는 시점 $\(i\)$ 에서 시점 $\(j\)$ 로 이동할 때의 전이 확률을 의미합니다.
- 행렬의 대각선 성분 $\(M_{ii}\)$ 는 각 시점의 데이터가 속한 상태의 자기 전이 확률(Self-transition probability)을 나타냅니다.

#### 어떻게 계산되는지?

마르코프 전이 필드(MTF)의 가장 독특한 점은 $\(Q\times Q\)$ 크기의 전이 행렬 $\(W\)$ 를 만든 후, 이를 다시 시계열의 길이 $\(n\)$ 에 맞춰 $\(n\times n\)$ 크기로 확장한다는 것입니다.

결론부터 말씀드리면, $\(M_{ij}\)$ 는 직접 계산하는 값이 아니라 전이 행렬 $\(W\)$ 에서 해당하는 값을 찾아와서 배치(Mapping)하는 값입니다.

#### 1. 상태 할당 (Mapping to States) 
먼저, 각 시점 $(\(t=1,2,...,n\))$ 의 데이터가 어느 구간(Bin)에 속하는지 확인합니다.  
시점 $\(i\)$ 의 데이터 $\(x_{i}\)$ 가 속한 상태를 $\(q_{\alpha }\)$ 라고 합니다.  
시점 $\(j\)$의 데이터 $\(x_{j}\)$ 가 속한 상태를 $\(q_{\beta }\)$ 라고 합니다. 

#### 2. $\(M_{ij}\)$ 의 값 결정 (Look-up Table 방식) 
$\(n\times n\)$ 크기의 행렬 $\(M\)$ 의 $\((i,j)\)$ 위치에 들어갈 값은, 앞서 계산해 둔 전이 행렬 $\(W\)$ 의 $\(\alpha \)$ 행 $\(\beta \)$ 열 값을 그대로 가져옵니다.  
$$\(M_{ij}=W_{\alpha \beta }\)$$ 

- 의미: "시점 $\(i\)$ 의 상태에서 시점 $\(j\)$ 의 상태로 전이될 확률이 얼마인가?"를 행렬의 $\(i\)$ 행 $\(j\)$ 열에 적어주는 것입니다. 

#### 3. 구체적인 예시 
시계열 데이터가 [상태1, 상태2, 상태1] 이라고 가정합시다 $(\(n=3\))$.  
이미 계산된 전이 행렬 $\(W\)$ 가 다음과 같다고 치면:

$`W=\left(\begin{matrix}W_{11}&W_{12}\\ W_{21}&W_{22}\end{matrix}\right)`$

이때 $\(3\times 3\)$ 크기의 MTF 행렬 $\(M\)$ 은 다음과 같이 채워집니다.

- $\(M_{11}\)$ : 시점 1(상태1) $\(\rightarrow \)$ 시점 1(상태1) 전이 확률 = $\(W_{11}\)$
- $\(M_{12}\)$ : 시점 1(상태1) $\(\rightarrow \)$ 시점 2(상태2) 전이 확률 = $\(W_{12}\)$
- $\(M_{13}\)$ : 시점 1(상태1) $\(\rightarrow \)$ 시점 3(상태1) 전이 확률 = $\(W_{11}\)$
- $\(M_{21}\)$ : 시점 2(상태2) $\(\rightarrow \)$ 시점 1(상태1) 전이 확률 = $\(W_{21}\)$ ... (이런 방식으로 $\(n\times n\)$ 을 모두 채움) 

#### 최종 행렬 $\(M\)$:

$`M=\left(\begin{matrix}W_{11}&W_{12}&W_{11}\\ W_{21}&W_{22}&W_{21}\\ W_{11}&W_{12}&W_{11}\end{matrix}\right)`$

#### 4. 왜 이렇게 하나요? 
이 과정이 MTF의 핵심인 '시간 의존성(Temporal Dependency)' 포착입니다. 
- 기존 전이 행렬 $\(W\)$ : "상태 1 다음엔 보통 뭐가 오지?"라는 정보만 있고, 언제 그 상태였는지는 모릅니다.
- 확장된 행렬 $\(M\)$ : 시점 $\(i\)$ 와 시점 $\(j\)$ 라는 시간축을 행과 열로 사용함으로써, "시간의 흐름에 따라 전이 확률이 어떻게 배열되는가"를 2차원 이미지 패턴으로 만들어냅니다.

이 방식을 통해 1차원 시계열은 각 시간대별 상태 변화의 확률값을 픽셀 강도(Intensity)로 가지는 2차원 이미지가 됩니다.
  
</details>

시간 $\(t\)$ 의 데이터가 상태 $\(q_{i}\)$ 에 속하고, 시간 $\(s\)$ 의 데이터가 상태 $\(q_{j}\)$ 에 속한다면, MTF 행렬 $\(M\)$ 의 $\((t,s)\)$ 요소는 다음과 같이 정의됩니다.

$$\(M_{ts}=W_{q_{i},q_{j}}\)$$

즉, $\(M\)$ 행렬의 각 좌표는 시간 $\(t\)$ 와 $\(s\)$ 사이의 상태 전이 확률을 나타내며, 이는 데이터의 동적 특성을 픽셀의 밝기로 표현하게 됩니다.

### 수식적 특성 요약 
- 동적 정보의 보존: $\(M\)$ 의 각 원소는 단순히 값의 차이가 아니라, 데이터가 변화하는 '확률적 패턴'을 담고 있습니다.
- 다중 정렬: $\(M\)$ 행렬은 시간 $\(i\)$ 와 $\(j\)$ 사이의 상관관계를 전이 확률이라는 매개체로 연결하여, 시계열 내의 장기적 의존성(Long-term dependency)을 2차원 공간에 투영합니다.

### 차원 축소 (Blurring) 
일반적으로 시계열 길이 $\(n\)$ 이 매우 크면 $\(n\times n\)$ 행렬도 너무 커지므로, 학습 효율을 위해 합성곱(Convolution)이나 평균 풀링(Averaging)과 유사한 방식으로 행렬의 크기를 줄이는 과정을 거칩니다. (예: $\(n\times n\)$ 을 $\(m\times m\)$으로 축소)

만약 주가 데이터에서 '어제 하락 $(\(q_{1}\))$ '하고 '오늘 상승 $(\(q_{2}\))$ '했다면, MTF의 해당 시점 좌표에는 $\(W_{12}\)$ (하락 후 상승할 확률) 값이 채워집니다.  
이 값들이 모여 만들어진 이미지는 특정 종목이나 지수의 고유한 '변동 지문' 역할을 하게 됩니다. 

## 3. MTF의 주요 장점
- 시간적 의존성 포착: 단순한 통계량과 달리, 시간에 따른 값의 변화 패턴을 2차원 공간에 보존합니다. MTF 논문 상세 설명에서 기술적 근거를 확인할 수 있습니다.
- 컴퓨터 비전 결합: 시계열 문제를 이미지 분류 문제로 치환하여 ResNet이나 EfficientNet 같은 강력한 CNN 아키텍처를 그대로 사용할 수 있습니다.
- 노이즈 저항성: 데이터를 구간화(Binning)하는 과정에서 미세한 노이즈가 제거되는 효과가 있습니다.

## 4. 활용 사례
- 금융 데이터 분석: 주가 변동 패턴을 이미지로 학습하여 급락/급등 예측.
- 생체 신호 분류: ECG(심전도)나 EEG(뇌파) 신호의 비정상 패턴 감지.
- 에너지 수요 예측: 전력 소비 패턴의 주기성을 시각적으로 분석.

## 단점
마르코프 전이 필드(MTF)는 강력한 시계열 이미지 변환 기법이지만, 마르코프 모델 자체의 근본적인 한계와 이미지 인코딩 과정에서 발생하는 몇 가지 주요 단점이 있습니다.

### 1. 마르코프 가정의 한계 (기억 상실)
MTF는 1차 마르코프 속성에 기반합니다. 이는 미래의 상태가 바로 직전의 현재 상태에만 의존하며, 그 이전의 과거 이력은 무시한다는 것을 의미합니다.  
- 문제점: 금융 데이터나 복잡한 자연 현상처럼 장기적인 기억(Long-term dependencies)이나 복잡한 인과 관계를 가진 시계열에서는 중요한 정보가 누락되거나 패턴을 정확하게 모델링하기 어렵습니다. 

### 2. 이산화(Quantization) 과정에서의 정보 손실 
연속적인 시계열 데이터를 $\(Q\)$ 개의 이산적인 구간(Bin)으로 나누는 과정에서 양자화 오류(Quantization Error)가 발생합니다. 
- 문제점:구간의 경계값 설정이 임의적이거나 부적절할 경우, 데이터의 미묘한 차이가 무시되어 정보가 손실될 수 있습니다. 이는 시스템의 성능을 저하시키는 '양자화 노이즈'를 발생시킬 수 있습니다. 

### 3. 정상성(Stationarity) 가정
전통적인 마르코프 전이 행렬은 전이 확률이 시간에 따라 일정하게 유지된다는 시간적 정상성(Temporal Stationarity)을 가정합니다. 
- 문제점: 현실의 동적 시스템은 끊임없이 변화하거나 외부 요인에 반응하여 확률이 달라지는 경우가 많습니다. 이러한 비정상성(Non-stationarity) 데이터에서는 모델의 정확도가 떨어질 수 있습니다. 

### 4. 계산 비용 (Computational Cost) 
시계열의 길이가 $\(n\)$ 일 때, MTF는 $\(n\times n\)$ 크기의 행렬을 생성합니다. 
- 문제점: 시계열 데이터의 길이가 매우 길 경우, 생성되는 이미지의 크기가 기하급수적으로 커져 저장 공간이 많이 필요하며, 이후 CNN 모델 학습 시 계산 비용이 증가합니다. 

### 5. 외생 변수 통합의 어려움
MTF는 단일 시계열 데이터를 이미지로 변환하는 데 중점을 둡니다.
- 문제점: 여러 개의 관련 변수(다변량 시계열)나 외부 변수(예: 날씨, 정책 변화 등)를 모델에 통합하여 고려하기가 어렵습니다. 

이러한 단점들로 인해, 실제 연구에서는 MTF 단독보다는 GASF/GADF와 같은 다른 인코딩 방식과 결합하여 여러 채널의 이미지를 생성하거나 (Triple-channel GASF-GADF-MTF), RNN이나 Attention 기반 모델과 함께 사용하는 하이브리드 접근 방식이 제안되기도 합니다.

## 해결 방법
MTF의 단점(정보 손실, 장기 의존성 부족, 계산 복잡도)을 보완하기 위해 제안된 기법들은 크게 '다중 채널 결합', '계층적 구조 활용', 그리고 '대안적 인코딩' 방식으로 나뉩니다. 2026년 현재 가장 많이 활용되는 해결책들은 다음과 같습니다.

### 1. 다중 채널 결합 (Multi-channel Encoding: GAF + MTF)
MTF는 '확률적 변화'는 잘 포착하지만, 시계열의 '절대적 값'이나 '시간적 상관관계'를 보존하는 데 약합니다. 이를 해결하기 위해 Gramian Angular Field(GAF)와 결합하여 사용합니다.
- 해결 방식: GAF(값의 상관관계)와 MTF(상태 전이 확률)를 각각 RGB 이미지의 R 채널과 G 채널에 할당합니다.
- 효과: 시계열의 동역학적 정보(MTF)와 정적/상관관계 정보(GAF)를 동시에 CNN에 전달하여 정보 손실을 최소화합니다.

### 2. 고차 마르코프 전이 필드 (Higher-order MTF) 
1차 마르코프 가정(직전 상태만 참조)의 한계를 극복하기 위해 제안되었습니다. 
- 해결 방식: 현재 상태가 $\(t-1,t-2\)$ 등 과거 여러 시점에 의존한다고 가정하고 전이 확률을 계산합니다.
- 효과: 시계열의 장기 의존성(Long-term dependency)을 더 정교하게 포착할 수 있습니다. 

### 3. 계층적 MTF 및 조각화 (Piecewise MTF) 
데이터가 너무 길어 $\(n\times n\)$ 행렬이 비대해지는 문제를 해결합니다. 
- 해결 방식: 전체 시계열을 일정한 윈도우로 나누어 각각 MTF를 만든 후 이를 쌓거나(Stacking), PAA(Piecewise Aggregate Approximation)를 통해 차원을 축소한 뒤 MTF를 적용합니다.
- 효과: 계산 복잡도를 획기적으로 낮추면서도 중요한 패턴을 보존합니다. pyts의 PAA 결합 예시에서 구현 방식을 확인할 수 있습니다. 

### 4. Recurrence Plot (RP) - 대안적 기법
이산화(Binning) 과정에서의 정보 손실이 치명적일 때 MTF 대신 사용합니다.
- 해결 방식: 상태 공간에서 두 시점 사이의 거리가 특정 임계값보다 가까우면 1, 아니면 0으로 표시하는 방식입니다.
- 효과: 구간을 나누지 않기 때문에 데이터의 미세한 변화를 더 잘 보존하며, 비선형 동역학 분석에 더 유리합니다.

### 5. 가중 마르코프 전이 필드 (Weighted MTF)
모든 전이에 동일한 가중치를 두는 대신, 시간적 거리나 중요도에 따라 가중치를 부여합니다.
- 해결 방식: 최근 데이터의 전이 확률에 더 높은 가중치를 주어 '개념 드리프트(Concept Drift)'와 같은 비정상성 문제를 완화합니다.

요약하자면, 현재 실무에서는 MTF 단독 사용보다는 GAF와 MTF를 결합한 이미지 스택을 구축하거나, 데이터의 특성에 따라 Recurrence Plot을 혼용하여 단점을 상쇄하는 전략이 주로 사용됩니다.  

### 다변량 시계열 데이터일 때에는 어떤 기법이 좋을까?
다변량 시계열 데이터(Multivariate Time Series)는 여러 변수가 서로 영향을 주고받는 '상호 의존성'을 포착하는 것이 핵심입니다.

#### 다채널 이미지 변환 (Multi-channel Image Encoding) 
가장 직관적이고 CNN의 성능을 극대화할 수 있는 방법입니다. 
- 작동 원리: 각 변수 $(\(Var_{1},Var_{2},...,Var_{k}\))$ 를 각각 MTF나 GAF 이미지로 변환한 뒤, 이를 이미지의 RGB 채널처럼 쌓는(Stacking) 방식입니다.
- 장점: 기존의 ResNet, EfficientNet 같은 강력한 비전 모델을 그대로 사용할 수 있습니다.
- 추천 도구: pyts 라이브러리의 다변량 모듈을 사용하면 여러 변수를 동시에 인코딩할 수 있습니다.

#### MTF + GAF 결합 (Hybrid Encoding)
단일 변수가 아닌 다변량에서는 정보의 밀도가 높아야 하므로, MTF의 단점을 보완한 결합 기법이 유리합니다.
- 작동 원리:
  - Channel 1: 각 변수의 MTF (상태 전이 확률 포착)
  - Channel 2: 각 변수의 GASF (값의 시간적 상관관계 포착)
  - Channel 3: 각 변수의 GADF (값의 위상 변화 포착)
- 장점: 확률적 패턴과 수치적 변화를 동시에 학습하므로 다변량 데이터의 복잡한 특징을 잘 잡아냅니다.

#### MTF를 넘어서는 최신 기법: GNN (Graph Neural Networks)
다변량 데이터의 변수 간 '관계'가 중요하다면 이미지 변환보다 그래프 기반 방식이 더 강력할 수 있습니다.
- 기법명: MTGNN (Multivariate Time Series Forecasting Graph Neural Networks) 또는 Graph-WaveNet
- 작동 원리: 각 변수를 그래프의 노드(Node)로 설정하고, 변수 간의 상관관계를 에지(Edge)로 학습합니다.
- 장점: 변수가 수십 개 이상일 때 이미지 변환보다 효율적이며, 변수 간의 인과 관계를 직접 모델링합니다. MTGNN 논문 참고

#### 변수 간의 상관관계(A가 오르면 B가 내리는 등)가 매우 중요하다면:
Recurrence Plot(RP)을 사용하거나, 이미지 변환 없이 Transformer 기반 모델(예: Informer, Autoformer)을 고려하는 것이 성능 면에서 유리합니다.

#### 데이터의 양이 방대하다면:
MTF 변환 과정의 메모리 소모를 줄이기 위해 MTGNN이나 PatchTST 같은 최신 시계열 전용 아키텍처를 추천합니다.

실제 구현 시에는 PyTorch Forecasting이나 Darts와 같은 라이브러리를 활용하면 다변량 시계열 전처리를 더 효율적으로 수행할 수 있습니다.
