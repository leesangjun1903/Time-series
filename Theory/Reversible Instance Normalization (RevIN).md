# Reversible Instance Normalization (RevIN)

Reversible Instance Normalization (RevIN)은 주로 시계열 데이터(Time-series)를 다룰 때 발생하는 분포 변화(Distribution Shift) 문제를 해결하기 위해 제안된 기법입니다.  
시계열 데이터는 시간에 따라 평균이나 분산이 변하는 '비정상성(Non-stationarity)'을 띠는 경우가 많은데, RevIN은 이를 효과적으로 정규화하고 다시 복원함으로써 모델의 예측 성능을 획기적으로 높여줍니다.

### 1. 등장 배경: 왜 필요한가?
일반적인 딥러닝 모델은 학습 데이터와 테스트 데이터의 분포가 유사하다고 가정합니다.  
하지만 실제 시계열 데이터(예: 주식, 기온, 전력 소모량)는 다음과 같은 문제를 겪습니다.
- 정상성 부족: 과거 데이터의 평균은 10인데, 미래 데이터의 평균이 100이 되는 상황.
- 분포 불일치: 모델이 학습한 수치 범위를 벗어난 값이 들어오면 예측력이 급격히 저하됨.

RevIN은 데이터를 모델에 넣기 전에 정규화(Normalization)하고, 모델의 출력값을 다시 역정규화(Denormalization)하여 원래의 스케일로 돌려놓는 대칭적인 구조를 가집니다.

### 2. RevIN의 핵심 수식
RevIN은 두 단계(Instance Normalization & Denormalization)로 구성됩니다. 입력 데이터를 x , 모델의 예측값을 y 라고 할 때의 과정은 다음과 같습니다.

#### ① 입력 단계: 정규화 (Normalization)
각 인스턴스(데이터 샘플)의 평균과 표준편차를 계산하여 데이터를 표준화합니다.

$$x_{n o r m} = \gamma \left(\right. \frac{x - \mathbb{E} \left[\right. x \left]\right.}{\sqrt{V a r \left[\right. x \left]\right. + \epsilon}} \left.\right) + \beta$$

- $$\mathbb{E} [x]$$ : 입력 데이터의 평균
- $$Var [x]$$ : 입력 데이터의 분산
- $$\gamma , \beta$$ : 학습 가능한 아핀(Affine) 파라미터 (데이터의 특성을 보존하기 위함)
- $$\epsilon$$ : 수치적 안정을 위한 아주 작은 값

#### ② 출력 단계: 역정규화 (Denormalization)
모델이 $$x_{n o r m}$$ 을 바탕으로 결과값 $$\left(\hat{y}\right)_{n o r m}$$을 내놓으면, 이를 다시 원래 데이터의 스케일로 복원합니다. 이 부분이 RevIN의 핵심입니다.

```math
\hat{y}_{final} = \sqrt{Var[x] + \epsilon} \left( \frac{\hat{y}_{norm} - \beta}{\gamma} \right) + \mathbb{E}[x]
```

정리하자면: 입력할 때 뺀 평균을 마지막에 다시 더해주고, 나눴던 표준편차를 다시 곱해줌으로써 모델이 오직 "패턴"에만 집중하게 만들고 "수치적 크기"는 나중에 보충해주는 방식입니다.

학습 데이터와 테스트 데이터의 분포가 다를 때(Out-of-Distribution, OOD)야말로 RevIN이 가장 강력한 위력을 발휘하는 상황입니다.

일반적인 모델은 학습 시 보지 못한 수치 범위(Scale)가 테스트 데이터에 등장하면 당황하며 엉뚱한 예측을 내놓지만, RevIN은 이를 '평균 이동'과 '스케일 조정'으로 해결합니다.

시계열 데이터에서 분포가 다르다는 것은 보통 평균이 치솟거나(Trend) 변동 폭이 커지는 것(Volatility)을 의미합니다.
- RevIN 미적용: 모델은 학습 데이터의 범위(예: 10~20)만 알고 있는데, 테스트 데이터가 100~120으로 들어오면 이를 "이상치"로 판단하거나 학습했던 최대치인 20 부근으로만 예측하려고 합니다.
- RevIN 적용: 100~120이라는 데이터를 정규화하여 다시 0~1 사이의 상대적 패턴으로 변환합니다. 모델은 "값이 얼마나 큰지"는 신경 쓰지 않고 "지금 오르고 있는가, 내리고 있는가"라는 구조적 패턴만 학습하게 됩니다.

RevIN은 전체 데이터셋의 평균을 사용하는 것이 아니라, 지금 들어온 바로 그 데이터(Instance)의 평균과 분산을 사용합니다.  
학습 데이터의 평균이 10이었더라도, 테스트 데이터의 특정 구간 평균이 500이라면 RevIN은 그 순간의 500을 기준으로 정규화를 수행합니다.  
즉, 모델 입장에서는 입력값이 항상 익숙한 분포(평균 0, 분산 1 근처)로 들어오기 때문에, 학습 시 배운 예측 로직을 그대로 적용할 수 있습니다.

비유하자면:
단순히 정규화만 하고 끝난다면 모델은 "이게 원래 얼마짜리 데이터였는지" 알 길이 없습니다. 하지만 RevIN은 마지막 단계에서 해당 시점의 평균과 분산을 다시 곱해줍니다.
- (정규화) 전교 1등의 점수(100점)와 전교 꼴찌의 점수(10점)를 각각 해당 학교의 수준에 맞춰 '상위 1%'와 '하위 90%'라는 상대적 지표로 바꿉니다.
- (예측) 모델은 이 상대적 지표를 보고 다음 성적도 '상위 1%'일 것이라고 예측합니다.
- (역정규화) 이 '상위 1%'라는 결과를 다시 해당 학교의 원래 점수 체계(100점 만점)로 환산하여 최종 결과값을 내놓습니다.

물론 RevIN이 만능은 아닙니다. 분포가 바뀔 때 '값의 범위'뿐만 아니라 '데이터 사이의 논리적 규칙(Dynamics)' 자체가 바뀌어버린 경우에는 효과가 제한적일 수 있습니다.
- 효과적일 때: 주식 가격이 10만 원에서 100만 원으로 올랐지만, 오르내리는 패턴(심리)은 비슷할 때.
- 한계가 있을 때: 과거에는 비가 오면 기온이 내려갔는데, 미래에는 비가 오면 기온이 올라가는 식으로 물리 법칙 자체가 변하는 경우.

RevIN은 "분포가 바뀌어도 데이터가 가진 고유의 움직임 패턴은 유지된다"는 가정을 극대화하는 기법입니다. 따라서 시계열 데이터의 추세(Trend)가 바뀌어 학습/테스트 분포가 불일치할 때 성능을 방어하는 데 매우 탁월한 선택입니다.

학습 데이터와 테스트 데이터의 분포가 다를 때 RevIN이 구체적으로 어떻게 작동하는지, 데이터의 흐름에 따라 수식과 함께 단계별로 설명해 드리겠습니다.

핵심은 RevIN이 학습 시점의 통계량을 고정해서 쓰지 않고, 매 입력(Instance)마다 그 안에서 통계량을 새로 계산한다는 점에 있습니다.

### 1. 학습 단계 (Training Phase)
학습 데이터 $$x_{t r a i n}$$ 이 모델에 들어올 때의 과정입니다.
- 통계량 추출: 현재 들어온 샘플 $$x_{t r a i n}$$의 평균( $$\mu_{t r}$$ )과 표준편차( $$\sigma_{t r}$$ )를 구합니다.
- 정규화 (Normalization): 모델( $$f_{\theta}$$ )에는 정규화된 값만 전달합니다.

$$x_{n o r m} = \frac{x_{t r a i n} - \mu_{t r}}{\sigma_{t r}}$$

- 예측: 모델은 분포가 고정된 $$x_{n o r m}$$ 을 보고 결과 $$\left(\hat{y}\right)_{n o r m}$$ 을 내놓습니다.

역정규화 (Denormalization): 모델의 출력에 다시 자기 자신의 통계량을 곱하고 더합니다.

$$\left(\hat{y}\right)_{f i n a l} = \left(\hat{y}\right)_{n o r m} \cdot \sigma_{t r} + \mu_{t r}$$

### 2. 테스트 단계 (Inference Phase) — 중요!
학습 데이터와 분포가 완전히 다른 $$x_{t e s t}$$가 들어왔다고 가정해 봅시다. (예: $$x_{t r a i n}$$의 평균은 10인데, $$x_{t e s t}$$의 평균은 1000인 상황)
- 실시간 통계량 추출: 모델은 학습 때 본 평균(10)을 쓰지 않습니다. 지금 들어온 $$x_{t e s t}$$ 에서 직접 평균( $$\mu_{t e} \approx 1000$$ )과 표준편차( $$\sigma_{t e}$$ )를 계산합니다.

바로 이 지점에서 "테스트 데이터의 분포를 몰라도 된다"는 의문이 해결됩니다. 전체 분포는 몰라도, 지금 내 눈앞에 있는 1000이라는 숫자들의 평균은 구할 수 있기 때문입니다.

- 정규화: $$x_{t e s t}$$ 를 자기 자신의 통계량으로 정규화합니다.

$$x_{n o r m} = \frac{x_{t e s t} - \mu_{t e}}{\sigma_{t e}}$$

이 결과 $$x_{n o r m}$$ 은 다시 평균 0, 표준편차 1 근처의 값이 됩니다. 모델 입장에서는 학습 때 보던 숫자 범위와 똑같은 값이 들어오는 셈입니다.

- 예측: 모델 $$f_{\theta}$$ 는 학습된 대로 패턴을 분석해 $$\left(\hat{y}\right)_{n o r m}$$ 을 출력합니다.
- 역정규화: 이 상대적인 예측값에 다시 테스트 샘플의 통계량을 입힙니다.

$$\left(\overset{̂}{y}\right)_{f i n a l} = \left(\overset{̂}{y}\right)_{n o r m} \cdot \sigma_{t e} + \mu_{t e}$$

결과값은 다시 1000대의 숫자로 복원됩니다.

### 3. 왜 성능이 좋아지는가? (수식의 의미)
만약 일반적인 모델이었다면, 테스트 시 $$x_{t e s t} = 1000$$ 이 들어올 경우 모델 내부의 연산 결과가 비정상적으로 커져서 예측이 실패했을 것입니다. 하지만 RevIN은 다음의 논리를 따릅니다.

"값의 절대적인 크기는 입력( $$x_{t e s t}$$ )에서 빌려오고( $$\mu , \sigma$$ ), 값의 변화하는 모양(Shape)은 모델( $$f_{\theta}$$ )이 결정한다."

### 4. 요약: "테스트 분포를 모른다"는 것의 의미
RevIN이 테스트 데이터의 분포를 모른다는 것은 "미래에 어떤 값이 올지 미리 알 필요가 없다"는 뜻입니다.  
하지만 "현재 예측을 위해 모델에 집어넣은 바로 그 입력값( $$x$$ )"은 우리가 알고 있으므로, 그 안에서 평균과 분산을 뽑아내는 것은 가능합니다.  
RevIN은 이 '현재의 통계량'이 '가까운 미래의 통계량'과 유사할 것이라는 시계열의 국소적 특성을 이용해, 분포 불일치 문제를 해결하는 것입니다.


