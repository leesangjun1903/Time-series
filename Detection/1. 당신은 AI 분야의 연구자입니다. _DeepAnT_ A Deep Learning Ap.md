<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# 1. 당신은 AI 분야의 연구자입니다. "DeepAnT: A Deep Learning Approach for Unsupervised Anomaly Detection in Time Series"의 핵심 주장과 주요 기여를 간결하게 요약하세요.

2. 이 논문이 "해결하고자 하는 문제, 제안하는 방법(수식 포함), 모델 구조, 성능 향상 및 한계"를 중심으로 자세하게 설명하세요.
3.특히 "모델의 일반화 성능 향상 가능성"과 관련된 내용을 중점적으로 다뤄 주세요.
4.요약 후에는 "해당 논문이 앞으로의 연구에 미치는 영향과 앞으로 연구 시 고려할 점"도 자세히 기술해 주세요.
“2020년 이후 관련 최신 연구 비교 분석”
수식을 LaTeX 코드로 답변 형식을 맞춰주고, LaTeX 렌더링을 위해 마크다운에 달러 기호를 사용해 주세요.
3. 핵심 주장과 주요 기여 (간단 요약)
DeepAnT는 “다음 시점 예측용 1D CNN”을 학습한 뒤, 예측값과 실제값의 오차를 이용해 완전 비지도(unsupervised)로 시계열 이상을 검출하는 방법을 제안한다. 이 방식으로 적은 양의 정상 데이터만으로도 점 이상(point), 문맥 이상(contextual), 디스코드(discord)를 효과적으로 검출하며, 10개 벤치마크·433개 시계열에서 기존 15개 방법보다 우수하거나 유사한 성능을 보였다고 주장한다.[^1_1][^1_2]

주요 기여는 다음 네 가지이다.[^1_1]

- (1) 비지도 설정에서 점/문맥/디스코드 이상을 모두 탐지하는 최초의 딥러닝 기반 일반 프레임워크 제안.[^1_3][^1_1]
- (2) 단변량·다변량 모두에 적용 가능한 간단한 CNN 기반 파이프라인 설계.[^1_1]
- (3) LSTM 기반보다 “데이터 비포화” 상황에서 더 나은 일반화(40% 길이만으로 학습 가능, 소량 데이터에서도 예측 정확).[^1_1]
- (4) Yahoo, NAB, UCI, NASA 등 10개 벤치마크에서 15개 SOTA 기법과 비교 실험을 통해 강건한 성능 입증.[^1_2][^1_1]

***

## 2. 문제 정의, 방법(수식), 모델 구조, 성능 및 한계

### 2.1 해결하고자 하는 문제

- 현실의 IoT/로그/센서 시계열은 주기성, 추세, 계절성, 불규칙성 등이 강해, 단순 거리·밀도 기반 이상 탐지(kNN, LOF, HBOS 등)는 “주기 내의 국소적인 작은 이상”을 잘 놓친다.[^1_2][^1_1]
- 대규모 스트리밍 환경에서는 라벨링이 거의 불가능하므로, 비지도·반지도 방식이 필요하다.[^1_1]
- DeepAnT가 직접 겨냥하는 문제는 다음과 같다.[^1_3][^1_1]
    - 라벨 없는 시계열 스트림에서
    - 주기성과 시간적 문맥을 고려하면서
    - 개별 시점 수준(point-wise)의 이상 점수와 임계값을 산출하는 통합 방법.


### 2.2 제안 방법: 예측 기반 비지도 이상 탐지

#### (a) 데이터 윈도잉과 학습 과제

단변량 시계열 $\{x_t\}_{t=0}^{T}$ 가 주어졌을 때, 길이 $w$의 히스토리 윈도우와 예측 지평 $p_w$를 정의한다.[^1_1]

- 입력–레이블 쌍 구성 (기본 설정은 $p_w = 1$)

$$
\mathbf{x}_t = [x_{t-w+1}, \dots, x_t], \quad y_t = x_{t+1}.
$$

이를 통해 “다대일(many-to-one) 회귀”로 다음 시점 예측을 학습한다.[^1_1]

다변량 시계열의 경우, $\mathbf{x}_t \in \mathbb{R}^d$ 를 같은 방식으로 윈도잉하여 CNN의 채널(또는 feature)로 넣는다.[^1_4][^1_1]

#### (b) CNN 기반 예측 모델

1D convolution 연산은 다음과 같이 정의된다.[^1_1]

$$
s(t) = (x * w)(t) = \sum_{\tau=-\infty}^{\infty} x(\tau)\,w(t-\tau).
$$

실제 구현에서는 유한 길이 필터를 사용하고, 층 $l$, 필터 인덱스 $j$, 위치 $i$에 대해

$$
z^{(l)}_{j,i} = \sum_{k=-K}^{K} W^{(l)}_{j,k}\,a^{(l-1)}_{i-k} + b^{(l)}_j,
$$

$$
a^{(l)}_{j,i} = \max\bigl(0,\, z^{(l)}_{j,i} \bigr)
$$

와 같은 ReLU 활성화가 사용된다.[^1_1]

완전연결층에서는

$$
z^{(l)}_j = \sum_{k} W^{(l)}_{j,k}\,a^{(l-1)}_k + b^{(l)}_j,\quad
a^{(l)}_j = \max\bigl(0,\, z^{(l)}_j \bigr)
$$

형태로 다음 시점(또는 시퀀스) 예측값 $\hat{y}_t$를 출력한다.[^1_1]

손실 함수는 정규화된 타깃 $y_j$, 예측값 $\hat{y}_j$ 에 대해 MAE를 사용한다.[^1_1]

$$
\mathrm{MAE} = \frac{1}{n}\sum_{j=1}^{n}\bigl|y_j - \hat{y}_j\bigr|.
$$

최적화는 SGD로 수행하며, 오차 역전파를 통해 $\partial C/\partial w,\, \partial C/\partial b$를 계산해 갱신한다.[^1_1]

#### (c) 이상 점수와 임계값

예측 후, 각 시점의 실제값 $y_t$와 예측값 $\hat{y}_t$ 사이의 유클리드 거리(단변량에서는 절대 오차와 동일)를 이상 점수로 사용한다.[^1_1]

$$
s_t = d(y_t, \hat{y}_t) = \sqrt{(y_t - \hat{y}_t)^2} = |y_t - \hat{y}_t|.
$$

- $s_t$가 클수록 해당 시점이 정상 패턴에서 많이 벗어난 것으로 간주된다.[^1_1]
- 임계값 $\theta$는 검증 세트 또는 서브 벤치마크 수준에서
    - 정규분포 가정 하 $K\sigma$ 편차,
    - 또는 비모수 밀도 기반 방법
으로 자동 추정한다.[^1_5][^1_1]
- 최종 라벨링:

$$
\text{anomaly at } t \Leftrightarrow s_t > \theta.
$$

디스코드(서브시퀀스) 탐지 시에는 서브시퀀스 내의 점별 이상 점수를 누적/평균하여 서브시퀀스 수준의 점수로 쓰고, 서브시퀀스 임계값을 적용한다.[^1_1]

### 2.3 모델 구조

논문에서 제안하는 기본 CNN 구조는 매우 단순한 1D CNN 회귀 네트워크이다.[^1_6][^1_1]

- 입력: 길이 $w$의 (정규화된) 시계열 윈도우.
- 합성곱층 1:
    - 필터 개수: 32
    - 커널 크기: 실험으로 선택 (예: 3, 5 등, 벤치마크별 튜닝)
    - 활성화: ReLU
- Max pooling 1
- 합성곱층 2:
    - 필터 개수: 32
    - ReLU
- Max pooling 2
- 완전연결층: 히든 유닛 여러 개, ReLU
- 출력층: 차원 = $p_w$ (기본 1, 시퀀스 예측 시 $>1$).[^1_2][^1_1]

이 구조를 단변량 및 다변량 시계열에 동일하게 적용하며, 다변량의 경우 feature 차원이 필터에서 자동으로 처리하는 채널로 취급된다.[^1_4][^1_1]

### 2.4 성능 향상: 실험 결과 요약

#### (a) Yahoo Webscope S5

- 4개 서브 벤치마크(A1–A4)에서 Twitter S-H-ESD, Yahoo EGADS, LSTM 기반 예측 모델과 비교.[^1_7][^1_1]
- 40% 길이만 학습에 사용(나머지 60% 테스트), 서브 벤치마크별 공통 임계값 사용.[^1_1]
- F1-score 기준으로:
    - A3, A4에서 DeepAnT가 가장 높은 F1.
    - A1, A2에서는 2위지만, 서브 벤치마크 공통 임계값을 쓰는 제약에도 불구하고 SOTA에 근접.[^1_1]
- LSTM 예측을 사용하는 변형과 비교 시, 3개 서브 벤치마크에서 CNN(DeepAnT)이 LSTM보다 우수, 1개에서는 근소 열세 → 적은 데이터에서 CNN의 파라미터 효율성이 부각.[^1_1]


#### (b) NAB (Numenta Anomaly Benchmark)

- 58개 스트림, 11개 알고리즘(Twitter AD, Skyline, Numenta HTM, Bayesian CPD, EXPoSE 등)과 비교.[^1_8][^1_1]
- NAB-score 대신, 정확한 이상 검출/거부를 중시하는 Precision, Recall, F1 사용.[^1_1]
- 특히 특정 도메인(클라우드 지연, 트래픽 등)에서 다른 방법들이 높은 정밀도–극히 낮은 재현율(0.001–0.36)에 머무는 반면, DeepAnT는 유사 정밀도에서 훨씬 높은 재현율을 달성.[^1_8][^1_1]
- 도메인별 평균 F1에서 DeepAnT가 모든 도메인에서 SOTA 또는 그 이상(2–13배 향상)으로 보고됨.[^1_1]


#### (c) 고전 다변량 벤치마크 (UCI, OpenML)

- Shuttle, Pima, ForestCover, Ionosphere, HTTP, SMTP, Mulcross, Mammography 등 8개 데이터셋에 대해 semi-supervised(정상만 학습) 환경에서 AUC 비교.[^1_1]
- 비교 대상: Isolation Forest, LOF, One-Class SVM.
- 40% 정상 데이터로 학습, 나머지 테스트.
- 대부분 데이터셋에서 DeepAnT의 AUC가 최고, 특히 OCSVM보다 좋은 결과를 보이며, 다변량 시계열에도 적용 가능함을 시연.[^1_9][^1_1]


#### (d) NASA Shuttle Valve Discord

- NASA valve 전류 시계열에서 정상 서브시퀀스로 예측 모델을 학습 후, 서브시퀀스별 누적 이상 점수로 디스코드 검출.[^1_1]
- downsampling(70% 감소)에도 디스코드 서브시퀀스를 잘 찾아내며, 디스코드 내부에서 구체적으로 이상이 발생한 구간을 점별 점수로 하이라이트 가능함을 시각적으로 보여줌.[^1_1]


### 2.5 한계와 논문에서 언급한 제약

논문에서 명시적으로 밝히는 한계는 다음과 같다.[^1_2][^1_1]

1. **데이터 오염(contamination)에 대한 민감도**
    - 전체 데이터 중 이상 비율이 5% 미만일 때는 정상 분포를 잘 모델링하지만, 그 이상이면 모델이 이상 패턴도 “정상”으로 학습해 버릴 수 있다.[^1_3][^1_1]
2. **아키텍처·하이퍼파라미터 선택 난이도**
    - 윈도우 길이 $w$, 필터 크기, 층 수, 임계값 $\theta$ 등은 데이터셋별로 튜닝이 필요하고, 보편적인 설정은 없다.
    - 향후 Neural Architecture Search(NAS)를 통해 사람의 설계 부담을 줄이는 방안을 제안.[^1_1]
3. **적대적 예제(adversarial examples)에 취약**
    - 대부분의 데이터 기반 모델과 마찬가지로, DeepAnT도 의도적으로 조작된 입력에 취약할 수 있으며, 보안·안전 중요 시스템에 직접 적용하기에는 한계가 있다.[^1_1]
4. **문제 정의·평가지표의 한계**
    - NAB 등 기존 벤치마크에서 “윈도우 기반 레이블링”이 재현율을 과소평가/혼란시키는 문제를 지적하면서, 성능 해석이 쉽지 않음을 인정한다.[^1_10][^1_1]

***

## 3. 모델의 일반화 성능 및 향상 가능성

### 3.1 논문이 주장하는 일반화 측면

논문이 강조하는 일반화 관련 포인트는 크게 세 가지이다.[^1_3][^1_1]

1. **소량 데이터에서도 학습 가능**
    - 40% 길이만 학습에 사용하고도, 나머지 60%를 잘 예측하여 이상을 검출.
    - 예: Yahoo 일부 시계열에서 568포인트, Ionosphere에서 140포인트의 학습만으로도 의미 있는 예측과 이상 검출 성능 확보.[^1_3][^1_1]
2. **이상 제거 없이 학습 가능 (contamination<5%)**
    - 학습 전 별도의 수동 이상 제거 없이 전체 스트림으로 예측기를 학습해도, 희소한 이상은 평균적 패턴에서 벗어나기 때문에 예측 오차 기반 점수로 검출 가능.[^1_4][^1_1]
3. **도메인·데이터셋 간 전반적으로 일관된 성능**
    - 다양한 도메인(웹 로그인, 네트워크 트래픽, 광고, 의료, 제조, 우주선 밸브 등)에서 알고리즘 변경 없이 동일한 딥러닝 프레임워크로 경쟁력 있는 결과.[^1_2][^1_1]

이러한 점 때문에, 이후 survey들은 DeepAnT를 “예측 기반(CNN forecasting–based) 비지도 TSAD의 대표 모델”로 분류하면서, 적은 라벨과 소량 데이터에서의 효율성을 장점으로 요약한다.[^1_5][^1_3]

### 3.2 일반화 성능 향상 가능성 (논문 및 이후 연구 관점)

논문 말미와 이후 연구를 함께 보면 DeepAnT의 일반화를 더 향상시킬 수 있는 방향은 다음과 같이 정리할 수 있다.

1. **도메인 적응·전이 학습 (논문이 직접 제안)**
    - 저자들은 향후 연구로 “domain adaptation과 transfer learning을 활용한 시계열 이상 탐지”를 명시적으로 제안한다.[^1_1]
    - 예: 한 공정/장비에서 학습한 DeepAnT를 다른 장비에 fine-tuning 하거나, 다수 라인을 joint pretraining 후 개별 라인으로 적응.
2. **정교한 전처리·분해와의 결합**
    - 논문은 전처리의 영향을 향후 평가하겠다고만 언급하지만,[^1_1]
    - 이후 RobustTAD처럼 STL/시즌-트렌드 분해와 CNN을 결합해 일반화 성능을 높이는 접근이 등장했다.[^1_11]
    - DeepAnT도 (a) trend/seasonality 제거 후 잔차만 예측, (b) 멀티스케일 CNN으로 다양한 시간 스케일을 동시에 모델링하면 drift·concept shift에 더 강해질 수 있다.
3. **자기지도(self-supervised) 및 대조학습(contrastive) 통합**
    - 최근 Transformer·BERT 계열 TSAD 모델(AnomalyBERT, DACR, Anomaly Transformer 등)은 self-supervised 마스킹/변형, 대조학습을 통해 더 풍부한 표현을 학습해 일반화력을 높인다.[^1_12][^1_13][^1_14]
    - DeepAnT 스타일의 CNN 예측기에
        - 다중 horizon 예측,
        - 입력 변형에 대한 consistency loss,
        - 정상·비정상 패턴 간 대조학습
등을 추가하면, 표본 수가 더 적거나 분포가 더 복잡한 환경에서도 성능이 향상될 가능성이 크다.[^1_15][^1_3]
4. **임계값 추정의 자동화·순응형(adaptive) 설계**
    - 현재는 서브 벤치마크 수준에서 고정 $\theta$를 찾지만, concept drift나 계절성 변화가 있는 실시간 시스템에서는 온라인으로 $\theta_t$를 갱신하는 순응형 임계값이 일반화에 중요하다.[^1_1]
    - CUSUM, POT(Peaks-over-Threshold) 기반 EVT, 베이지안 업데이트 등을 결합하면, 데이터 분포 변화에 따라 오탐/미탐 균형을 안정적으로 유지할 수 있다.[^1_16][^1_3]
5. **설명가능성·원인 분석(Explainability)**
    - 이후 ECATS와 같은 “concept-based” 또는 그래프/주의 메커니즘 기반 모델들이, 이상 점의 원인·변수 기여도를 설명하는 방향으로 발전했다.[^1_17][^1_18]
    - DeepAnT도 1D-CNN 필터 활성화, Grad-CAM 유사 기법을 활용하면, 어떤 시간 위치/주기가 이상 판단에 기여했는지 설명하는 방식으로 실무 적용에서 신뢰성과 일반화(모델 선택) 판단에 도움을 줄 수 있다.[^1_4]

***

## 4. 2020년 이후 관련 최신 연구 비교 분석 및 향후 연구에의 영향

### 4.1 2020년 이후 DeepAnT와 유사한/대조적인 주요 흐름

여기서는 DeepAnT와 같은 “예측 기반 비지도 TSAD” 라인과, 이후 발전한 대표적 방법들을 중심으로 비교한다.


| 범주 | 대표 방법 (연도) | 핵심 아이디어 | DeepAnT와의 관계 |
| :-- | :-- | :-- | :-- |
| CNN 예측 기반 | RobustTAD (2020) | 강건한 계절-추세 분해 + CNN encoder–decoder, 멀티스케일 정보, 데이터 증강·가중 손실로 레이블 희소성 대응.[^1_11] | DeepAnT의 “CNN 예측 + 이상 점수” 아이디어를 계절/추세 분해, 멀티스케일 구조로 확장해 일반화·성능 향상. |
| Graph·멀티변수 | MTAD-TF, GANet 등 (2020) | 그래프 attention으로 변수 간 의존성 + CNN/LSTM 예측/재구성.[^1_19][^1_20] | DeepAnT는 변수 간 구조를 explicit하게 모델링하지 않지만, 이들 연구는 multivariate에서의 관계 모델링을 보완. |
| GAN 기반 | TadGAN, TAnoGAN, LSTM-VAE-GAN (2020) | GAN/AE-GAN으로 재구성 오차+판별자 점수 기반 이상 탐지.[^1_21][^1_22][^1_23] | DeepAnT는 예측 오차 기반; GAN들은 생성-판별 구조로 더 복잡한 분포를 학습. 그러나 학습 안정성·속도 문제. |
| Transformer 기반 | Anomaly Transformer, TranAD 등 (2021–) | self-attention 기반 association discrepancy나 attention 기반 예측/재구성으로 긴 시퀀스·멀티변수 처리.[^1_12][^1_24] | DeepAnT보다 long-range dependency와 변수 간 상호작용을 더 잘 포착하지만, 파라미터 수·연산량 증가. |
| 간단·해석 지향 | SimAD, 단순 통계 방법 (2022–24) | 복잡 모델보다 적절한 윈도잉·유사도 측정·평가의 중요성을 강조.[^1_15][^1_10] | DeepAnT처럼 비교적 단순한 구조가, 잘 설계된 파이프라인·평가지표와 결합될 때 여전히 경쟁력 있음을 지지. |
| 서베이·비교 연구 | 다양한 survey (2020–2025) | DeepAnT를 대표 CNN 예측 기반 비지도 TSAD로 인용, 장점(데이터 효율성, 단/다변량, 디스코드)과 단점(긴 의존성 제한, contamination 민감도)을 명시.[^1_3][^1_5][^1_25] | DeepAnT의 위치를 “baseline이자 출발점”으로 확립. |

요약하면, DeepAnT는 이후 RobustTAD, MTAD-TF, Anomaly Transformer, AnomalyBERT 등 보다 복잡한 모델들로 확장되는 “예측 기반 비지도 TSAD” 계열의 초기 대표 모델로 자리 잡았고, 현재는

- (i) 계절-추세 분해 + CNN,
- (ii) 그래프/attention을 통한 변수 간 관계 학습,
- (iii) self-/contrastive 학습을 통한 일반화 향상,
- (iv) Transformer·foundation model 기반 장기 의존성 학습

방향으로 크게 확장되었다.[^1_18][^1_24][^1_11][^1_12]

### 4.2 향후 연구에 미치는 영향

DeepAnT가 이후 연구에 남긴 영향은 다음과 같이 정리할 수 있다.

1. **예측 기반 TSAD의 표준 패턴 확립**
    - “윈도우 기반 다음 시점 예측 + 오차 기반 이상 점수 + 통계적 임계값”이라는 패턴은 현재 대부분의 예측 기반 TSAD에서 기본 골격으로 사용된다.[^1_26][^1_3]
    - 최신 Transformer/Graph/TCN 모델들도 예측 손실을 이상 점수로 쓰는 경우 DeepAnT와 동일한 철학을 따른다.[^1_24][^1_12]
2. **CNN의 시계열 적용 가능성을 입증**
    - 당시 LSTM·RNN이 주류였지만, DeepAnT는 파라미터 효율적인 1D CNN도 충분히 강력한 시계열 표현을 학습할 수 있다는 것을 실증했다.[^1_4][^1_1]
    - 이후 C-LSTM, TCN, TimesNet 등 CNN/TCN 기반 시계열 모델들의 개발에 간접적으로 기여했다는 평가가 많다.[^1_27][^1_24]
3. **벤치마크와 평가에 대한 비판적 시각 촉발**
    - NAB, Yahoo 등의 label·window 설계를 비판적으로 사용하면서, 실제 F1/precision/recall을 별도로 보고한 점은, 이후 “벤치마크가 진보를 착시시키고 있다”는 비판적 논문들(예: Keogh 계열)에 선행 사례로 볼 수 있다.[^1_10][^1_7]
4. **실무용 라이브러리·툴링에의 영향**
    - MathWorks, 오픈소스 구현 등이 DeepAnT 구조를 직접 구현한 detector를 제공하면서, 산업계 모니터링 툴킷에서 “DeepAnT 스타일 CNN 예측 기반 이상 탐지”가 표준 옵션 중 하나가 되었다.[^1_28][^1_29]

### 4.3 앞으로 연구 시 고려할 점 (연구자 관점 제안)

DeepAnT를 기반 혹은 비교 대상으로 연구를 설계할 때, 다음과 같은 점을 특히 고려하는 것이 좋다.

1. **공정한 벤치마크와 평가 지표 선택**
    - Yahoo/NAB만으로는 충분치 않으므로, 최근 제안된 UCR Time Series Anomaly Archive, TSB-AD, 멀티도메인 데이터셋 등을 포함해 평가해야 한다.[^1_30][^1_10]
    - NAB-score 뿐 아니라, point-wise/segment-wise F1, delay, precision-recall 곡선, detection latency 등을 함께 보고, “진짜” 성능 향상을 입증해야 한다.[^1_26]
2. **Baseline으로서 DeepAnT의 활용**
    - 새로운 복잡한 모델(Transformer, GAN, Graph 등)을 제안할 때, DeepAnT 수준의 간단한 CNN 예측기가 이미 상당히 강력한 베이스라인임을 인지하고, 반드시 포함해 비교하는 것이 필요하다.[^1_5][^1_3]
3. **일반화와 오버피팅의 균형**
    - 긴 시퀀스·고차원 멀티변수에 대해 Transformer·Graph 기반 모델이 더 높은 표현력을 제공할 수 있지만, 실질적인 데이터 수와 라벨(혹은 pseudo-label)의 품질에 비해 과도한 용량을 갖지 않도록 정규화, dropout, 증강, self-supervision을 적절히 설계해야 한다.[^1_14][^1_31]
4. **도메인 적응과 온라인 학습**
    - DeepAnT가 고정 모델+고정 임계값 구조인 반면, 실제 환경에서는 concept drift가 빈번하므로,
        - 온라인/continual 업데이트,
        - domain adaptation, federated learning 기반 분산 학습,[^1_9]
        - 순응형 임계값 업데이트
를 통합한 “실제 서비스 가능한 TSAD 프레임워크” 연구가 중요하다.
5. **설명가능성·원인 분석 통합**
    - 이상 탐지 결과만 제공하는 것이 아니라, 어떤 변수/주기/이벤트가 기여했는지 명확히 설명할 수 있는 구조(예: 그래프·주의·개념 기반)와 시각화 도구를 함께 연구해야 한다.[^1_17][^1_18][^1_4]

***

정리하면, DeepAnT는 “간단한 1D CNN 예측기 + 오차 기반 이상 점수”라는 구조로, 비지도 시계열 이상 탐지의 강력한 기준선을 제시했고, 이후의 다양한 딥러닝·Transformer·Graph 기반 TSAD 방법들은 대부분 이 틀 위에서, 더 긴 의존성·멀티변수 관계·self-supervised 학습·설명가능성 등을 추가하는 방향으로 발전하고 있다.[^1_5][^1_3][^1_1]
<span style="display:none">[^1_32][^1_33][^1_34][^1_35][^1_36][^1_37][^1_38][^1_39][^1_40][^1_41][^1_42][^1_43][^1_44][^1_45][^1_46]</span>

<div align="center">⁂</div>

[^1_1]: DeepAnT_A_Deep_Learning_Approach_for_Unsupervised_Anomaly_Detection_in_Time_Series.pdf

[^1_2]: https://doaj.org/article/7b86a483a4ea4defae1fa52bccd4164f

[^1_3]: https://arxiv.org/html/2211.05244v3

[^1_4]: https://pmc.ncbi.nlm.nih.gov/articles/PMC11723367/

[^1_5]: https://dl.acm.org/doi/10.1145/3691338

[^1_6]: https://www.dfki.de/fileadmin/user_upload/import/10175_DeepAnt.pdf

[^1_7]: https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0262463

[^1_8]: https://www.youtube.com/watch?v=M4d2lZ7Kwpk

[^1_9]: https://arxiv.org/pdf/2007.09712.pdf

[^1_10]: https://ieeexplore.ieee.org/document/9537291/

[^1_11]: https://www.semanticscholar.org/paper/6c1eeab447252f5897209ca50abc863ad7e83f9e

[^1_12]: http://arxiv.org/pdf/2110.02642v5.pdf

[^1_13]: https://arxiv.org/pdf/2305.04468.pdf

[^1_14]: https://arxiv.org/pdf/2401.11271.pdf

[^1_15]: https://arxiv.org/html/2405.11238v1

[^1_16]: https://ieeexplore.ieee.org/document/9361797/

[^1_17]: http://arxiv.org/pdf/2405.10608.pdf

[^1_18]: https://arxiv.org/pdf/2312.09478.pdf

[^1_19]: https://ieeexplore.ieee.org/document/9338317/

[^1_20]: https://www.hindawi.com/journals/complexity/2020/8846608/

[^1_21]: https://ieeexplore.ieee.org/document/9378139/

[^1_22]: https://www.mdpi.com/1424-8220/20/13/3738

[^1_23]: https://ieeexplore.ieee.org/document/9308512/

[^1_24]: https://arxiv.org/html/2602.01359v1

[^1_25]: https://www.sciencedirect.com/science/article/abs/pii/S1574013725000632

[^1_26]: https://arxiv.org/pdf/2402.09082.pdf

[^1_27]: https://onlinelibrary.wiley.com/doi/10.1155/2023/6597623

[^1_28]: https://github.com/EnsiyeTahaei/DeepAnT-Time-Series-Anomaly-Detection

[^1_29]: https://www.mathworks.com/help/predmaint/ref/deepantdetector.html

[^1_30]: https://arxiv.org/html/2412.20512v1

[^1_31]: https://arxiv.org/pdf/2204.01637.pdf

[^1_32]: https://linkinghub.elsevier.com/retrieve/pii/S0952197620302499

[^1_33]: https://www.semanticscholar.org/paper/ead0e4dd30fbd7d233db32d3383928e3f921b959

[^1_34]: https://arxiv.org/pdf/2202.03944.pdf

[^1_35]: https://arxiv.org/pdf/2211.05244.pdf

[^1_36]: https://arxiv.org/html/2412.02474v1

[^1_37]: https://arxiv.org/html/2302.02173v6

[^1_38]: https://arxiv.org/pdf/2406.08627.pdf

[^1_39]: https://arxiv.org/html/2411.05793v3

[^1_40]: https://www.semanticscholar.org/paper/Deep-Anomaly-Detection-for-Time-Series-Data-in-IoT:-Liu-Garg/64553f320cb3e9ea5285e14315380f04d4168ff4

[^1_41]: https://www.arxiv.org/pdf/2602.01359.pdf

[^1_42]: https://arxiv.org/pdf/2302.02173.pdf

[^1_43]: https://arxiv.org/pdf/2506.22837.pdf

[^1_44]: https://scienceon.kisti.re.kr/srch/selectPORSrchArticle.do?cn=NART136304754

[^1_45]: https://www.sciencedirect.com/science/article/abs/pii/S0925231224005629

[^1_46]: https://pmc.ncbi.nlm.nih.gov/articles/PMC12453818/

