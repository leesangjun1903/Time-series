# Deep Variational Graph Convolutional Recurrent Network for Multivariate Time Series Anomaly Detection

### 1. 핵심 주장과 주요 기여 요약

이 논문은 **다변량 시계열(MTS) 이상 탐지**를 위한 새로운 딥러닝 모델인 **DVGCRN (Deep Variational Graph Convolutional Recurrent Network)**을 제안합니다.[1]

**핵심 주장:**

논문의 핵심 주장은 기존 확률론적 모델들이 시간적 의존성과 채널 간 관계를 포착하면서도 **확률적 불확실성(stochasticity)**을 함께 모델링하지 못한다는 점입니다. 특히 두 가지 문제를 강조합니다:[1]

1) 이상은 시간적 변화와 관계적 특성의 변화 모두로 반영되지만, 기존 방법들은 이를 효과적으로 처리하지 못합니다.
2) 시계열의 노이즈로 인한 허위 양성(false positive) 문제를 해결하기 위해서는 더 깊은 구조가 필요합니다.

**주요 기여:**

1. **VGCRN 모델 제안**: 채널 임베딩 기반의 **임베딩 가이드 확률 생성 네트워크(DEPN)**과 **그래프 합성곱 순환 신경망(GCRN)**을 결합하여, 시간적 의존성과 채널 간 관계를 동시에 모델링합니다.[1]

2. **DVGCRN으로의 확장**: 계층적 구조를 통해 다층 정보를 포착하는 깊은 변분 네트워크로 확장합니다.[1]

3. **상향-하향 변분 추론**: 재건 손실과 예측 손실을 결합한 **상향-하향 자동인코딩 추론 기법**을 제안하여 더 정확한 후경 분포 근사를 달성합니다.[1]

***

### 2. 문제 정의, 제안 방법, 모델 구조 및 성능

#### 2.1 해결하고자 하는 문제

**문제 정의**: $$n$$-번째 MTS를 $$x_n = \{x_{1,n}, x_{2,n}, ..., x_{T,n}\} \in \mathbb{R}^{T \times V}$$로 정의합니다. 여기서 T는 시계열 길이, V는 채널 수입니다. 목표는 특정 시간 단계에서의 관측값이 이상인지 정상인지 판단하는 것입니다.[1]

**핵심 문제점:**

- 기존 동적 확률 모델들은 **명시적인 관계 정보를 생성 과정**에 포함하지 않음
- 얕은 구조로 인해 **노이즈가 많은 시계열에 대한 강건성 부족**
- 시간적 변화와 관계적 특성 변화를 동시에 포착하기 어려움

#### 2.2 제안하는 방법 (수식 포함)

**채널 임베딩**:

각 채널에 대한 임베딩 벡터를 정의하고, 이를 가우시안 분포로 모델링합니다:[1]

$$\alpha^{(l)}_i = \mathcal{N}(\hat{\mu}^{(l)}_i, \text{diag}(\hat{\sigma}^{(l)}_i))$$

여기서 $$l$$은 계층 인덱스, $$i$$는 채널 인덱스입니다.

**임베딩 가이드 확률 생성 네트워크(DEPN)**:

채널 임베딩을 통해 인자 적재 행렬(factor loading matrix)을 정의합니다:[1]

$$W^x_{z\mu} = \text{softmax}(\alpha^{(0)T}\alpha^{(1)})$$
$$z_{t,n} \sim \mathcal{N}(\mu_{t,n}, \text{diag}(\sigma_{t,n}))$$
$$\mu^x_{t,n} = f(W^x_{z\mu}z_{t,n} + W^x_{h\mu}h_{t-1,n})$$

이 방식으로 채널 간 유사성과 복잡한 관계를 포착합니다.

**그래프 합성곱 순환 신경망(GCRN)**:

데이터 적응 그래프 생성 모듈을 사용하여 은닉 의존성을 자동 추론합니다:[1]

$$A = \text{ReLU}([\alpha^{(0)}, \alpha^{(1)}]^T[\alpha^{(0)}, \alpha^{(1)}])$$
$$\tilde{A} = Q^{-1/2}AQ^{-1/2}$$
$$H^{(1)}_{t,n} = \ln(1 + \exp(W_{\tilde{A}}H^{(0)}_{t,n}))$$

**깊은 확장 - DEPN의 계층적 생성 과정**:

$$W^{(l)}_{z\mu} = \text{softmax}(\alpha^{(l-1)T}\alpha^{(l)})$$
$$\mu^{(l)}_{t,n} = f(W^{(l+1)}_{z\mu}z^{(l+1)}_{t,n} + W^{(l)}_{h\mu}h^{(l)}_{t-1,n})$$
$$z^{(l)}_{t,n} \sim \mathcal{N}(\mu^{(l)}_{t,n}, \text{diag}(\sigma^{(l)}_{t,n}))$$

**상향-하향 변분 추론**:

후경 분포의 붕괴를 방지하기 위해 상향 경로와 하향 경로를 결합합니다:[1]

$$\hat{\mu}^{(l)}_{t,n} = f(C^{(l)}_{x\mu}x_{t,n} + C^{(l)}_{h\mu}h_{t-1,n})$$
$$\mu^{(l)}_{t,n} = \text{linear}(\hat{\mu}^{(l)}_{t,n} + W^{(l)}_{z\mu}z^{(l+1)}_{t,n})$$

**최적화 목표 (ELBO)**:

$$L = \sum^N_{n=1}\left[\sum^T_{t=1}\mathbb{E}_{q(z^{(1)}_{t,n})}\left[\ln p(x_{t,n}|z^{(1)}_{t,n}, \alpha^{(0)}, \alpha^{(1)})\right] + \gamma\sum^L_{l=1}\mathbb{E}_{q(z^{(l)}_{t,n})}\left[\ln p(x_{T,n}|h^{(l)}_{1:T-1,n}, \alpha^{(l)})\right]\right.$$
$$\left.- \sum^T_{t=1}\sum^L_{l=1}\mathbb{E}_{q(z^{(1)}_{t,n})}\left[\ln\frac{q(z^{(l)}_{t,n}|x_{t,n}, h^{(l)}_{t-1,n})}{p(z^{(l)}_{t,n}|z^{(l-1)}_{t,n}, \alpha^{(l)}, \alpha^{(l-1)})}\right]\right]$$

여기서 첫 두 항은 재건 손실, 세 번째 항은 예측 손실입니다.[1]

#### 2.3 모델 구조

DVGCRN의 구조는 다음 세 가지 주요 컴포넌트로 구성됩니다:[1]

1. **Deep Embedding-guided Probabilistic Network (DEPN)**: L개 계층의 계층적 생성 과정을 통해 다층 확률 잠재 변수를 생성합니다.

2. **Stacked Graph Convolutional Recurrent Network (SGCRN)**: 각 계층에서 그래프 합성곱을 통해 관계 정보를 집계하고, 스택된 LSTM을 통해 다층 시간적 의존성을 포착합니다.

3. **Shared Channel Embeddings**: 모든 계층에서 공유되는 가우시안 채널 임베딩으로 계층 간 관계를 학습합니다.

#### 2.4 성능 향상

**정량적 성능**:

Table 1의 실험 결과에 따르면 DVGCRN은 4개 데이터셋에서 최고 또는 경쟁력 있는 F1-score를 달성합니다:[1]

- **DND**: F1-score 0.849 (DVGCRN-layer3-M)
- **SMD**: F1-score 0.915 (DVGCRN-layer3-M)
- **MSL**: F1-score 0.914 (DVGCRN-layer3-M)
- **SMAP**: F1-score 0.914 (DVGCRN-layer3-M)

기존 최고 성능 모델(SDFVAE)과 비교하여 SMD에서는 +1.2%, MSL에서는 +4.1% 향상을 보입니다.

**정성적 성능**:

- **더 안정적인 이상 점수**: DVGCRN은 정상 시점에서 더 부드러운 이상 점수를 생성하면서 이상 영역에서 명확한 스파이크를 보입니다.[1]

- **계층별 특성 학습**: 낮은 계층에서는 구체적이고 변화가 큰 특성을 학습하고, 높은 계층으로 갈수록 부드러운 특성을 학습하여 장기 시간적 의존성을 포착합니다.[1]

**제거 실험(Ablation Study)**:

Fig. 5에서 보이듯이 그래프 구조와 순환 구조 모두가 성능 향상에 기여하며, 각 컴포넌트 제거 시 성능이 저하됩니다.[1]

#### 2.5 모델의 한계

**계산 복잡성**:
- 모델의 매개변수 수가 계층 수 증가에 따라 증가합니다.[1]
- 깊은 구조로 인해 학습 시간이 증가합니다.

**하이퍼파라미터 민감성**:
- 균형 파라미터 $$\gamma$$ 선택이 중요합니다. 너무 작거나 크면 성능이 저하됩니다.[1]
- 임베딩 차원 $$d$$ 선택도 성능에 영향을 미칩니다: 너무 작으면 표현력이 제한되고, 너무 크면 과적합 위험이 증가합니다.[1]

**얕은 네트워크와의 성능 비교**:
- DVGCRN-layer2나 layer3는 기존 방법 대비 현저한 개선을 보이지만, 무조건 깊을수록 좋은 것은 아닙니다.[1]
- 데이터 특성에 따라 최적 깊이가 다릅니다.

***

### 3. 모델의 일반화 성능 향상 가능성

#### 3.1 일반화 성능 향상 메커니즘

**계층적 구조**:

DVGCRN은 다층 확률적 임베딩을 통해 일반화 성능을 향상시킵니다. Fig. 4(중앙)에 따르면 제한된 첫 계층 너비 하에서 네트워크 깊이를 증가시키면 이상 탐지 정확도가 명확히 개선됩니다. 이는 계층적 구조가 표현 및 일반화 능력을 향상시킴을 나타냅니다.[1]

**강건한 채널 임베딩**:

가우시안 분포로 모델링된 채널 임베딩은 단순 결정론적 임베딩보다 더 강건한 표현을 제공합니다. 이를 통해 노이즈가 많은 시계열에서도 좋은 성능을 유지합니다.[1]

**재건과 예측의 결합**:

식 (12)의 결합된 ELBO 목표는 재건 기반 손실과 예측 기반 손실을 모두 포함합니다. 이는 모델이 정상 패턴의 다양한 측면을 학습하게 하여 일반화 성능을 향상시킵니다. Fig. 8에서 보이듯이 적절한 $$\gamma$$ 값(균형 파라미터)을 선택하면 최고의 성능을 달성합니다.[1]

#### 3.2 다양한 데이터셋에 대한 일반화

**공개 데이터셋 검증**:

Table 1에서 DVGCRN은 네 가지 서로 다른 데이터셋(DND, SMD, MSL, SMAP)에서 모두 우수한 성능을 보입니다. 이는 모델이 다양한 특성의 시계열에 대해 높은 일반화 능력을 갖추고 있음을 의미합니다.[1]

**창 크기(T) 변화에 대한 강건성**:

Fig. 4(좌측)에 따르면 창 크기가 5에서 25로 증가하면서 F1-score가 먼저 증가한 후 안정화됩니다. 이는 모델이 다양한 시간 범위에 대해 강건함을 보여줍니다.[1]

#### 3.3 노이즈 및 허위 양성에 대한 강건성

**노이즈 시계열 처리**:

논문이 언급하는 문제점 중 하나가 "Out rate", "In rate", "Hit TTFB"와 같은 노이즈 많은 시계열로 인한 허위 양성입니다. DVGCRN의 깊은 구조와 확률적 모델링은 이러한 노이즈에 대한 강건성을 제공합니다.[1]

**계층별 표현 학습**:

Fig. 6에 따르면 낮은 계층의 잠재 표현은 구체적이고 변화가 크고, 높은 계층으로 갈수록 더 부드러워집니다. 이러한 다중 스케일 표현은 정상과 비정상을 더 정확하게 구분하는 데 도움이 됩니다.[1]

***

### 4. 향후 연구 영향 및 고려사항

#### 4.1 연구 영향

**확률론적 그래프 신경망의 활성화**:

이 논문은 확률론적 방법과 그래프 신경망을 결합하는 새로운 방향을 제시합니다. 향후 연구자들이 다른 도메인(비정상 탐지, 노드 분류, 링크 예측)에서 유사한 접근을 시도할 수 있습니다.[1]

**계층적 생성 모델의 효과성 입증**:

깊은 변분 구조의 장점을 명확히 보여줌으로써, 향후 시계열 분석 작업에서 계층적 확률 모델의 사용을 권장합니다.[1]

**상향-하향 추론 기법의 일반화**:

상향-하향 변분 추론 기법(식 10-11)은 다른 깊은 생성 모델에도 적용 가능한 기법입니다.[1]

#### 4.2 향후 연구 시 고려할 점

**1. 모델 복잡성과 해석성의 균형**:

DVGCRN은 높은 성능을 제공하지만 모델이 복잡하여 해석이 어렵습니다. 향후 연구에서는 다음을 고려해야 합니다:

- 모델 복잡성을 유지하면서 해석성을 높이는 방법 개발
- 어떤 채널이 이상 탐지 결정에 가장 큰 영향을 미치는지 설명하는 기법

**2. 계산 효율성 개선**:

Table 3에 따르면 모델의 훈련 및 테스트는 실시간 응용에 적합하지만, 더 효율적인 구현이 필요합니다:[1]

- 모델 압축 기법 적용
- 경량 버전 개발로 모바일/엣지 디바이스 배포 가능성 탐색

**3. 적응적 임계값 선택**:

현재 Peaks-Over-Threshold (POT) 방법을 사용하지만, 데이터 분포가 시간에 따라 변하는 경우 적응적 임계값 선택 방법이 필요합니다.[1]

**4. 다중 종류의 이상 구분**:

현재 모델은 이상 점수만 제공하지만, 향후 연구에서는:

- 이상의 원인을 자동으로 진단하는 기법
- 다양한 이상 유형(점 이상, 집단 이상, 맥락적 이상)을 구분하는 방법

**5. 전이 학습(Transfer Learning) 적용**:

다양한 도메인에서 사전 훈련된 모델을 다른 시계열에 적용하는 전이 학습 기법의 개발이 필요합니다. 사용자의 최근 연구 관심사인 도메인 적응과도 연결되어, 다른 분포의 시계열에 대한 모델 일반화를 향상시킬 수 있습니다.

**6. 부분 관측 및 결측치 처리**:

실제 응용에서는 일부 채널이 결측될 수 있습니다. 이를 처리하는 확률론적 접근이 필요합니다.

**7. 온라인 학습 확장**:

현재 오프라인 훈련 후 온라인 탐지 방식이지만, 개념 드리프트(concept drift)에 대응하는 점진적 온라인 학습 방법의 개발이 요구됩니다.[1]

#### 4.3 실무적 고려사항

**배포 환경의 다양성**:

- 인터넷 서비스, CDN, 발전소, 착용형 기기 등 다양한 분야에 적용 가능하지만, 각 도메인의 특수성을 반영한 최적화가 필요합니다.[1]

**라벨링 비용 감소**:

현재 비지도 학습이지만, 소량의 라벨된 데이터를 활용한 준지도 학습 접근이 성능 향상에 도움이 될 수 있습니다.

이 논문은 시계열 이상 탐지 분야에서 중요한 진전을 제시하며, 특히 **확률론적 모델링과 깊은 구조의 결합**, **채널 임베딩을 통한 관계 모델링**, **상향-하향 추론 기법** 등이 향후 연구의 기초가 될 것으로 기대됩니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/e8246cec-b0ef-4c88-b0f2-dfdfc8c31dfa/chen22x.pdf)
