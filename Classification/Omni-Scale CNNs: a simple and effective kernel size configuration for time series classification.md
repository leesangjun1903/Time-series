# Omni-Scale CNNs: a simple and effective kernel size configuration for time series classification

## 1. 핵심 주장과 주요 기여

Omni-Scale CNNs(OS-CNN)는 시간 시계열 분류(Time Series Classification, TSC)에서 가장 중요한 문제 중 하나인 **최적 Receptive Field(RF) 크기 선택 문제**를 해결하기 위해 제안된 방법입니다. 이 논문은 ICLR 2022에 발표되었으며, 다음과 같은 핵심 기여를 합니다:[1]

**주요 기여**

**OS-block(Omni-Scale block) 제안**: 데이터셋마다 수동으로 하이퍼파라미터를 탐색할 필요 없이 모든 스케일의 RF를 자동으로 커버하는 단순하면서도 효과적인 1D-CNN 블록을 제안합니다.[1]

**Goldbach의 추측 기반 설계 원리**: 모든 양의 짝수는 두 소수의 합으로 표현할 수 있다는 수학적 원리를 활용하여, 소수(prime number) 크기의 커널들을 사용함으로써 모든 정수 크기의 RF를 효율적으로 커버합니다.[1]

**SOTA 성능 달성**: 4개의 벤치마크(총 159개 데이터셋)에서 최첨단 성능을 달성했으며, 특히 MEG-TLE 데이터셋에서는 기존 최고 방법 대비 3.6%p 향상, 모델 크기는 10배 감소를 보였습니다.[1]

**일반화 능력 검증**: 의료(간질 진단), 음성 인식(Google SpeechCommands), 인간 활동 인식, 스펙트럼 분석 등 다양한 도메인에서 통일된 하이퍼파라미터로 안정적인 성능을 입증했습니다.[1]

## 2. 해결하고자 하는 문제

### 2.1 문제 정의

시간 시계열 데이터는 다양한 정보 소스(의료 센서, 경제 지표, 로그 등)와 기록 설정(샘플링 레이트, 기록 길이, 대역폭)으로 인해 본질적으로 다양한 시간 스케일의 신호들로 구성됩니다. 1D-CNN에서 RF 크기는 모델이 입력 시계열에서 "볼 수 있는" 시간 스케일을 결정하며, 이는 분류 성능에 결정적인 영향을 미칩니다.[1]

**핵심 문제점**

- UCR 85 데이터셋 통계에 따르면, RF 크기만 변경해도 대부분의 데이터셋에서 정확도가 5% 이상 변동합니다[1]
- 단일 RF 크기로는 여러 데이터셋에서 일관되게 최고 성능을 낼 수 없습니다[1]
- 기존 방법들(MCNN, TapNet 등)은 각 데이터셋마다 최적 RF 크기를 찾기 위해 비용이 많이 드는 grid search가 필요합니다[1]
- 길이 512인 데이터셋의 경우에도 71개의 서로 다른 부분 시퀀스 길이를 시도해야 했습니다[1]

### 2.2 동기가 된 두 가지 현상

저자들은 1D-CNN의 두 가지 중요한 현상을 발견했습니다:[1]

**현상 1**: RF 크기는 중요하지만, 1D-CNN은 그 크기를 구성하는 **특정 커널 구성에는 민감하지 않습니다**. 예를 들어, RF 크기가 9인 경우 $$5-5-1-1-1$$, $$5-4-2-1-1$$, $$5-3-3-1-1$$, $$5-2-2-2-2$$ 구성 모두 유사한 성능을 보입니다.[1]

**현상 2**: 다중 RF 크기를 가진 모델의 성능은 주로 그중 **최고의 RF 크기에 의해 결정**됩니다. 더 작은(성능이 낮은) RF 크기들이 추가로 있어도 성능에 큰 영향을 주지 않습니다.[1]

이 두 현상은 "모든 RF 크기를 커버하는 모델을 만들면, 최적 RF 크기를 가진 모델과 유사한 성능을 낼 수 있다"는 통찰로 이어집니다.[1]

## 3. 제안하는 방법 (수식 포함)

### 3.1 OS-block 아키텍처

OS-block은 **3-layer multi-kernel 구조**로 설계되었으며, 각 커널은 입력과 동일한 패딩 convolution을 수행합니다.[1]

**커널 크기 구성**

$$i$$번째 레이어의 커널 크기 집합 $$P(i)$$는 다음과 같이 정의됩니다:[1]

$$
P(i) = 
\begin{cases} 
\{1, 2, 3, 5, 7, ..., p_k\} & \text{if } i \in \{1, 2\} \\
\{1, 2\} & \text{if } i = 3
\end{cases}
$$

여기서 $$\{1, 2, 3, 5, 7, ..., p_k\}$$는 1부터 $$p_k$$까지의 소수 집합입니다. $$p_k$$는 시계열 길이 $$l$$에 따라 1부터 $$l$$까지의 모든 RF 크기를 커버할 수 있는 가장 작은 소수입니다.[1]

### 3.2 수학적 원리

**RF 크기 집합**

3-layer OS-block(pooling 없음, stride=1)의 RF 크기 집합 $$S$$는 모든 경로의 RF 크기 집합으로, 다음과 같이 표현됩니다:[1]

$$
S = \{p^{(1)} + p^{(2)} + p^{(3)} - 2 \mid p^{(i)} \in P(i), i \in \{1, 2, 3\}\}
$$

**Goldbach's Conjecture 활용**

$$i \in \{1, 2\}$$일 때 $$P(i)$$가 소수 리스트이므로, Goldbach의 추측에 의해 집합 $$\{p^{(1)} + p^{(2)} \mid p^{(i)} \in P(i), i \in \{1, 2\}\}$$는 모든 짝수의 집합 $$E$$입니다. 따라서:[1]

$$
S = \{e + p^{(3)} - 2 \mid p^{(3)} \in P(3), e \in E\}
$$

식 (1)을 적용하면:

$$
S = \{e \mid e \in E\} \cup \{e - 1 \mid e \in E\} \equiv \mathbb{N}^+
$$

여기서 $$\mathbb{N}^+$$는 범위 내 모든 양의 정수 집합입니다. 이는 모든 실수가 짝수이거나 홀수이고, $$E$$는 짝수 집합, $$\{e-1 \mid e \in E\}$$는 홀수 집합이기 때문입니다.[1]

**효율성 분석**

RF 크기 $$r$$까지 커버하는 모델 크기 복잡도는:[1]
- **소수 커널 사용 (OS-block)**: $$O(r^2/\log(r))$$
- **짝수/홀수 쌍 사용**: $$O(r^2)$$

따라서 소수 기반 설계가 더 효율적입니다.[1]

### 3.3 모델 구조

**OS-CNN**은 다음으로 구성됩니다:[1]
1. **OS-block**: 특징 추출
2. **Global Average Pooling**: 차원 축소
3. **Fully Connected Layer**: 분류

OS-block은 유연하게 확장 가능하며, residual connection, ensemble learning, attention, transformer, dilated convolution 등 다른 구조와 쉽게 결합할 수 있습니다.[1]

## 4. 성능 향상 및 일반화 성능

### 4.1 벤치마크 성능

**MEG-TLE (간질 진단 데이터셋)**:[1]

| 방법 | 정확도 | F1-score | 파라미터 수 |
|------|--------|----------|------------|
| MSAM (기존 최고) | 83.6% | 83.4 | 2.3M |
| ROCKET | 87.7% | 89.9 | - |
| **OS-CNN** | **91.3%** | **91.6** | **235k** |

OS-CNN은 기존 최고 대비 3.6%p 향상, 모델 크기는 10배 감소를 달성했습니다.[1]

**UEA 30 (다변량 시계열)**:[1]
- **평균 랭크**: 3.13 (최고)
- TapNet 대비 **23승 7패**
- MLSTM-FCN 대비 **23승 7패**

**UCR 85/128 (단변량 시계열)**:[1]
- UCR 85: 평균 랭크 **3.59** (ROCKET 3.64, InceptionTime 4.05)
- UCR 128: 평균 랭크 **2.02** (ROCKET 2.36, InceptionTime 2.41)
- ROCKET 대비 **44승 33패 8무** (UCR 85)

### 4.2 최적 스케일 포착 능력

OS-CNN의 정확도를 20개의 서로 다른 RF 크기를 가진 FCN 모델들과 비교한 결과:[1]

- **56% 이상의 데이터셋**에서 OS-CNN이 20개 후보 중 상위 5%(95 percentile 이상) 성능 달성
- **96% 데이터셋**에서 중간값(50 percentile) 이상 성능 달성
- 대부분의 데이터셋에서 OS-CNN의 정확도는 최고 RF 크기 모델의 정확도 범위 상단에 위치

이는 OS-block이 **데이터셋마다 최적 시간 스케일을 효과적으로 포착**함을 보여줍니다.[1]

### 4.3 일반화 성능 향상 메커니즘

**도메인 간 일반화**

OS-CNN은 통일된 하이퍼파라미터(learning rate=0.001, batch size=16, Adam optimizer)로 다양한 도메인에서 안정적인 성능을 보입니다:[1]
- 의료 데이터 (MEG 뇌파 측정, ECG 심전도)
- 인간 활동 인식 (모션 센서)
- 음성 인식 (Google SpeechCommands)
- 스펙트럼 분석
- 이미지 윤곽선 분류

**스케일 간 일반화**

시계열 길이에 따라 자동 적응:[1]
- 짧은 시계열 (길이 24)부터 긴 시계열 (길이 2709)까지 커버
- 클래스 수 2~60개의 다양한 분류 문제 해결
- 학습 데이터가 16개로 적거나 8,926개로 많은 경우 모두 처리

**특징 추출 일관성**

Class Activation Map 분석 결과, OS-CNN이 추출하는 특징은 최적 RF 크기를 가진 모델과 유사한 패턴을 보입니다:[1]
- **ScreenType 데이터셋**: FCN(10)이 최고 성능일 때, OS-CNN의 activation map이 FCN(10)과 유사
- **InsectWingbeatSound 데이터셋**: FCN(200)이 최고 성능일 때, OS-CNN의 activation map이 FCN(200)과 유사

이는 OS-block이 데이터에 따라 적응적으로 적절한 스케일에 가중치를 부여함을 시사합니다.[1]

## 5. 한계점

### 5.1 이론적 한계

**수학적 기반의 불완전성**: Goldbach의 추측은 $$4 \times 10^{14}$$까지 검증되었지만 아직 수학적으로 증명되지 않았습니다. 다만 이는 모든 가능한 시계열 데이터 길이보다 훨씬 큰 값입니다.[1]

**현상에 대한 이론적 설명 부족**: 두 가지 핵심 현상(RF 크기의 중요성과 구성의 비민감성, 최고 RF 크기의 지배성)이 왜 발생하는지에 대한 이론적 설명이 부족합니다. 저자들은 경험적 분석만 제공합니다.[1]

**최적성 보장 부재**: OS-block이 최적 스케일을 포착하는 메커니즘에 대한 수학적 보장이 없습니다.[1]

### 5.2 실용적 한계

**설계 효율성**: 소수 기반 설계가 $$O(r^2/\log(r))$$ 복잡도로 효율적이지만, 더 효율적인 대안이 있을 가능성이 있습니다.[1]

**고정된 3-layer 구조**: OS-block은 3-layer로 고정되어 있어, 더 깊은 네트워크에서의 성능과 확장성이 불명확합니다.[1]

**중요 스케일 가중치 부재**: 모든 스케일을 커버하지만 명시적으로 중요한 스케일에 더 큰 가중치를 부여하는 메커니즘이 없습니다. 이는 attention이나 adaptive 메커니즘으로 개선될 여지가 있습니다.[1]

**매우 긴 시계열**: 길이가 $$10^4$$를 초과하는 매우 긴 시계열에 대한 확장성과 계산 효율성은 명시적으로 검증되지 않았습니다.[1]

### 5.3 구조적 한계

**OS-block 최적화 구조 미탐색**: OS-block과 시너지를 낼 수 있는 고유한 구조나 기존 구조(residual, attention, transformer)의 특정 변형이 있을 수 있지만, 이는 체계적으로 탐색되지 않았습니다.[1]

**동적 적응 부재**: 모델이 학습 중에 동적으로 중요한 RF 크기를 학습하고 적응하는 메커니즘(예: adaptive receptive field)은 포함되지 않았습니다.[1]

## 6. 앞으로의 연구에 미치는 영향과 고려사항

### 6.1 연구에 미치는 영향

**하이퍼파라미터 탐색의 패러다임 전환**: OS-block은 시간 시계열 분류에서 RF 크기 탐색이라는 핵심 병목을 제거하여, 연구자들이 다른 중요한 문제(모델 아키텍처, 정규화, 손실 함수 등)에 집중할 수 있게 합니다.[1]

**범용 시계열 모델 개발 가능성**: 도메인에 관계없이 통일된 하이퍼파라미터로 작동하는 OS-block의 특성은 사전 학습된 범용 시계열 모델(foundation model) 개발의 기반이 될 수 있습니다.[1]

**다른 시간적 태스크로의 확장**: 비디오 분류, 시계열 예측, 이상 탐지 등 시간 차원이 중요한 다른 태스크에도 소수 기반 커널 설계 원리를 적용할 수 있습니다.[1]

**경량화 연구**: 10배 작은 모델 크기로 더 높은 성능을 달성한 사례는 엣지 디바이스나 실시간 애플리케이션을 위한 경량 시계열 모델 연구에 영향을 줄 것입니다.[1]

### 6.2 향후 연구 시 고려사항

**더 효율적인 커버리지 설계 탐색**: 소수 외에 모든 RF 크기를 더 적은 계산 비용으로 커버할 수 있는 수학적 원리나 구조를 탐색해야 합니다.[1]

**이론적 기반 강화**: 두 가지 핵심 현상에 대한 이론적 증명과 OS-block의 최적 스케일 포착 능력에 대한 수학적 보장을 개발해야 합니다.[1]

**동적 가중치 메커니즘 통합**: Attention이나 adaptive receptive field 기법을 결합하여, 모든 스케일을 커버하면서도 중요한 스케일에 동적으로 더 큰 가중치를 부여하는 하이브리드 방법을 개발할 수 있습니다.[1]

**최적 구조 탐색**: Neural Architecture Search(NAS)나 AutoML 기법을 활용하여 OS-block과 가장 잘 맞는 레이어 수, 채널 수, residual connection 구조 등을 체계적으로 탐색해야 합니다.[1]

**실제 응용 검증**: 의료 진단, 금융 예측, IoT 센서 모니터링 등 실제 산업 환경에서의 배포와 검증을 통해 실용성을 입증해야 합니다. 특히 실시간 처리, 메모리 제약, 불완전한 데이터 조건에서의 성능을 평가해야 합니다.[1]

**Transfer Learning 및 Meta-Learning**: OS-block의 범용성을 활용하여, 소량의 데이터로도 새로운 도메인에 빠르게 적응할 수 있는 transfer learning이나 few-shot learning 방법론을 개발할 수 있습니다.[1]

**다변량 시계열 특화 개선**: 현재 OS-block은 각 변량에 동일한 구조를 적용하는데, 변량 간 상호작용과 각 변량의 최적 스케일 차이를 고려한 개선이 필요합니다.[1]

이 논문은 간단하면서도 효과적인 설계 원리를 통해 시간 시계열 분류의 핵심 문제를 해결했으며, 향후 시계열 딥러닝 연구의 새로운 방향을 제시했다는 점에서 중요한 기여를 합니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/ce6e5e94-7bce-45fb-a4aa-9fb58874f944/2002.10061v3.pdf)
