# ShapeNet: A Shapelet-Neural Network Approach for Multivariate Time Series Classification

### 1. 핵심 주장과 주요 기여 요약[1]

**ShapeNet**은 다변량 시계열(Multivariate Time Series, MTS) 분류 문제를 해결하기 위한 새로운 샤플릿-신경망 기반 방법입니다. 논문의 핵심 주장은 다음과 같습니다:

**주요 기여:**
- **통합 임베딩 공간 구성**: 서로 다른 길이와 변수를 가진 샤플릿 후보들을 하나의 통합된 임베딩 공간으로 변환하여 비교 가능하게 만듦
- **클러스터-와이즈 트리플릿 손실 함수**: 기존 트리플릿 손실보다 개선된 손실 함수로 빠르고 안정적인 수렴을 달성
- **해석 가능한 분류**: 샤플릿의 본질적 해석 가능성을 유지하면서 높은 정확도 달성
- **신경망 기반 샤플릿 발견**: MTS 분류에서 신경망을 활용한 샤플릿 발견의 첫 사례

---

### 2. 문제 정의, 제안 방법, 모델 구조 및 성능

#### 2.1 해결하고자 하는 문제[1]

MTS 분류 기존 방법의 세 가지 주요 한계:

1. **샤플릿 후보의 이질성**: 다변량 데이터에서 샤플릿 후보는 다양한 변수와 길이를 가지고 있어 체계적인 탐색이 어려움
2. **비교 불가능성**: 서로 다른 길이의 샤플릿들을 직접 비교할 수 없음
3. **해석성 부족**: 기존 신경망 기반 방법들은 "블랙박스" 접근 방식으로 분류 결과의 설명이 어려움

#### 2.2 제안 방법: 핵심 구성 요소[1]

**A. 다중-길이 입력 희석 인과 CNN (Mdc-CNN)**

Mdc-CNN은 다음과 같은 구조로 이루어집니다:

- **희석 인과 합성곱(Dilated Causal Convolution)**: 기울기 소실 문제 없이 매우 긴 시계열의 장기 의존성을 처리할 수 있음
- **전역 최대 풀링(Global Max Pooling)**: 길이가 다른 입력을 동일한 크기로 통합
- **선형 계층**: 최종 임베딩 차원 조정

수식으로는, 입력 시계열 부분수열 $$x$$에 대해 Mdc-CNN은 함수 $$f(\cdot) \in \mathbb{R}^z$$를 학습하여 통합 임베딩 공간의 표현을 생성합니다.[1]

**B. 클러스터-와이즈 트리플릿 손실 함수**[1]

기존 트리플릿 손실의 한계를 극복하기 위해 제안된 새로운 손실 함수입니다.

앵커 샘플 $$x$$, 양성 샘플 집합 $$\mathbf{x}^+$$, 음성 샘플 집합 $$\mathbf{x}^-$$가 주어질 때:

**양성/음성 샘플 평균 거리:**

$$
D_{AP} = \frac{1}{K^+} \sum_{i=1}^{K^+} ||f(x) - f(x_i^+)||_2^2
$$

$$
D_{AN} = \frac{1}{K^-} \sum_{i=1}^{K^-} ||f(x) - f(x_i^-)||_2^2
$$

**내부 거리 (Intra-sample distances):**

```math
D_{pos} = \max_{i,j \in (1,K^+), i < j} \{||f(x_i^+) - f(x_j^+)||_2^2\}
```

```math
D_{neg} = \max_{i,j \in (1,K^-), i < j} \{||f(x_i^-) - f(x_j^-)||_2^2\}
```

$$
D_{intra} = D_{pos} + D_{neg}
$$

**최종 손실 함수:**

$$
L(f(x), f(\mathbf{x}^+), f(\mathbf{x}^-)) = \log \frac{D_{AP} + \mu}{D_{AN}} + \lambda D_{intra}
$$

여기서 $$\mu$$는 양성과 음성 샘플 간의 마진, $$\lambda$$는 하이퍼파라미터입니다.[1]

**기존 트리플릿 손실과의 차이:**
- 다중 양성 및 음성 샘플 고려
- 양성(음성) 샘플 간 거리도 고려하여 클러스터 내 응집성 증대
- 수렴 속도 및 안정성 향상 (Figure 4에서 확인 가능)[1]

**C. 다변량 샤플릿 변환 (Multivariate Shapelet Transformation, MST)**[1]

최종 샤플릿 선택을 위한 유틸리티 함수:

$$
U(f(x_i)) = \beta \cdot \frac{\log(size(f(x_i)))}{\log(\max_{i=1}^Y size(f(x_i)))} + (1-\beta) \frac{\log \sum_{j=1}^Y ||f(x_i) - f(x_j)||_2^2}{\log(\max_{i=1}^Y (\sum_{j=1}^Y ||f(x_i) - f(x_j)||_2^2))}
$$

이 함수는 두 가지 측면을 고려합니다:
- **첫 번째 항**: 클러스터 크기 (클수록 더 많은 샤플릿 후보를 대표)
- **두 번째 항**: 다른 클러스터와의 거리 (차별성)

MST는 동일 변수의 시계열과 샤플릿 간 거리만을 계산하여 형식적으로 정의됩니다.[1]

#### 2.3 모델 구조 개요[1]

ShapeNet의 전체 파이프라인은 다음과 같습니다:

1. **샤플릿 후보 생성**: 슬라이딩 윈도우를 사용하여 다양한 길이의 부분수열 생성
2. **Mdc-CNN을 통한 임베딩**: 모든 후보를 통합 임베딩 공간으로 변환
3. **클러스터-와이즈 트리플릿 손실 학습**: 비지도 학습으로 Mdc-CNN 훈련
4. **최종 샤플릿 선택**: 유틸리티 함수로 대표적이고 다양화된 샤플릿 선택
5. **MST 변환**: 모든 MTS를 저차원 표현으로 변환
6. **분류기 학습**: 선형 SVM 적용

#### 2.4 성능 향상[1]

**정확도 개선:**
- 30개 UEA MTS 데이터셋 중 **14개에서 최고 성능** 달성
- 비교 방법들 대비 "총 최고 정확도(Total best accuracy)" 약 2배 우수
- 평균 순위(Rank Mean): 2.23으로 모든 비교 방법 중 최고

**신뢰도:**
- Friedman 검정: p-value = 0.00 (통계적으로 유의미)
- Wilcoxon 검정: 대부분 기준 방법들에 대해 p < 0.05

**세부 성능 특성:**
- 1-to-1 비교에서 ShapeNet의 **29번 승리** (모든 방법 중 최고)
- 3번 동점, 0번 패배 (EDI 기준)

**수렴 안정성:**
- 클러스터-와이즈 트리플릿 손실 적용 시 기존 트리플릿 손실보다 **훨씬 빠르고 안정적인 수렴** (Figure 4)[1]

---

### 3. 일반화 성능 향상 가능성 분석[1]

#### 3.1 현재 일반화 능력의 강점

**대표성과 다양성 보장:**
- 유틸리티 함수의 설계로 **대표적 샤플릿**과 **다양화된 샤플릿** 모두 선택
- 클러스터 크기와 다른 클러스터와의 거리 측정으로 과적합 위험 감소

**안정한 임베딩 학습:**
- 클러스터-와이즈 손실 함수가 내부 응집성을 높여 **더 안정적인 임베딩 공간** 형성
- 이는 보이지 않은 데이터에 대한 더 나은 일반화를 의미

**해석성을 통한 일반화:**
- 샤플릿의 해석 가능성으로 모델이 **의미있는 패턴**에 초점
- 수치적 노이즈보다 구조적 패턴 학습

#### 3.2 일반화 개선을 위한 고려사항

**잠재적 한계:**
1. **데이터셋 편의**: 특정 데이터셋(Epiclay, HandMovementDirection)에서 상대적으로 낮은 성능 (33.8%, 27.8%)
2. **하이퍼파라미터 민감도**: $$\beta$$, $$\lambda$$, $$\mu$$ 등의 파라미터 선택이 성능에 영향
3. **다중 스케일 패턴**: 단일 길이의 샤플릿만 사용 가능할 수 있어 다중 시간 스케일 정보 손실 가능성

**향상 전략:**
- **교차 검증**: k-fold 교차 검증으로 보다 안정적인 성능 평가
- **이상치 처리**: 결측값이나 이상 데이터 처리 메커니즘 추가
- **동적 하이퍼파라미터**: 데이터 특성에 따른 적응형 파라미터 조정
- **앙상블 방법**: 여러 ShapeNet 모델 조합으로 일반화 개선

---

### 4. 실험 결과 종합[1]

#### 4.1 주요 실험 결과

| 항목 | 결과 |
|------|------|
| 최고 정확도 달성 데이터셋 | 14개 / 30개 (46.7%) |
| 평균 순위 | 2.23 (최고) |
| 통계적 유의성 | Friedman p=0.00 |
| 트리플릿 샘플링 효과 | 무작위 샘플링 대비 우수 |
| 유틸리티 기반 선택 | 무작위 선택 대비 명확히 우수 |

#### 4.2 샤플릿 수의 영향[1]

- 5~50개: 급격한 정확도 향상
- 50~100개: 안정적 성능 유지
- 100개 이상: 미세한 성능 감소 (과적합 가능성)
- **최적값: 기본 50개 설정**

---

### 5. 한계 및 미래 연구 방향[1]

#### 5.1 현재 방법의 한계

1. **결측값 처리 부재**: 실제 데이터의 결측값 대응 미흡
2. **계산 복잡도**: 다량의 샤플릿 후보 생성 및 임베딩 계산 비용
3. **매개변수 튜닝**: 여러 하이퍼파라미터의 최적화 필요
4. **극단적 클래스 불균형**: 특정 데이터셋에서 낮은 성능 (0.133 이하)

#### 5.2 앞으로의 연구 고려사항

**이론적 확장:**
- 결측값을 처리하는 강화된 Mdc-CNN 개발
- 적응형 샤플릿 길이 선택 메커니즘

**실무적 개선:**
- 실시간 스트리밍 데이터 대응
- 매개변수 자동 선택 알고리즘
- 다양한 도메인 (의료, IoT, 금융) 특화 모델

**방법론적 혁신:**
- 그래프 신경망과의 결합 (변수 간 상호작용 모델링)
- 트랜스포머 구조 통합 (장거리 의존성 강화)
- 자기 지도 학습 추가 (라벨링 데이터 의존성 감소)

***

### 6. 논문의 영향과 의의[1]

#### 6.1 학술적 기여

**새로운 연구 방향:**
- MTS 분류의 **신경망-샤플릿 하이브리드 패러다임** 확립
- **클러스터-와이즈 손실 함수**의 일반화 가능성 시사

**방법론적 우수성:**
- 기존 트리플릿 손실의 한계를 명확히 지적하고 개선
- 해석성과 성능의 **균형 달성**

#### 6.2 실무적 응용

ShapeNet의 결과는 다음 분야에서 직접 응용 가능합니다:

- **의료**: ECG, EEG 데이터 분류 (해석 가능성이 중요)
- **인간 행동 인식**: 센서 데이터 기반 활동 분류
- **이상 탐지**: 변칙적 시계열 패턴 식별
- **산업 진단**: 다중 센서 신호 기반 장비 상태 모니터링

***

### 결론

**ShapeNet**은 다변량 시계열 분류에서 **해석 가능성을 유지하면서도 최첨단 성능**을 달성한 혁신적 방법입니다. 특히 **클러스터-와이즈 트리플릿 손실 함수**는 더 일반적인 임베딩 학습 작업에도 적용 가능한 통찰력을 제공합니다. 

향후 연구에서는 결측값 처리, 계산 효율성 개선, 그리고 다양한 도메인 특화 등을 통해 실무 적용성을 더욱 높일 수 있을 것으로 예상됩니다. 특히 의료 영상 분석이나 복합 센서 시스템과 같이 **해석성이 중요한 응용 분야**에서 ShapeNet의 영향력은 지속적으로 증가할 것으로 기대됩니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/8b1bc43f-ef45-444d-ad50-742ed8fdbcf6/17018-Article-Text-20512-1-2-20210518.pdf)
