<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# 1. 당신은 AI 분야의 연구자입니다. "Multiple-Output Modelling for Multi-Step-Ahead Time Series Forecasting"의 핵심 주장과 주요 기여를 간결하게 요약하세요.

2. 이 논문이 "해결하고자 하는 문제, 제안하는 방법(수식 포함), 모델 구조, 성능 향상 및 한계"를 중심으로 자세하게 설명하세요.
3.특히 "모델의 일반화 성능 향상 가능성"과 관련된 내용을 중점적으로 다뤄 주세요.
4.요약 후에는 "해당 논문이 앞으로의 연구에 미치는 영향과 앞으로 연구 시 고려할 점"도 자세히 기술해 주세요.
“2020년 이후 관련 최신 연구 비교 분석”
수식을 LaTeX 코드로 답변 형식을 맞춰주고, LaTeX 렌더링을 위해 마크다운에 달러 기호를 사용해 주세요.

아래는 논문 전문을 기반으로 작성한 종합 분석입니다.

***

## 1. 핵심 주장 및 주요 기여

이 논문(Ben Taieb et al., Neurocomputing 2010)의 핵심 주장은 기존 단일 출력(Single-Output) 방식인 Iterated·Direct 전략이 공유하는 구조적 결함을 극복하기 위해 **다중 출력(Multiple-Output) 패러다임**이 필요하다는 것입니다.[^1_1]

**두 가지 신규 기여는 다음과 같습니다:**[^1_1]

- MISMO(Multiple-Input Several Multiple-Outputs) 방법론을 **쿼리 기반(Query-based) 기준**으로 확장한 LL-MISMO-L 알고리즘 제안
- NN3 국제 예측 경진대회의 111개 실제 시계열에 대한 Single-Output vs. Multiple-Output 전략의 광범위한 비교 평가

***

## 2. 상세 분석

### 해결하고자 하는 문제

기존 두 전략은 모두 **다중 입력-단일 출력(MISO)** 매핑을 학습합니다.[^1_1]


| 방법 | 구조적 문제 |
| :-- | :-- |
| **Iterated** | 이전 예측값을 입력으로 재사용 → **오차 누적(error propagation)** |
| **Direct** | H개 모델을 독립적으로 학습 → **예측값 간 조건부 독립성**, 시계열의 확률적 의존 구조 무시 |

### 제안 방법과 수식

**① Iterated 방법 (기준선)**

$\phi_{t+1} = f(\phi_t, \phi_{t-1}, \ldots, \phi_{t-d+1}) + w \tag{1}$

이를 H번 반복하면 $h \geq d+1$일 때:

$\hat{\phi}_{N+h} = \hat{f}(\hat{\phi}_{N+h-1}, \ldots, \hat{\phi}_{N+h-d}) \tag{2}$

[^1_1]

**② Direct 방법 (기준선)**

수평선 $h$마다 별도 모델 $f_h$를 학습:

$\phi_{t+h} = f_h(\phi_t, \phi_{t-1}, \ldots, \phi_{t-d+1}) + w, \quad h \in \{1, \ldots, H\} \tag{3}$

$\hat{\phi}_{N+h} = \hat{f}_h(\phi_N, \ldots, \phi_{N-d+1}) \tag{4}$

[^1_1]

**③ MIMO 방법 (핵심 제안)**

단 하나의 다중 출력 모델 $F$로 전체 수평선을 동시에 예측:

$\{\phi_{t+H}, \ldots, \phi_{t+1}\} = F(\phi_t, \phi_{t-1}, \ldots, \phi_{t-d+1}) + \mathbf{w} \tag{5}$

$\{\hat{\phi}_{N+H}, \ldots, \hat{\phi}_{N+1}\} = \hat{F}(\phi_N, \phi_{N-1}, \ldots, \phi_{N-d+1}) \tag{6}$

[^1_1]

**④ MISMO 방법 (핵심 제안)**

MIMO의 과도한 제약을 완화하기 위해 출력 크기 파라미터 $s$를 도입. $n = H/s$개의 모델을 학습:

$\{\phi_{t+ps}, \ldots, \phi_{t+(p-1)s+1}\} = F_p(\phi_t, \ldots, \phi_{t-d+1}) + \mathbf{w}, \quad p \in \{1, \ldots, n\} \tag{7}$

$s = 1$이면 Direct, $s = H$이면 MIMO와 동일하므로 **편향-분산 트레이드오프를 조절하는 통합 프레임워크**입니다.[^1_1]

### 모델 구조: Lazy Learning (LL)

모든 전략의 구현에는 로컬 학습 기법인 **Lazy Learning(LL)** 을 사용합니다. 예측 요청 시점에 학습을 수행하는 쿼리 기반 방식으로, 최적 이웃 수 $k$를 **LOO(Leave-One-Out)** 교차 검증으로 선택합니다.[^1_1]

상수 모델에서 LOO 오차:

$e_{LOO}(k) = \frac{1}{k} \sum_{j=1}^{k} (e_j(k))^2, \quad e_j(k) = \frac{k \cdot y_{[j]} - \hat{y}^k}{k-1} \tag{8}$

다중 출력(MIMO/MISMO)에서는 이를 수평선 H 전체로 확장:

$E_{LOO}(k) = \frac{1}{H} \sum_{h=1}^{H} (e^h_{LOO}(k))^2 \tag{9}$

$k^* = \arg\min_{k \in \{2,\ldots,K\}} E_{LOO}(k) \tag{10}$

[^1_1]

**LL-MISMO-G vs LL-MISMO-L의 핵심 차이:**

- **LL-MISMO-G**: 전체 학습셋에서 $s^*$를 전역적으로 결정 후 고정
- **LL-MISMO-L**: 쿼리 포인트마다 $s$와 $k$를 동시에 지역적으로 결정 (완전한 Lazy 방식)


### 평가 지표 (SMAPE)

$SMAPE = \frac{1}{18} \sum_{i=1}^{18} \frac{|\phi_{t+i} - \hat{\phi}_{t+i}|}{(\phi_{t+i} + \hat{\phi}_{t+i})/2} \times 100 \tag{11}$

[^1_1]

### 성능 향상

NN3 데이터셋(111개 월별 비즈니스 시계열, H=18) 실험 결과:[^1_1]


| 전략 유형 | 방법 | SMAPE* |
| :-- | :-- | :-- |
| Single-Output | LL-DIRECT | 22.57% |
| Single-Output | LL-ITER | 21.17% |
| Multiple-Output | LL-MIMO | 18.19% |
| Multiple-Output | LL-MISMO-L (조합) | 16.57% |
| **Multiple-Output** | **LL-MISMO-G (조합)** | **16.50%** |
| NN3 우승자 (Illies et al.) | ESN | 15.18% |

Multiple-Output 전략이 Single-Output 대비 약 **26~27% SMAPE 개선**을 달성했으며, NN3 상위 8개 참가팀 수준에 근접했습니다.[^1_1]

### 한계

- $s$ 파라미터 최적화에 계산 비용이 소모되며, 시계열마다 최적 $s$가 매우 다양함 (111개 중 80%가 $s=1$ 또는 $s=H$가 아님)[^1_1]
- LL의 특성상 데이터가 많아질수록 예측 시 계산량이 선형 증가하여 대규모 확장에 취약
- 유클리드 거리와 상수 커널만 사용하여 복잡한 비선형 구조 포착에 제한적
- 단변량(univariate) 시계열만 대상으로 하여 다변량 시계열에 대한 일반화 미검증

***

## 3. 일반화 성능 향상 가능성

이 논문에서 일반화(generalization) 향상을 위한 핵심 메커니즘은 **LOO 기반 자동 모델 선택**입니다.  고정된 모델 구조 대신 각 쿼리에 대해 $k^*$를 동적으로 결정함으로써 특정 데이터에 과적합되는 위험을 구조적으로 낮춥니다.[^1_1]

**일반화 성능에 영향을 미치는 세 가지 요인:**[^1_1]

- **$s$의 전역 vs. 지역 선택**: LL-MISMO-G(전역 $s$)가 LL-MISMO-L(지역 $s$)보다 일관되게 우수한데, 이는 너무 많은 파라미터를 지역적으로 결정할수록 분산(variance)이 높아지기 때문
- **조합(combination) 기준**: 단일 $s^*$를 선택하는 대신 여러 $s$ 값에서 얻은 예측을 평균하면 SMAPE* 16.50%로 최고 성능을 달성 — **앙상블 효과에 의한 분산 감소**
- **편향-분산 트레이드오프의 명시적 제어**: $s \in [1, H]$를 조정함으로써 모델 복잡도를 데이터에 맞게 적응시키는 구조 자체가 일반화 성능의 원천

111개 시계열의 최적 $s$ 분포 분석에서, $s=1$(Direct)이 최적인 경우는 15개에 불과하고 $s=H$(MIMO)가 최적인 경우는 4개에 불과했습니다.  이는 데이터 구조에 따라 의존 관계의 범위가 다양하므로, 고정된 전략보다 **적응적 다중 출력 전략이 더 나은 일반화 성능**을 보임을 시사합니다.[^1_1]

***

## 4. 미래 연구에 미치는 영향 및 고려점

### 연구에 미치는 영향

이 논문은 **MIMO/MISMO 패러다임**을 시계열 예측의 핵심 분류 체계로 확립시켰습니다.  이후 RecMO, DirMO, DirRecMO 등 다수의 변형 전략이 파생되었으며, 2024년 Stratify 프레임워크는 MIMO/Direct/Recursive를 단일 파라미터화된 공간으로 통합하는 방향으로 발전했습니다.[^1_2][^1_3]

### 2020년 이후 최신 연구 비교 분석

| 논문/모델 | 연도 | 핵심 기여 | 이 논문과의 관계 |
| :-- | :-- | :-- | :-- |
| **Stratify** (arXiv:2412.20510) | 2024 | Recursive·Direct·MIMO·MO 변형 전략을 파라미터 $\sigma$로 통합한 프레임워크 | MISMO의 $s$ 파라미터 개념을 일반화 [^1_2] |
| **Encoder-Decoder LSTM** (arXiv:2103.14250) | 2021 | 딥러닝 기반 다중 단계 예측 비교 — Bidirectional LSTM이 최고 성능 | MIMO 출력 구조를 딥러닝으로 구현 [^1_4] |
| **PatchTST / iTransformer** | 2023–24 | Patch 기반 Transformer로 장기 의존성 포착 및 Direct 예측 구조 채용 | 단일 모델로 전체 수평선 예측(MIMO 철학) [^1_5] |
| **Moirai / TimeGPT** (2024) | 2024 | 대규모 사전학습 모델(Foundation Model)로 Zero-shot 예측 일반화 | MISMO의 적응적 출력 크기를 학습 데이터 규모로 대체 [^1_6][^1_7] |
| **AEnbMIMOCQR** (Neurocomputing, 2024) | 2024 | MIMO 기반 적응형 앙상블 + 공형 예측(conformal prediction)으로 불확실성 정량화 | MISMO의 조합 기준을 확률 보정으로 확장 [^1_8] |
| **Timer-XL** (arXiv:2410.04803) | 2024 | Generative Transformer로 1D·2D 시계열 통합 예측 | 인과 자기 어텐션으로 출력 의존성을 암묵적으로 모델링 [^1_9] |

### 앞으로 연구 시 고려할 점

1. **스케일 문제 해결**: LL의 계산 복잡도는 데이터 증가에 선형 비례하므로, Approximate Nearest Neighbor나 딥러닝 기반 임베딩으로 대체할 필요가 있습니다.[^1_1]
2. **다변량 확장**: 현재 논문은 단변량만 다루는데, 다변량 MISMO에서는 출력 의존성 구조가 채널 간 상관성과 결합되므로 입력 선택 전략(Delta test의 다변량 확장)을 재설계해야 합니다.
3. **$s$ 파라미터의 학습 가능화**: 현재는 교차 검증으로 이산적 탐색을 하지만, 신경망에서 $s$에 해당하는 출력 그룹핑을 미분 가능한 방식으로 학습하면 end-to-end 최적화가 가능합니다.[^1_2]
4. **불확실성 정량화 통합**: AEnbMIMOCQR처럼 MISMO 구조에 공형 예측(Conformal Prediction)을 결합하면 구간 예측의 커버리지 보장이 가능합니다.[^1_8]
5. **비정상 시계열 처리**: 논문은 Mann-Kendall 검정 기반 추세 제거(detrending)만 수행하지만, 개념 드리프트(concept drift)나 분포 변화(distribution shift)가 있는 시계열에 대한 적응형 MISMO 전략이 필요합니다.[^1_10][^1_1]
6. **Foundation Model과의 통합**: Moirai, TimeGPT 등 대규모 사전학습 모델은 암묵적으로 MIMO 출력 구조를 채택하지만, MISMO처럼 출력 그룹 크기를 데이터에 맞게 동적으로 조절하는 메커니즘이 결여되어 있어 이를 결합하는 연구가 가치 있습니다.[^1_6][^1_7]
<span style="display:none">[^1_11][^1_12][^1_13][^1_14][^1_15][^1_16][^1_17][^1_18][^1_19][^1_20][^1_21][^1_22][^1_23][^1_24][^1_25][^1_26][^1_27][^1_28][^1_29][^1_30][^1_31][^1_32][^1_33][^1_34]</span>

<div align="center">⁂</div>

[^1_1]: Multiple_output_modeling_for_multi_step.pdf

[^1_2]: https://arxiv.org/html/2412.20510v1

[^1_3]: https://arxiv.org/html/2509.22395v1

[^1_4]: https://arxiv.org/abs/2103.14250

[^1_5]: https://peerj.com/articles/cs-3001/

[^1_6]: https://arxiv.org/html/2507.02907v1

[^1_7]: https://arxiv.org/pdf/2402.02592.pdf

[^1_8]: https://www.sciencedirect.com/science/article/pii/S0925231224012050

[^1_9]: https://arxiv.org/html/2410.04803v2

[^1_10]: https://arxiv.org/html/2502.14045v1

[^1_11]: https://arxiv.org/pdf/2412.20510.pdf

[^1_12]: https://arxiv.org/html/2601.07640v1

[^1_13]: https://arxiv.org/html/2509.15843v1

[^1_14]: https://arxiv.org/html/2503.08410v2

[^1_15]: https://arxiv.org/html/2503.20148v1

[^1_16]: https://arxiv.org/html/2402.09373v1

[^1_17]: https://arxiv.org/pdf/2503.08410.pdf

[^1_18]: https://arxiv.org/html/2408.02279v1

[^1_19]: https://arxiv.org/pdf/2410.04803.pdf

[^1_20]: https://arxiv.org/pdf/2307.01616.pdf

[^1_21]: https://arxiv.org/pdf/2306.09364.pdf

[^1_22]: https://arxiv.org/pdf/2402.16516.pdf

[^1_23]: https://arxiv.org/pdf/2109.12218.pdf

[^1_24]: http://arxiv.org/pdf/2404.14197.pdf

[^1_25]: https://arxiv.org/html/2410.14630

[^1_26]: https://arxiv.org/pdf/2509.22395.pdf

[^1_27]: https://www.sciencedirect.com/science/article/abs/pii/S0925231210001013

[^1_28]: https://pmc.ncbi.nlm.nih.gov/articles/PMC12453695/

[^1_29]: https://souhaib-bentaieb.com/publications/phd-thesis/

[^1_30]: https://www.sciencedirect.com/science/article/pii/S2665963824001040

[^1_31]: https://lib.jucs.org/article/114357/

[^1_32]: https://public.pensoft.net/items/?p=7TVeXpoqfNYT89tyrm3ifrTeG9Wv8P676JSQp%2FH2pj9hhtoybol4GF7LEbj3fxHT5Fo8esHssd8WepBmZBDZahbEH%2F96bYoga45KiCTMEQerWxsfkgx1LbDC%2FzPse%2BUY\&n=qC9NWZh9LY9D7YU3%2Bi%2ByQrzYCtG%2Fs%2BvktsGL5LO1pz9tjw%3D%3D

[^1_33]: https://secundo.tistory.com/107

[^1_34]: https://www.kjas.or.kr/journal/view.html?doi=10.5351%2FKJAS.2024.37.5.583

