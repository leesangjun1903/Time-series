# Pre-training Enhanced Spatial-temporal Graph Neural Network for Multivariate Time Series Forecasting

### 1. 핵심 주장 및 주요 기여 요약

이 논문의 **핵심 주장**은 기존 공간-시간 그래프 신경망(STGNN)이 계산 복잡도 제약으로 인해 단기 시계열 데이터(예: 과거 1시간)만 처리할 수 있다는 문제에서 출발합니다. 논문은 장기 이력 데이터로부터 시간적 패턴을 학습한 사전학습 모델이 하위 STGNN의 성능을 크게 향상시킬 수 있음을 주장합니다.[1]

**주요 기여**는 다음과 같습니다:

1. **STEP 프레임워크**: STGNN을 확장 가능한 시계열 사전학습 모델(TSFormer)로 강화하는 새로운 프레임워크를 제안합니다. TSFormer는 매우 긴 시계열(예: 2주)로부터 시간적 패턴을 효율적으로 학습하고 **세그먼트 수준의 표현(segment-level representations)**을 생성합니다.[1]

2. **TSFormer 설계**: Transformer 블록 기반의 효율적인 비지도 사전학습 모델을 설계하며, 마스크된 자동인코딩(masked autoencoding) 전략으로 학습합니다.[1]

3. **그래프 구조 학습**: TSFormer의 표현을 기반으로 의존성 그래프를 학습하는 그래프 구조 학습기를 설계합니다.[1]

### 2. 문제 정의 및 해결 방법

#### 해결 대상 문제

기존 STGNN의 두 가지 핵심 문제점을 해결합니다:

**첫 번째 문제 - 문맥 정보 부재**: 짧은 윈도우 내 데이터는 노이즈가 많아 정확한 예측이 어렵습니다. Figure 1(b)에서 보듯이 서로 다른 문맥(context)의 유사한 시계열 패턴을 구분하기 어려워 예측 오류가 발생합니다.[1]

**두 번째 문제 - 불완전한 종속성 모델링**: 시계열 간의 상관관계는 장기 데이터를 통해야 정확히 파악되는데, 단기 정보는 노이즈에 취약합니다. Figure 1(c)에서 보듯이 짧은 윈도우에서 관찰된 유사도는 신뢰할 수 없습니다.[1]

#### 제안하는 방법론

STEP 프레임워크는 두 단계로 구성됩니다:

**사전학습 단계 (Pre-training Stage)**

시계열 $$S_i \in \mathbb{R}^{T \times C}$$를 길이 $$L$$의 $$P$$개 비중복 패치로 분할합니다. 입력 임베딩은 다음과 같이 정의됩니다:[1]

$$U_i^j = W \cdot S_i^j + b$$

여기서 $$W \in \mathbb{R}^{d \times (LC)}$$, $$b \in \mathbb{R}^d$$는 학습 가능한 매개변수이고, $$U_i^j \in \mathbb{R}^d$$는 모델 입력 벡터입니다.[1]

마스킹 비율을 75%로 설정하여 도전적인 자기지도 학습 작업을 생성합니다. 인코더는 마스크되지 않은 패치에만 작동하며, 4개의 Transformer 블록으로 구성됩니다.[1]

**핵심 특징**:

- **학습 가능한 위치 인코딩**: 기존 MAE와 달리 결정론적 사인 인코딩 대신 학습 가능한 위치 임베딩을 사용하여 시계열의 주기적 특징 학습을 촉진합니다.[1]

- **비대칭 설계**: 인코더는 부분적으로 보이는 신호에만 작동하고, 디코더는 전체 신호에 가벼운 네트워크를 사용하여 계산량을 크게 줄입니다.[1]

- **재구성 손실**: 마스크된 패치에만 평균 절대 오차(MAE) 손실을 계산합니다.[1]

**예측 단계 (Forecasting Stage)**

STGNN 기반 예측에서 사전학습된 TSFormer 인코더의 표현을 활용합니다. Graph WaveNet를 기본 백엔드로 사용할 때, 다음과 같이 표현을 융합합니다:[1]

$$H_{\text{final}} = \text{SP}(H_P) + H_{gw}$$

여기서 $$\text{SP}(\cdot)$$는 의미 투영기(semantic projector)로, TSFormer의 표현을 Graph WaveNet의 의미 공간으로 변환합니다.[1]

**그래프 구조 학습**

모든 시계열의 표현을 연결하여 특징을 만듭니다:[1]

$$H_i = H_i^1 \parallel H_i^2 \cdots H_i^{P-1} \parallel H_i^P \in \mathbb{R}^{Pd}$$

이를 기반으로 k-NN 그래프 $$A_a$$를 계산하고, 다음 식으로 가장자리 확률을 계산합니다:[1]

$$\Theta_{ij} = \text{FC}(\text{relu}(\text{FC}(Z_i \parallel Z_j)))$$
$$Z_i = \text{relu}(\text{FC}(H_i)) + G_i$$

Gumbel-Softmax 재매개변수화 트릭을 적용하여 미분 가능한 이산 그래프 학습을 가능하게 합니다:[1]

$$A_{ij} = \text{softmax}((\Theta_{ij} + g)/\tau)$$

전체 손실함수는:[1]

$$L = L_{\text{regression}} + \lambda L_{\text{graph}}$$

### 3. 모델 구조 상세 설명

#### 아키텍처 개요

STEP은 두 개의 주요 구성 요소로 이루어집니다:[1]

| 구성 요소 | 역할 | 특징 |
|---------|------|------|
| TSFormer | 장기 시계열 패턴 학습 | 비지도 사전학습, 4개 인코더 층 + 1개 디코더 층 |
| Graph WaveNet | 공간-시간 예측 | 사전학습 표현과 융합하여 성능 향상 |

#### TSFormer 구조

**인코더**:
- 입력 임베딩 계층: 선형 변환으로 패치를 잠재 공간으로 변환
- 학습 가능한 위치 인코딩: 모든 패치에 순차 정보 추가
- 4개의 Transformer 블록: 언어 모델(12개 계층)이나 비전 모델(24개 계층)보다 훨씬 적음[1]

**이유**: 시계열 정보 밀도가 자연어보다 낮기 때문에 더 적은 층으로도 충분합니다.[1]

**디코더**:
- 1개의 Transformer 블록
- MLP를 통한 수치 정보 재구성
- 사전학습 단계에서만 사용[1]

#### 일반화 가능성

STEP은 거의 모든 STGNN에 확장 가능합니다. 논문에서는 Graph WaveNet뿐만 아니라 DCRNN에도 적용하여 효과를 입증했습니다. DCRNN에 적용할 때도 Equation (5)와 유사한 방식으로 표현을 seq2seq 인코더의 잠재 표현에 융합합니다.[1]

### 4. 성능 향상 분석

#### 실험 결과

세 개의 실제 교통 데이터셋(METR-LA, PEMS-BAY, PEMS04)에서 실험을 수행했습니다. 주요 성과:[1]

| 데이터셋 | 메트릭 | Horizon 3 | Horizon 6 | Horizon 12 |
|---------|--------|----------|----------|-----------|
| METR-LA | MAE | 2.61* | 2.96* | 3.37* |
| PEMS-BAY | MAE | 1.26* | 1.55* | 1.79* |
| PEMS04 | MAE | 17.34* | 18.12* | 19.27* |

*는 통계적 유의성(p < 0.05)을 나타냅니다.[1]

#### 성능 향상의 원인

**1. 장기 문맥 정보 활용**

Figure 3(a)-(c)의 분석에 따르면:[1]

- TSFormer는 유사한 패치를 정확하게 식별
- 일일 및 주간 주기성을 명확히 학습
- 노이즈에 강건한 표현 생성

**2. 개선된 위치 인코딩**

Figure 3(d)에서 보듯이, 학습 가능한 위치 임베딩은 데이터의 다중 주기성을 더 잘 반영합니다. 이는 TSFormer의 성공에 핵심적인 요소입니다.[1]

**3. 견고한 그래프 구조 학습**

장기 표현을 기반으로 계산된 k-NN 그래프는 노이즈에 덜 취약하여 더 정확한 의존성 모델링을 가능하게 합니다.[1]

#### 비교 분석

STEP vs 주요 기준선 비교 (METR-LA, Horizon 12):[1]

- Graph WaveNet: MAE 3.53 → STEP: MAE 3.37 (4.5% 개선)
- MTGNN: MAE 3.49 → STEP: MAE 3.37 (3.4% 개선)
- GTS: MAE 3.46 → STEP: MAE 3.37 (2.6% 개선)

모든 기준선에 대해 일관되게 우수한 성능을 달성합니다.[1]

### 5. 일반화 성능 및 한계

#### 일반화 성능 향상 메커니즘

**1. 다중 시각(Multi-perspective) 학습**

STEP은 세 가지 관점에서 일반화 성능을 개선합니다:[1]

- **시간적 관점**: 매우 긴 시계열(2주)로부터 주기적 패턴 학습
- **공간적 관점**: 장기 표현을 기반으로 더 정확한 종속성 그래프 학습
- **문맥적 관점**: 세그먼트 표현이 단기 입력에 풍부한 문맥 정보 제공

**2. 프리트레이닝의 이점**

사전학습 단계에서 고정된 인코더는 예측 단계에서 추가 계산 오버헤드 없이 안정적인 표현을 제공합니다. 이는 과적합을 방지하면서도 강력한 특징을 유지합니다.[1]

#### 한계 분석

**1. 데이터셋별 사전학습 필요성**

논문에서 명시적으로 각 데이터셋에 대해 별도로 사전학습을 수행했습니다. 이는 모델의 전이 학습 능력이 제한적임을 시사합니다.[1]

**2. 장기 의존성 학습의 한계**

4개의 Transformer 블록만 사용하므로, 매우 복잡한 장기 의존성은 완전히 포착하지 못할 수 있습니다.

**3. 하이퍼파라미터 민감성**

Figure 4의 절제 연구에서:

- 마스킹 비율 r: 최적값은 75% (r이 너무 작으면 자명한 작업, 너무 크면 정보 손실)
- k-NN 그래프의 k: 최적값은 10 (k가 작으면 불완전, 크면 중복)[1]

**4. 확장성 고려사항**

논문에서 사용된 최대 시계열 길이는 2주(336 패치 × 12 스텝) 수준입니다. 더 긴 기간에 대한 확장성은 불명확합니다.

**5. 노이즈 민감성**

Figure 3(c)에서 파란색 선(센서 고장이나 큰 노이즈)이 있는 시점은 다른 패치와 다릅니다. 모델이 이를 완벽히 처리하는지는 명확하지 않습니다.

#### 성능 한계의 증거

**1. 절제 연구 결과**

STEP w/o GSL은 여전히 좋은 성능을 보이므로, 그래프 구조 학습 모듈의 기여도는 세그먼트 표현보다 상대적으로 작습니다.[1]

**2. 긴 예측 지평에서의 성능 감소**

예측 지평이 길어질수록 성능 향상의 절대값이 작아집니다. Horizon 12에서의 개선이 Horizon 3의 개선보다 절대값으로 작습니다.[1]

### 6. 실제 적용 시 고려 사항

#### 효율성 분석

Figure 5-6의 효율성 연구에서:[1]

- TSFormer 사전학습: 마스킹 비율이 높을수록 빠름 (75%에서 341.5초/epoch)
- 예측 단계: 전처리를 통해 반복 계산 제거 가능 (181.3초/epoch로 단축)
- 전체 프레임워크는 단일 NVIDIA 3090 GPU에서 훈련 가능[1]

#### 실무 적용 전략

| 실무 상황 | 권장사항 | 이유 |
|---------|--------|------|
| 새로운 도메인 | 별도 사전학습 필요 | 데이터셋 이질성으로 인한 사전학습 필요성 |
| 제한된 계산 자원 | 패치 크기 L 축소 | 계산량 선형 감소 |
| 고빈도 데이터 | 마스킹 비율 75% 유지 | 실증적 최적값 |
| 불완전한 그래프 | GSL 활성화 | 노이즈 저항성 향상 |

### 7. 연구에 미치는 영향 및 향후 고려사항

#### 학문적 영향

**1. 사전학습-미세조정 패러다임의 시계열 확장**

이 논문은 자연어와 컴퓨터 비전에서 성공한 사전학습 패러다임을 시계열 예측으로 처음 효과적으로 확장했습니다.[1]

**2. 마스크된 자동인코딩의 시계열 적응**

MAE의 핵심 아이디어를 시계열에 맞게 조정하였으며, 특히:
- 높은 마스킹 비율(75%) 사용
- 패치 기반 입력 단위 도입
- 학습 가능한 위치 인코딩 필요성 입증[1]

**3. 공간-시간 그래프 신경망의 새로운 설계 원칙**

장기 정보와 단기 정보를 이원화하여 활용하는 새로운 설계 원칙을 제시했습니다.

#### 향후 연구 방향

**1. 전이 학습 개선**

현재 각 데이터셋마다 사전학습이 필요한 한계를 극복하기 위해:
- 다중 도메인 사전학습(multi-domain pre-training)
- 적응적 미세조정(adaptive fine-tuning) 기법 개발

**2. 모델 깊이 확장**

4개 인코더 블록이 최적인지 검증 필요:
- 다양한 모델 깊이 실험
- 계산-성능 트레이드오프 분석

**3. 초장기 예측(Very Long-term Forecasting)**

현재 2주 수준의 장기 정보만 활용:
- 월/년 단위 패턴 학습
- 계절성 및 트렌드 명시적 모델링

**4. 도메인 적응(Domain Adaptation)**

다양한 응용 분야로의 확장:
- 에너지 수요 예측
- 금융 시계열 예측
- 의료 데이터 분석

**5. 해석 가능성 강화**

Figure 3의 시각화 넘어서:
- 주의력 기법(attention visualization)
- 특징 중요도 분석
- 예측 불확실성 추정

**6. 비지도 표현 학습의 이론적 기초**

왜 마스크된 자동인코딩이 시계열에 효과적인지:
- 정보 이론 관점 분석
- 최적성 조건 도출

### 결론

STEP 프레임워크는 **사전학습을 통한 공간-시간 그래프 신경망의 근본적 한계 극복**이라는 중요한 문제를 해결했습니다. 단순하지만 효과적인 아이디어(장기 정보의 세그먼트 표현을 단기 모델에 주입)를 통해 일관된 성능 향상을 달성했으며, 효율성도 유지했습니다. 

특히 **학습 가능한 위치 인코딩**의 도입은 시계열의 주기적 특징을 학습하는 데 핵심적 역할을 하며, 이는 향후 시계열 기반 사전학습 모델 개발의 중요한 설계 원칙이 될 것입니다. 다만 데이터셋별 사전학습 필요성, 전이 학습 제한성, 초장기 예측으로의 확장 가능성 등은 향후 개선이 필요한 영역입니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/9a55f3f3-c69a-4586-b756-9f1c6852149d/2206.09113v2.pdf)
