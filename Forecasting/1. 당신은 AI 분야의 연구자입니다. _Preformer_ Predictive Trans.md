<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# 1. 당신은 AI 분야의 연구자입니다. "Preformer: Predictive Transformer with Multi-Scale Segment-wise Correlations for Long-Term Time Series Forecasting"의 핵심 주장과 주요 기여를 간결하게 요약하세요.

2. 이 논문이 "해결하고자 하는 문제, 제안하는 방법(수식 포함), 모델 구조, 성능 향상 및 한계"를 중심으로 자세하게 설명하세요.
3.특히 "모델의 일반화 성능 향상 가능성"과 관련된 내용을 중점적으로 다뤄 주세요.
4.요약 후에는 "해당 논문이 앞으로의 연구에 미치는 영향과 앞으로 연구 시 고려할 점"도 자세히 기술해 주세요.
“2020년 이후 관련 최신 연구 비교 분석”
수식을 LaTeX 코드로 답변 형식을 맞춰주고, LaTeX 렌더링을 위해 마크다운에 달러 기호를 사용해 주세요.

이제 논문 내용과 최신 연구 동향을 바탕으로 상세한 분석을 제공하겠습니다.[^1_1][^1_2][^1_3]

## 핵심 주장과 주요 기여

Preformer는 장기 시계열 예측(Long-Term Time Series Forecasting)을 위한 Transformer 기반 모델로, Multi-Scale Segment-Correlation (MSSC) 메커니즘을 도입하여 계산 효율성과 예측 성능을 동시에 개선합니다. 기존 Transformer 모델의 point-wise attention이 시계열 길이에 대해 $O(L^2)$의 계산 복잡도를 가지는 문제를 segment-wise correlation으로 해결하여 $O(L^2/L_0)$으로 감소시켰습니다.[^1_1]

주요 기여는 다음과 같습니다:

- **효율적인 MSSC 메커니즘**: 시계열을 세그먼트로 분할하여 상관관계 기반 attention 수행
- **Multi-scale 구조**: 다양한 시간 해상도에서 종속성 추출하여 segment length 선택 문제 완화
- **Predictive paradigm**: 디코더에서 key-value 간 1-segment delay를 도입하여 예측 종속성 명시적 모델링


## 해결하고자 하는 문제

### 기존 방법의 한계

**1. Point-wise Attention의 문제점**
기존 Transformer 기반 방법들은 point-wise self-attention을 사용하여 두 가지 근본적인 문제가 있습니다:[^1_1]

- 계산 복잡도가 시계열 길이의 제곱에 비례하여 장기 예측에 부적합
- Query, Key, Value가 모두 동일한 시점에서 변환되어 예측 종속성을 명시적으로 포착하지 못함

**2. 기존 Sparse Attention의 한계**

- Informer의 ProbSparse attention, LogTrans의 LogSparse attention은 여전히 point 단위로 작동[^1_1]
- Autoformer의 Auto-Correlation은 series-wise correlation을 사용하지만 복잡한 Fast Fourier Transform이 필요하고 전체 시계열에 대한 연산 수행[^1_1]

**3. 실제 시계열의 특성 미반영**
교통 흐름 예측 예시: 오늘 저녁의 흐름은 단일 이전 시점보다는 어제 저녁의 흐름과 더 강한 상관관계를 가짐 - 이러한 segment-level correlation이 기존 방법에서 무시됨[^1_1]

## 제안하는 방법

### Segment-Correlation 메커니즘

**기본 수식**

입력 $H \in \mathbb{R}^{L \times d}$를 segment length $L_{seg}$로 분할하여 query, key, value 세그먼트를 생성합니다:[^1_1]

$$
Q = HW_q, \quad K = HW_k, \quad V = HW_v
$$

세그먼트 간 상관관계는 다음과 같이 계산됩니다:[^1_1]

$$
c_{ij} = \text{Correlation}(Q_i, K_j) = \frac{1}{d \times L_{seg}} Q_i \odot K_j
$$

여기서 $\odot$는 동일 크기 행렬 간 내적 연산입니다.

각 query 세그먼트 $Q_i$에 대해 Softmax로 정규화된 가중치를 계산합니다:[^1_1]

$$
\hat{c}_{i1}, \hat{c}_{i2}, \ldots, \hat{c}_{in} = \text{Softmax}(c_{i1}, c_{i2}, \ldots, c_{in})
$$

출력은 value 세그먼트의 가중 합으로 계산됩니다:[^1_1]

$$
Y_i = \sum_{j=1}^{n} \hat{c}_{ij}V_j
$$

최종 Segment-Correlation 출력은 다음과 같습니다:[^1_1]

$$
\text{SC}(H; L_{seg}) = \text{Concat}(Y_1, \ldots, Y_m)
$$

### Multi-Scale Segment-Correlation (MSSC)

단일 segment length의 한계를 극복하기 위해 multi-scale 구조를 도입합니다:[^1_1]

$$
\text{MSSC}(H) = \sum_{l=0}^{l_{max}} \alpha_l \cdot \text{SC}(H; 2^l L_0)
$$

여기서 가중치는 다음과 같이 정의됩니다:[^1_1]

$$
\alpha_l = \frac{2^l}{\sum_{l=0}^{l_{max}} 2^l}, \quad l_{max} = \lfloor \log_2(\frac{L}{L_0}) \rfloor
$$

**계산 복잡도**: MSSC의 전체 계산 복잡도는 다음과 같이 유도됩니다:[^1_1]

$$
O(\text{MSSC}) = (2 - \frac{L_0}{L}) \cdot O(\frac{L^2}{L_0}) < 2 \cdot O(\frac{L^2}{L_0})
$$

이는 단일 scale Segment-Correlation의 2배 이하로, 여전히 $O(L \log L)$ 수준을 유지합니다.[^1_1]

### Predictive Paradigm (PreMSSC)

디코더에서 예측을 위한 핵심 혁신은 key와 value 간 1-segment delay입니다:[^1_1]

**기존 패러다임**:

$$
Y_i = \sum_{j=1}^{n} \hat{c}(Q_i, K_j)V_j
$$

**Predictive 패러다임**:

$$
Y_1 = \sum_{j=1}^{n-1} \hat{c}(Q_m, K_j)V_{j+1}
$$

$$
Y_i = \sum_{j=1}^{n-1} \hat{c}(Q_{i-1}, K_j)V_{j+1}, \quad i > 1
$$

이는 "현재 세그먼트 $i$의 출력 $Y_i$를 얻기 위해 이전 세그먼트 $Q_{i-1}$의 query를 사용하여 모든 key $K_j$와의 상관관계를 계산하고, 다음 세그먼트 $V_{j+1}$을 가중 합산"하는 방식입니다.[^1_1]

## 모델 구조

### 전체 아키텍처

Preformer는 vanilla Transformer와 유사한 encoder-decoder 구조를 채택하되, 다음과 같은 차이점이 있습니다:[^1_1]

**Encoder**:

- N개의 동일한 레이어로 구성 (실험에서는 N=2)
- 각 레이어: MSSC 모듈 + Feed-Forward Network
- 각 모듈 뒤에 series decomposition module과 residual connection 적용
- Decomposition은 trend와 seasonal 성분으로 분리[^1_1]

**Decoder**:

- M개의 동일한 레이어로 구성 (실험에서는 M=1)
- 추가적인 PreMSSC 모듈이 encoder 출력을 key/value로 활용
- 입력: seasonal 성분 $[X_{half}^{season}, X_0]$와 trend 성분 $[X_{half}^{trend}, X_{mean}]$[^1_1]

**입력 처리**:

- Encoder 입력: 과거 시계열 $X_{1:t_0}$와 covariates $C_{1:t_0}$ 연결
- Decoder 입력: 후반부 절반 $X_{half}$를 decompose하고 placeholder와 결합
- Embedding layer를 통해 feature dimension으로 변환[^1_1]

**손실 함수**:
Fully connected layer가 디코더 출력을 최종 예측값으로 변환하고, 실제 값과의 MSE loss $L_{MSE}$로 학습합니다.[^1_1]

## 성능 향상

### Multivariate 예측 결과

6개 데이터셋(ETTm2, Electricity, Exchange, Traffic, Weather, ILI)에서 state-of-the-art 성능 달성:[^1_1]

**ETTm2 (input-96-predict-192)**:

- MSE: 0.281 → 0.269 (4.3% 개선)

**Electricity (input-96-predict-192)**:

- MSE: 0.222 → 0.189 (14.9% 개선)

**Exchange (input-96-predict-192)**:

- MSE: 0.300 → 0.268 (10.7% 개선)

**ILI (input-24-predict-36)**:

- MSE: 3.103 → 2.793 (10% 개선)


### Univariate 예측 결과

장기 예측(prediction length ≥ 192)에서 특히 우수한 성능:[^1_1]

**ETTm2 (input-96-predict-336)**:

- MSE: 0.154 → 0.139 (9.7% 개선)

**Exchange (input-96-predict-336)**:

- MSE: 0.508 → 0.426 (16% 개선)


### Ablation Study 결과

**1. Segment-Correlation vs 다른 sparse mechanisms** (ETTh1 데이터셋):[^1_1]

- Segment-Correlation이 Auto-Correlation, Full Attention, LogSparse, ProbSparse보다 우수
- Full Attention과 LogSparse는 메모리 초과로 긴 horizon에서 실행 불가

**2. Multi-scale 구조의 영향**:[^1_1]

- Multi-scale 있음 vs 없음: ETTh1에서 상당한 성능 향상
- Segment length에 대한 민감도 크게 감소 (표준편차: ETTm2에서 1.34 → 0.47)

**3. Predictive paradigm의 영향**:[^1_1]

- 대부분의 경우 predictive paradigm이 성능 향상
- 특히 Exchange 데이터셋에서 뚜렷한 개선


### 효율성 분석

메모리 및 시간 효율성에서 MSSC가 이론적 복잡도 $O(L \log L)$를 가진 다른 방법들보다 실제로 더 효율적:[^1_1]

- 메모리: Full Attention > Auto-Correlation > ProbSparse > Segment-Correlation
- 시간: LogSparse > Auto-Correlation > ProbSparse > Segment-Correlation


## 모델의 일반화 성능 향상 가능성

### Multi-scale 구조의 일반화 효과

**1. Hyperparameter 민감도 감소**
Multi-scale 구조는 segment length 선택에 대한 민감도를 크게 줄입니다:[^1_1]

- 단일 scale: segment length에 따라 MSE 표준편차 6.16-8.53×10⁻³
- Multi-scale: 표준편차 0.47-6.7×10⁻³ (약 1/10 감소)

이는 새로운 데이터셋에 적용 시 hyperparameter tuning 부담을 줄여 일반화 성능을 향상시킵니다.[^1_1]

**2. 다양한 시간 해상도 포착**
Multi-scale 구조는 초기 segment length $L_0$부터 $2^{l_{max}}L_0$까지 지수적으로 증가하는 길이를 동시에 고려합니다:[^1_1]

- Fine-grained dependencies (작은 $L_{seg}$): 세밀한 변화 포착
- Coarse-grained dependencies (큰 $L_{seg}$): 장기 추세 포착

이를 통해 다양한 주기성과 패턴을 가진 시계열에 대한 적응력이 향상됩니다.[^1_1]

### Predictive Paradigm의 일반화 기여

**1. 물리적 인과관계 반영**
Predictive paradigm은 "현재 상태가 과거의 유사한 상태 다음에 일어난 변화와 유사할 것"이라는 직관적인 인과관계를 모델링합니다. 이는 도메인 지식과 부합하여 다양한 시계열에서 일반화 가능성을 높입니다.[^1_1]

**2. 실험적 검증**
6개의 서로 다른 특성을 가진 데이터셋에서 일관된 성능 향상:[^1_1]

- 강한 주기성 (ETTm2, Electricity): 효과적
- 약한 주기성 (Exchange): 효과적
- 노이즈가 많은 데이터 (Traffic): 효과적


### Channel-independence의 일반화 효과

논문은 직접 언급하지 않지만, multivariate와 univariate 예측 모두에서 우수한 성능을 보이며, 이는 모델이 변수 간 복잡한 관계에 과적합하지 않고 개별 시계열의 temporal pattern을 잘 학습함을 시사합니다.[^1_1]

## 한계점

### 1. 명시적으로 언급된 한계

논문에서는 구체적인 한계를 명시하지 않지만, 실험 결과에서 다음을 유추할 수 있습니다:[^1_1]

**단기 예측 성능**:

- ETTm2 univariate (input-96-predict-96): Autoformer가 우수 (MSE 0.065 vs 0.072)
- Exchange univariate (input-96-predict-96): ARIMA가 우수 (MSE 0.112 vs 0.141)

이는 Preformer가 장기 예측에 특화되어 단기 예측에서는 상대적으로 불리할 수 있음을 시사합니다.[^1_1]

### 2. 구조적 한계

**Segment-based 접근의 제약**:

- Segment 내부의 fine-grained variation은 평균화되어 일부 정보 손실 가능
- 비정상적인 spike나 anomaly 감지에는 덜 효과적일 수 있음

**Decomposition 의존성**:

- Series decomposition이 trend와 seasonal을 명확히 분리할 수 없는 시계열에서는 성능 저하 가능
- 비정상 시계열이나 regime change가 빈번한 경우 제약


### 3. 계산 효율성의 한계

Multi-scale 구조가 단일 scale의 약 2배 계산량을 요구하며, 이는 극도로 긴 시계열(예: 수백만 개 시점)에서는 여전히 부담이 될 수 있습니다.[^1_1]

## 향후 연구에 미치는 영향

### 1. Segment-based Attention의 새로운 방향 제시

Preformer는 point-wise와 series-wise의 중간인 segment-wise attention의 효과를 입증했습니다. 이는 이후 연구에 다음과 같은 영향을 미쳤습니다:[^1_1]

**PatchTST (2023)**: "A Time Series is Worth 64 Words"라는 제목으로 patching 개념을 더욱 발전시켰습니다. Preformer의 segment와 유사하게 시계열을 patch로 분할하되, channel-independence를 명시적으로 강조하여 각 변수를 독립적으로 처리합니다.[^1_2][^1_4]

**CrossFormer (2023)**: Preformer의 segment 개념을 확장하여 patch 단위에서 two-stage attention을 수행합니다:[^1_5]

- 1단계: 시간 차원에서 attention
- 2단계: 변수 차원에서 attention


### 2. Multi-scale 모델링의 중요성 강조

Preformer의 multi-scale 구조는 다양한 후속 연구에 영감을 주었습니다:[^1_1]

**Ada-MSHyper (2024)**: Adaptive Multi-Scale Hypergraph Transformer로 Preformer의 multi-scale 개념을 hypergraph 구조와 결합하여 group-wise interaction을 모델링합니다.[^1_6]

**Sentinel (2025)**: Multi-patch attention mechanism을 도입하여 patching 과정을 활용하되, multi-head splitting 과정을 대체합니다.[^1_7]

### 3. Decomposition과 Transformer 결합 패러다임

Preformer는 Autoformer와 함께 시계열 분해(decomposition)와 Transformer를 결합하는 패러다임을 확립했습니다:[^1_1]

**FEDformer (2022)**: Frequency Enhanced Decomposed Transformer로 Preformer와 유사하게 decomposition을 사용하되, 주파수 도메인에서 처리합니다. Fourier transform과 wavelet transform을 활용하여 attention 모듈을 가속화합니다.[^1_3][^1_5]

**ETSformer (2022)**: Exponential Smoothing Transformer로 고전적인 exponential smoothing 원리를 Transformer에 통합합니다.[^1_8]

### 4. iTransformer (2023)의 패러다임 전환

Preformer 이후 가장 중요한 발전 중 하나는 iTransformer입니다. Preformer가 temporal token에 대한 문제를 segment로 완화했다면, iTransformer는 근본적으로 접근을 전환합니다:[^1_9]

- **Preformer**: Temporal dimension에서 segment-wise attention
- **iTransformer**: Variable을 token으로 사용하여 전체 시간 정보를 한 번에 포착[^1_9]

이는 Preformer의 segment-based 접근의 한계를 극복하는 새로운 방향을 제시했습니다.[^1_10][^1_11]

### 5. Foundation Model 방향으로의 발전

Preformer의 일반화 가능성 향상 노력은 최근 time series foundation model 연구로 이어졌습니다:[^1_12][^1_13]

**TimePFN (2025)**: Prior-data Fitted Networks 개념을 적용하여 few-shot learning 능력을 강화합니다.[^1_12]

**TIMER-XL (2024)**: Long-context information을 효율적으로 관리하여 다양한 데이터셋에서 학습한 대규모 모델입니다.[^1_13]

## 향후 연구 시 고려할 점

### 1. Segment Length vs Patch Size의 최적화

**동적 분할 전략**:
Preformer는 고정된 segment length를 사용하지만, 시계열의 local 특성에 따라 동적으로 조정하는 연구가 필요합니다. 다음을 고려해야 합니다:[^1_1]

- 변동성이 큰 구간: 작은 segment/patch
- 안정적인 구간: 큰 segment/patch
- Attention 기반 동적 분할 학습


### 2. Multi-scale과 Multi-resolution의 통합

**주파수 도메인과의 결합**:
Preformer의 temporal multi-scale과 FEDformer의 frequency enhancement를 결합하는 연구가 유망합니다:[^1_3][^1_1]

- Time domain: Segment-wise correlation
- Frequency domain: Sparse frequency components
- 두 정보의 효과적 융합 메커니즘


### 3. Channel Dependency 모델링

**Cross-variate Attention**:
Preformer는 channel 간 상호작용을 명시적으로 모델링하지 않지만, 많은 실제 시계열은 변수 간 강한 종속성을 가집니다. 다음이 필요합니다:[^1_1]

- iTransformer의 inverted attention과 Preformer의 segment correlation 결합[^1_9]
- Selective cross-variate modeling (중요한 변수 쌍만 모델링)


### 4. Predictive Paradigm의 확장

**Multi-step Predictive Dependency**:
현재는 1-segment delay만 고려하지만, 다음과 같은 확장이 가능합니다:[^1_1]

- Variable-length delay: 데이터 특성에 따라 학습
- Multi-hop prediction: $Q_{i-k}$로 $V_{j+k}$ 예측
- Attention weight로 optimal lag 자동 결정


### 5. 비정상성(Non-stationarity) 처리

**Adaptive Normalization**:
Preformer는 series decomposition을 사용하지만, 더 강력한 비정상성 처리가 필요합니다:[^1_1]

- Instance normalization의 적응적 적용
- Regime detection 및 conditional modeling
- Distribution shift에 robust한 segment representation


### 6. Interpretability 향상

**Segment-level Explanation**:
Preformer의 segment correlation은 해석 가능성 측면에서 장점이 있지만, 더 발전시켜야 합니다:[^1_1]

- 어떤 historical segment가 예측에 기여했는지 시각화
- Predictive paradigm의 causal relationship 명시
- Attention weight의 시간적 패턴 분석


### 7. Efficiency-Performance Trade-off

**경량화 연구**:
Multi-scale 구조는 2배의 계산량을 요구하므로, 다음이 필요합니다:[^1_1]

- Progressive multi-scale: 초기 레이어는 single-scale, 후반만 multi-scale
- Pruning: 중요하지 않은 scale level 제거
- Knowledge distillation: 경량 모델로 증류


### 8. Long-horizon Forecasting의 한계 극복

**Recursive vs Direct Forecasting**:
매우 긴 horizon(예: 1440, 2880 steps)에서는 오류 누적 문제가 있습니다. 다음을 고려해야 합니다:

- Hybrid approach: 중간 horizon까지 recursive, 이후 direct
- Uncertainty quantification: 예측 불확실성 명시적 모델링
- Multi-resolution forecasting: 먼 미래는 coarse-grained prediction


### 9. 도메인 특화 적응

**Task-specific Customization**:
Preformer의 일반적 구조를 유지하면서 도메인 지식 통합:[^1_1]

- Covariates의 효과적 활용 (현재는 단순 concatenation)
- Physics-informed constraints (에너지 보존, 계절성 등)
- Hierarchical modeling (도시 → 구역 → 개별 센서)


### 10. Foundation Model로의 발전

**Pre-training 전략**:
Preformer의 segment-based representation은 pre-training에 유리합니다:

- Contrastive learning: 유사한 segment끼리 가깝게
- Masked segment modeling: 일부 segment를 mask하고 복원
- Cross-dataset transfer learning


## 2020년 이후 관련 최신 연구 비교 분석

### Transformer-based Time Series Models 타임라인

**2021-2022: Decomposition + Attention**

- **Autoformer (2021)**: Auto-correlation mechanism + series decomposition[^1_5][^1_1]
- **Preformer (2022)**: Segment correlation + multi-scale + predictive paradigm[^1_1]
- **FEDformer (2022)**: Frequency-enhanced decomposition + Fourier/Wavelet transform[^1_3][^1_5]
- **ETSformer (2022)**: Exponential smoothing attention[^1_8]

**2023: Patching Revolution**

- **PatchTST (2023)**: Patching + channel-independence, "64 words" 개념[^1_4][^1_2]
- **iTransformer (2023)**: Inverted architecture (variable as token)[^1_9]
- **CrossFormer (2023)**: Two-stage attention (time + variable)[^1_5]

**2024-2025: Advanced Architectures**

- **Ada-MSHyper (2024)**: Adaptive multi-scale hypergraph[^1_6]
- **Sentinel (2025)**: Multi-patch attention with temporal and channel attention[^1_7]
- **TimePFN (2025)**: Prior-data fitted networks for few-shot learning[^1_12]
- **HTMformer (2025)**: Hybrid time and multivariate transformer[^1_14]


### 핵심 기술 비교

| 모델 | 핵심 메커니즘 | 계산 복잡도 | 주요 장점 | 한계 |
| :-- | :-- | :-- | :-- | :-- |
| **Preformer (2022)**[^1_1] | Segment correlation + Multi-scale | $O(L^2/L_0)$ | Efficiency, predictive paradigm | 단기 예측 약함 |
| **PatchTST (2023)**[^1_2][^1_4] | Patching + Channel-independence | $O((L/P)^2)$ | 최고 정확도, 단순함 | Channel dependency 무시 |
| **iTransformer (2023)**[^1_9] | Inverted attention | $O(V^2 L)$ | 전체 temporal info 포착 | 변수 수 증가 시 비효율 |
| **FEDformer (2022)**[^1_3] | Frequency attention | $O(L \log L)$ | 주기성 강한 데이터 우수 | 주파수 해석 복잡 |
| **Informer (2021)**[^1_1] | ProbSparse attention | $O(L \log L)$ | 긴 시퀀스 처리 | Query selection overhead |

*P: patch size, V: number of variables*

### 성능 비교 (최신 벤치마크 기준)

**Electricity 데이터셋 (input-96-predict-192)**:[^1_14][^1_1]

- Preformer: MSE 0.189
- iTransformer: MSE ~0.175 (추정)
- PatchTST: MSE ~0.180 (추정)
- HTMformer (2025): 상당한 개선 보고[^1_14]

**Weather 데이터셋**:[^1_11][^1_10]

- Transformer-based (Informer, iTransformer, PatchTST): 높은 정확도
- Preformer: 경쟁력 있는 성능[^1_1]
- RNN-based (TCN, BiTCN): 단기 예측 우수[^1_10]


### 일반화 성능 측면 비교

**1. Multi-scale Approaches**

- **Preformer**: 명시적 multi-scale with exponential growth[^1_1]
- **Ada-MSHyper**: Adaptive multi-scale with hypergraph learning[^1_6]
- **TimeMixer**: Multi-resolution mixing for temporal patterns[^1_14]

**2. Channel Modeling**

- **Preformer**: Implicit through multivariate forecasting[^1_1]
- **iTransformer**: Variable-centric attention[^1_9]
- **CrossFormer**: Explicit two-stage cross-dimension attention[^1_5]
- **HTMformer**: Hybrid time and multivariate embedding[^1_14]

**3. Foundation Model Capabilities**

- **TIMER-XL (2024)**: Pre-trained on diverse datasets[^1_13]
- **TimePFN (2025)**: Few-shot learning through synthetic data[^1_12]
- **Preformer**: 일반화 가능하나 pre-training 미지원[^1_1]


### 실무 적용 관점 비교

**계산 효율성** (Training time, 상대적):[^1_14][^1_1]

1. PatchTST: 가장 빠름 (단순 구조)[^1_4]
2. Preformer: 빠름 (segment-based)[^1_1]
3. iTransformer: 중간 (변수 수에 따라 변동)[^1_9]
4. FEDformer: 느림 (FFT overhead)[^1_3]
5. Autoformer: 느림 (Auto-correlation)[^1_1]

**Hyperparameter 민감도**:[^1_14][^1_1]

- **낮음**: Preformer (multi-scale), HTMformer
- **중간**: PatchTST (patch size 중요), iTransformer
- **높음**: Informer (query selection), FEDformer (frequency components)

**도메인 적응성**:

- **금융**: iTransformer, TFT (Temporal Fusion Transformer)[^1_11]
- **에너지**: PatchTST, Preformer, HTMformer[^1_15][^1_16]
- **교통**: Preformer (segment 개념 적합), CrossFormer[^1_1]
- **기상**: Informer, PatchTST, Transformer 계열 전반[^1_10][^1_11]


### 최신 트렌드 (2024-2025)

**1. Hybrid Approaches**:[^1_14]
HTMformer는 temporal과 multivariate feature extraction을 분리하여 35.8-43.6% 성능 향상을 달성했습니다.

**2. Attention Mechanism Evolution**:[^1_7]
Sentinel은 multi-patch attention으로 multi-head splitting을 대체하여 구조적 효율성을 개선했습니다.

**3. Foundation Models**:[^1_13][^1_12]
TimePFN과 TIMER-XL은 대규모 사전 학습을 통해 few-shot 및 zero-shot forecasting 능력을 향상시켰습니다.

**4. Cross-Modality Learning**:[^1_17]
CC-Time은 시계열 데이터와 텍스트 등 다중 모달리티 지식을 활용하는 방향을 제시했습니다.

**5. Interpretability Focus**:
최근 연구들은 정확도뿐 아니라 해석 가능성과 신뢰성을 강조하는 추세입니다.[^1_18][^1_19]

## 결론

Preformer는 2022년 segment-wise correlation과 predictive paradigm을 도입하여 장기 시계열 예측의 새로운 방향을 제시했습니다. Multi-scale 구조를 통한 일반화 성능 향상, 계산 효율성, 직관적인 예측 메커니즘은 이후 PatchTST, iTransformer 등의 발전에 영감을 주었습니다.[^1_2][^1_9][^1_1]

향후 연구는 (1) 동적 segment/patch 분할, (2) cross-variate modeling 강화, (3) foundation model로의 확장, (4) 비정상성 robust 처리, (5) 해석 가능성 향상에 초점을 맞춰야 합니다. 특히 2024-2025년의 HTMformer, TimePFN, Sentinel 등은 Preformer의 핵심 아이디어를 계승하면서도 새로운 차원의 발전을 보여주고 있습니다.[^1_7][^1_12][^1_14]
<span style="display:none">[^1_20][^1_21][^1_22][^1_23][^1_24][^1_25][^1_26][^1_27][^1_28][^1_29][^1_30][^1_31][^1_32][^1_33][^1_34][^1_35][^1_36][^1_37][^1_38][^1_39]</span>

<div align="center">⁂</div>

[^1_1]: 2202.11356v1.pdf

[^1_2]: http://arxiv.org/pdf/2211.14730v2.pdf

[^1_3]: https://proceedings.mlr.press/v162/zhou22g.html

[^1_4]: https://github.com/yuqinie98/PatchTST

[^1_5]: https://blog.csdn.net/AuGuSt_81/article/details/139782137

[^1_6]: http://arxiv.org/pdf/2410.23992.pdf

[^1_7]: http://arxiv.org/pdf/2503.17658.pdf

[^1_8]: https://arxiv.org/pdf/2202.01381.pdf

[^1_9]: https://www.datasciencewithmarco.com/blog/itransformer-the-latest-breakthrough-in-time-series-forecasting

[^1_10]: https://pmc.ncbi.nlm.nih.gov/articles/PMC12453695/

[^1_11]: https://peerj.com/articles/cs-3001/

[^1_12]: https://arxiv.org/pdf/2502.16294.pdf

[^1_13]: https://arxiv.org/html/2507.02907v1

[^1_14]: https://arxiv.org/html/2510.07084v1

[^1_15]: https://arxiv.org/html/2501.08620v1

[^1_16]: https://arxiv.org/html/2501.08620v3

[^1_17]: https://arxiv.org/html/2508.12235v1

[^1_18]: https://icml.cc/virtual/2025/poster/44262

[^1_19]: https://openreview.net/forum?id=kHEVCfES4Q\&noteId=mrNbq9EkQa

[^1_20]: https://journal.astanait.edu.kz/index.php/ojs/article/view/913

[^1_21]: https://rjor.ro/considerations-on-the-efficiency-of-time-series-analysis-in-forecasting-new-influenza-cases-in-the-2024-2025-season/

[^1_22]: https://dl.acm.org/doi/10.1145/3757749.3757774

[^1_23]: https://dl.acm.org/doi/10.1145/3785706.3785719

[^1_24]: https://www.mdpi.com/2073-4433/16/3/292

[^1_25]: https://journal.unpacti.ac.id/index.php/pjphsr/article/view/2114

[^1_26]: https://www.semanticscholar.org/paper/7ae7d3fd2464d8ed14599df54c1fa8f1e6842f31

[^1_27]: https://jurnal.polibatam.ac.id/index.php/JAIC/article/view/8840

[^1_28]: https://ojs.revistagesec.org.br/secretariado/article/view/5361

[^1_29]: https://www.frontiersin.org/articles/10.3389/fmed.2025.1582277/full

[^1_30]: https://arxiv.org/pdf/2502.13721.pdf

[^1_31]: http://arxiv.org/pdf/2410.12184.pdf

[^1_32]: https://arxiv.org/pdf/2206.05495.pdf

[^1_33]: https://arxiv.org/html/2508.16641v1

[^1_34]: https://arxiv.org/html/2602.00589v1

[^1_35]: https://arxiv.org/pdf/2412.13769.pdf

[^1_36]: https://arxiv.org/html/2509.04782v1

[^1_37]: https://arxiv.org/pdf/2402.07570.pdf

[^1_38]: https://aihorizonforecast.substack.com/p/influential-time-series-forecasting-8c3

[^1_39]: https://www.sciencedirect.com/science/article/pii/S2665963824001040

