# Expand and Compress: Exploring Tuning Principles for Continual Spatio-Temporal Graph Forecasting

### 1. 핵심 주장 및 주요 기여 (요약)

**논문의 핵심 주장**: 실시간으로 확장되는 센서 네트워크에서 시공간 그래프 예측을 효율적으로 수행하기 위해, **프롬프트 기반 지속적 학습 방법(EAC)**을 제안합니다. 기존 방법들이 재학습의 비효율성과 재앙적 망각(catastrophic forgetting) 문제로 고통받는 반면, EAC는 **"Expand(확장)"과 "Compress(압축)"** 두 가지 근본적인 튜닝 원칙에 기반하여 이 문제들을 동시에 해결합니다.

**5가지 주요 기여**:
1. **간단성**: 프롬프트 파라미터 풀 튜닝만으로 복잡한 지속적 학습 실현
2. **효과성**: 교통, 날씨, 에너지 등 다양한 실제 데이터셋에서 3-8% 성능 개선
3. **보편성**: 6가지 서로 다른 STGNN 아키텍처 조합에서 일관된 성능 향상
4. **효율성**: 학습 속도 2-3배 가속, 파라미터 50-60% 감소
5. **경량성**: 제한된 수의 튜닝 파라미터만 필요

***

### 2. 해결하고자 하는 문제, 제안 방법, 모델 구조

#### 2.1 문제 정의

동적 스트리밍 시공간 그래프에서의 연속 예측 문제를 다음과 같이 정의합니다:

$$f_\theta^{(\tau)*} = \arg\min_{\theta^{(\tau)}} \mathbb{E}_{D_\tau \sim P^{(\tau)}} [L(f_{\theta^{(\tau)}}(G_\tau, X_\tau), Y_\tau)]$$

여기서 $G_\tau = (V_\tau, E_\tau, A_\tau)$는 $\tau$-번째 기간의 동적 그래프, $X_\tau \in \mathbb{R}^{n \times t \times c}$는 시공간 데이터입니다.

**구체적 문제점**:
- **재학습 비효율성**: 새로운 센서 추가 시마다 전체 모델을 처음부터 재학습
- **재앙적 망각**: 새 데이터 학습 시 과거 지식의 급격한 손실
- **네트워크 확장**: 노드 수 증가에 따른 파라미터 팽창
- **분포 변화**: 서로 다른 시간 기간 간 데이터 분포의 차이

#### 2.2 Expand 원칙: 이질성 기반 프롬프트 풀 성장

**경험적 관찰**:
Average Node Deviation(AND) 메트릭을 통해 프롬프트 주입 후 노드 특성 공간의 분산이 지속적으로 확대됨을 확인:

$$D(X) = \frac{1}{n \times n} \sum_{i=1}^{n} \sum_{j=1}^{n} \sum_{k=1}^{d} (X_{ik} - X_{jk})^2$$

**이론적 분석 (명제 1)**:

원본 노드 특성 행렬 $X = [x_1, \cdots, x_n] \in \mathbb{R}^{n \times d}$에 프롬프트 파라미터 행렬 $P = [p_1, \cdots, p_n] \in \mathbb{R}^{n \times d}$를 도입하고, 불변성을 갖는 시공간 학습함수 $f_\theta$를 통해 새로운 특성 행렬 $X^\theta = f(\theta; X, P)$를 얻으면:

$$D(X^\theta) - D(X) = 2\left(\frac{1}{n}\sum_{i=1}^{n} \|p_i^\theta\|^2 - \|\mu_p^\theta\|^2\right) \geq 0$$

여기서 $\mu_p^\theta = \frac{1}{n}\sum_{i=1}^{n} p_i^\theta$는 최적화된 파라미터 행렬의 평균 벡터입니다. Cauchy-Schwarz 부등식의 따름정리로 이 부등식이 항상 0 이상임이 보증되므로, **프롬프트 파라미터가 노드 이질성을 캡처함으로써 표현 공간을 확장**함을 의미합니다.

**구현 방식**:
- 초기 단계: 모든 노드에 학습 가능한 파라미터 벡터 제공 → $P^{(1)}$
- 이후 기간 $\tau$: 새로 추가된 노드에만 프롬프트 파라미터 벡터 추가 → $P^{(\tau)}$
- 결합: 원소별 덧셈 $X'\_\tau = X_\tau + P$

#### 2.3 Compress 원칙: 저순위 속성 기반 프롬프트 풀 축소

**경험적 관찰**:
특이값 분해(SVD) 분석 결과 프롬프트 파라미터 행렬이 **명확한 저순위 구조**를 보임:
- 모든 연도에서 긴꼬리 스펙트럼 분포 관찰
- 6번째 최대 특이값에서 누적 비율이 0.75 이상 → 정보가 소수의 주성분에 집중

**이론적 분석 (명제 2)**:

노드 프롬프트 파라미터 행렬 $P \in \mathbb{R}^{n \times d}$에 대해 다음을 만족하는 행렬 $A \in \mathbb{R}^{n \times k}$와 $B \in \mathbb{R}^{k \times d}$가 존재합니다:

$$\Pr(\|P - AB\|_F \leq \epsilon \|P\|_F) \geq 1 - o(1), \quad k = O(\log(\min(n,d)))$$

여기서 $o(1)$은 $n$이 증가함에 따라 무시할 수 있는 항입니다.

**증명 개요**: 무작위 행렬 $\Phi \in \mathbb{R}^{k \times n}$을 표준 정규분포에서 샘플링하여 $A = \Phi^\top$, $B = \Phi P$로 정의하면, 농도 부등식(concentration inequality)을 이용하여 증명됩니다. 이는 **고차원 프롬프트를 저차원 두 성분의 곱으로 효과적으로 압축**할 수 있음을 보장합니다.

**구현 방식**:
- 초기 단계: $P^{(1)} \approx A^{(1)}B$ (서브스페이스 파라미터와 조정 파라미터의 곱)
- 이후 기간 $\tau$: 새 노드에만 $A^{(\tau)}$ 추가, $B$는 고정
- 효과: 파라미터 크기가 선형 증가 대신 로그 증가

#### 2.4 EAC 알고리즘 (Algorithm 1)

```
입력: 동적 스트리밍 시공간 그래프 G = (G_1, ..., G_T)
      관찰 데이터 X = (X_1, ..., X_T)
출력: 메모리에 저장된 프롬프트 파라미터 풀 P

1. while 스트림 그래프 G가 남아있음 do
2.    if τ == 1 then
3.       초기 프롬프트 풀: P = A^{(τ)}B  [압축 원칙]
4.       X'_τ = X_τ + P  [원소별 덧셈]
5.       백본 STGNN f_θ와 P를 공동 최적화
6.    else
7.       프롬프트 풀 P와 모델 f_θ* 재로드
8.       새 노드 감지 및 A^{(τ)} 생성
9.       P = P.append(A^{(τ)}B)  [확장 원칙]
10.      X'_τ = X_τ + P
11.      고정된 백본으로 P만 최적화  [재앙적 망각 회피]
12.   end if
13.   현재 기간 예측 수행
14. end while
15. return 프롬프트 파라미터 풀 P
```

#### 2.5 모델 구조

**핵심 아키텍처**:

```
시간 흐름 (Period #1 → #2 → ... → #N)
    ↓
[데이터 입력]
    ↓
[프롬프트 결합] ← P (Expand/Compress로 동적 조정)
    ↓
[동결된 STGNN 백본] (재앙적 망각 회피)
    ↓
[결과 예측]
```

**STGNN 백본**: 6가지 조합 지원
- 그래프 연산자: Spectral-based 또는 Spatial-based
- 순차 연산자: Recurrent-based, Convolution-based, Attention-based

**프롬프트 파라미터 풀 구조**:
- 초기: $P^{(1)} = A^{(1)} \times B$ (크기: $n \times d$)
- 누적: $P = [P^{(1)}, P^{(2)}, \ldots, P^{(\tau)}]$ (총 크기 제어)
- 저순위 특성으로 파라미터 폭발 방지

***

### 3. 성능 향상 및 한계

#### 3.1 성능 개선 결과

**PEMS-Stream 교통 데이터셋** (노드: 655→871, 2011-2017년):

| 메트릭 | 기존 방법(STKEC) | EAC | 개선율 |
|--------|-----------------|-----|--------|
| MAE | 14.14 | 13.53 | **-4.31%** |
| RMSE | 23.04 | 21.77 | **-5.51%** |
| MAPE(%) | 19.53 | 18.98 | **-2.81%** |

**Air-Stream 날씨 데이터셋** (노드: 1087→1202, 2016-2019년):
- MAE: 20.75 (STKEC 대비 -1.75%)
- 모든 시간 범위(3, 6, 12단계)에서 일관된 개선

**Energy-Stream 에너지 데이터셋** (노드: 103→134, 245일):
- MAE: 5.10 (Retrain-ST 대비 -4.33%)
- 가장 큰 상대 개선 달성

**보편성 검증** (6가지 STGNN 조합):
- 모든 조합에서 1-6% 개선
- Spectral-based 방법에서 특히 큰 효과 (최대 -6.48%)

**효율성 메트릭**:
- 파라미터 감소: 약 50-60% (7.1E3 → 2.3E3에서 EAC-Efficient)
- 학습 시간: 2-3배 가속 (작은 데이터셋에서 최대 3배)
- Few-shot 성능: 20% 데이터로도 우수한 성능 유지

#### 3.2 모델의 한계

**방법론적 한계**:
1. **조정 파라미터 B의 고정성**: 모든 기간에서 동일한 $B$를 사용하므로, 극단적 분포 변화에서는 비적응적
2. **노드별 일정한 프롬프트 차원**: 모든 노드가 동일 차원 $k$를 사용하여 노드의 역동성 차이 미반영
3. **이질성 측정 제약**: Average Node Deviation이 유클리드 거리 기반이므로 비선형 이질성 포착 제약

**실험적 한계**:
1. **비교 기준의 제약**: PECPM, TFMoE는 공식 코드 미제공으로 재현 결과 사용
2. **데이터셋 범위**: 3개 도메인, 최대 7기간만 테스트
3. **에너지 데이터 특이성**: 터빈 셧다운 기간으로 인해 MAPE 매우 높음

***

### 4. 모델의 일반화 성능 향상 가능성

#### 4.1 일반화 성능 향상의 이론적 근거

**메커니즘 1: 노드 이질성 포착 (명제 1)**

$$D(X^\theta) - D(X) = 2\left(\frac{1}{n}\sum_{i=1}^{n} \|p_i^\theta\|^2 - \|\mu_p^\theta\|^2\right) \geq 0$$

프롬프트가 노드 간 다양성을 증가시키므로, **새로운 노드 추가 시에도 더 나은 표현이 가능**해져 보이지 않는 데이터(새 노드)에 대한 일반화 능력 향상.

**메커니즘 2: 저순위 정규화 효과 (명제 2)**

$$\Pr(\|P - AB\|_F \leq \epsilon \|P\|_F) \geq 1 - o(1)$$

저순위 압축이 본질적으로 정규화 역할을 하므로:
- 높은 주성분만 유지 → 노이즈 감소
- 파라미터 수 제한 → 과적합 방지
- **결과**: 보이지 않는 분포에 대한 향상된 일반화

**메커니즘 3: 점진적 적응 (기간별 프롬프트 축적)**

기간 $\tau = 1, 2, \ldots, T$에서 순차적으로 프롬프트 추가:
- 초기 기간: 기본 시공간 패턴 학습 ($P^{(1)}$)
- 중간 기간: 변화 패턴 추가 학습 ($P^{(2)}, P^{(3)}, \ldots$)
- 최종 기간: 최신 트렌드 반영

이는 **다양한 시간적 패턴의 앙상블** 효과를 제공하여 장기 일반화 능력 향상.

#### 4.2 실증적 증거

1. **다도메인 일관성**: 교통, 날씨, 에너지 3개 도메인 모두에서 개선
2. **다양한 아키텍처 보편성**: 6가지 STGNN 조합에서 모두 성능 향상
3. **제한된 데이터 강건성**: 20% 데이터만 사용 시에도 상대적 강건성 유지
4. **장기 예측 안정성**: 기간 연장에 따른 성능 저하가 완만

#### 4.3 일반화 성능의 한계

- **노드 감소 시나리오 미처리**: 현재는 노드 확장만 처리
- **극단적 분포 변화**: $B$의 고정성으로 인해 급격한 변화 대응 제한
- **노드별 차별화 부재**: 모든 노드에 동일한 프롬프트 차원 적용

***

### 5. 2020년 이후 관련 최신 연구 비교 분석

#### 5.1 지속적 학습 기반 시공간 예측 방법

| 방법 | 연도 | 동적그래프 | 프롬프트 | 이론분석 | CF완화 | 효율성 |
|-----|------|---------|--------|--------|-------|-------|
| **TrafficStream** | 2021 | ✓ | ✗ | ✗ | 부분 | 낮음 |
| **STKEC** | 2023 | ✓ | ✗ | ✗ | 부분 | 낮음 |
| **PECPM** | 2023 | ✓ | ✗ | ✗ | 부분 | 낮음 |
| **TFMoE** | 2024 | ✓ | ✗ | ✗ | 부분 | 중간 |
| **PromptST** | 2024 | ✗ | ✓ | ✗ | ✓ | 높음 |
| **TProG** | 2024 | ✗ | ✓ | ✗ | ✓ | 높음 |
| **FlashST** | 2024 | ✗ | ✓ | ✗ | ✓ | 높음 |
| **EAC (제안)** | **2025** | **✓** | **✓** | **✓** | **✓** | **매우높음** |

#### 5.2 주요 경쟁 방법과의 비교

**TrafficStream (2021)** vs EAC:
- TrafficStream: 정규화 + 재현 기반, 전체 모델 튜닝 필요, 재앙적 망각 부분 완화
- EAC: 백본 동결 + 프롬프트 풀, 재앙적 망각 근본 해결, 효율성 2-3배 향상

**STKEC/PECPM (2023)** vs EAC:
- STKEC/PECPM: 복잡한 패턴 인식 메커니즘, 전체 모델 튜닝
- EAC: 단순한 프롬프트 기반, 이론적 근거 제공, 성능은 비슷 또는 우수(-3.90%)

**PromptST (2024)** vs EAC:
- PromptST: 정적 그래프 가정
- EAC: 동적 노드 확장 시나리오 명시 처리, 두 가지 명제로 이론화

**TProG (2024)** vs EAC:
- TProG: 시간 상호작용 그래프 중심
- EAC: 시공간 그래프 + 센서 네트워크 확장 중심, expand/compress 원칙 신규 제시

#### 5.3 재앙적 망각 관련 최신 연구

- **Benchmarking Catastrophic Forgetting (Hallak & Kem, 2025)**: 연합 학습 환경에서 시간 시리즈 CF 벤치마킹
- **Are TSFMs Susceptible to CF? (Karaouli et al., 2025)**: TimesFM 기초 모델의 CF 취약성 입증
- **EAC의 기여**: 명시적 이론(명제 1, 2)으로 CF 원인 분석 및 원칙적 해결

#### 5.4 저순위 방법론과의 관계

- **기존 접근**: Tensor Decomposition을 원본 시공간 데이터에만 적용
- **EAC의 신규성**: **프롬프트 파라미터에 저순위 구조 적용**, Random Matrix Theory 기반 이론화
- **LoRA 비교**: LoRA 기반 방법 대비 EAC가 +8% 성능 향상 (공간시간 특화로)

***

### 6. 앞으로의 연구에 미치는 영향 및 고려사항

#### 6.1 기여의 영향

**이론적 기여**:
1. **이질성 메커니즘 규명** (명제 1): 노드별 프롬프트의 이론적 정당성 제공
   - 기존: 경험적 관찰만 존재
   - 이제: 수학적 보증으로 설계 원칙화
   - 파급: 다른 노드 기반 표현학습에 적용 가능

2. **저순위 압축 원칙** (명제 2): Random Matrix Theory 응용
   - 파라미터 팽창 문제의 근본 해결 기초 제공
   - 다른 연속 학습 분야에도 확장 가능

**방법론적 기여**:
- **Expand & Compress 패러다임**: 이제까지 정성적 직관을 구체적 구현으로 전환
- **백본 동결 + 프롬프트 튜닝**: 대규모 모델 시대의 표준 패러다임 제시

**실무적 기여**:
- **경량성**: 파라미터 50-60% 감소로 엣지 디바이스 배포 가능
- **효율성**: 학습 시간 2-3배 단축으로 실시간 적응 가능
- **다중 도메인**: 교통/날씨/에너지 모두에서 입증된 효과

#### 6.2 향후 권장 연구 방향

**1. 기초 모델 사전학습 ("Foundation Model for Spatio-Temporal Data")**
- 다양한 도메인의 광대한 데이터에서 사전학습
- EAC를 지속적 미세조정 기초로 사용
- 기대효과: 더 나은 일반화, 빠른 적응

**2. 노드 감소 시나리오 ("Bidirectional Continual Learning")**
- 센서 고장, 폐지, 이전 등 노드 감소 처리
- 프롬프트 병합(merging) 기법 개발
- 현실 적용 범위 확대

**3. 적응적 프롬프트 차원 ("Node-Adaptive Prompt Dimensionality")**
- 노드별 역동성에 따른 개별 차원 설정
- 예: 교통에서 주요 도로는 $k_i=8$, 소도로는 $k_i=2$
- 추가 10-20% 파라미터 효율성 향상

**4. 다중 작업 지속적 학습 ("Multi-Task Continual ST Forecasting")**
- 여러 작업(교통+공기질+에너지) 동시 처리
- 작업별 프롬프트 풀 유지 및 간섭 관리

**5. 불확실성 정량화 ("Uncertainty Quantification in Continual ST")**
- 예측 신뢰도 평가
- Bayesian 프롬프트, Ensemble 프롬프트 기법
- 위험 민감 응용(의료, 금융) 지원

**6. 연합학습 통합 ("Federated Continual Spatio-Temporal Learning")**
- 분산 센서가 각각 EAC 실행, 프롬프트 공유
- 프라이버시 보호 + 통신 효율성

**7. 계절성 모델링 ("Seasonal Prompt Pools")**
- 계절별, 요일별 분리된 프롬프트 풀
- 계절 변화에 더 민감한 적응

#### 6.3 산업 응용 분야

- **스마트 도시**: 신규 도로/신호등 추가 시 교통 예측 실시간 조정
- **금융**: 고빈도 거래에서 새로운 자산 추가 시 가격 예측 동적 조정
- **헬스케어**: 변이 바이러스 추가 시 전염병 확산 예측 실시간 갱신
- **제조업**: 신규 생산 라인 추가 시 유지보수 예측 자동 조정

#### 6.4 연구 수행 시 고려사항

**데이터 관점**:
- 5개 이상 도메인 테스트 (현재 3개)
- 다양한 그래프 크기 및 확장 속도 검토
- 결측치 및 이상치 처리 전략 수립

**방법론 관점**:
- 명제 1, 2의 상한/하한 분석
- 점근적 수렴성 증명
- 일반화 오류 경계 도출

**평가 관점**:
- 예측 성능 + 계산 비용 + 메모리 사용 동시 평가
- 냉시작(cold-start) 성능 측정
- 최소 10-20 기간 장기 시뮬레이션

***

### 결론

**EAC는 다음의 혁신적 기여를 제시합니다:**

1. **이론적 엄밀성**: 명제 1, 2로 expand/compress 원칙을 수학적으로 정당화
2. **실용적 효율성**: 파라미터 50-60% 감소, 학습 시간 2-3배 단축
3. **보편적 효과**: 3개 도메인, 6가지 아키텍처에서 일관된 3-8% 성능 개선
4. **근본적 문제 해결**: 백본 동결로 재앙적 망각을 원천적으로 차단

**시공간 예측의 지속적 학습 분야에서**, 단순한 기술(프롬프트 풀)로도 복잡한 문제(재앙적 망각, 효율성)를 동시에 해결할 수 있음을 입증했습니다. 이는 **이론과 실험의 조화가 강력한 설계 원칙을 도출**할 수 있음을 보여주는 모범 사례입니다. 향후 기초 모델 사전학습, 노드 감소 시나리오, 다중 작업 학습 등 다양한 확장 연구가 활발할 것으로 예상됩니다.

<span style="display:none">[^1_1][^1_10][^1_11][^1_12][^1_13][^1_14][^1_15][^1_16][^1_17][^1_18][^1_19][^1_2][^1_20][^1_21][^1_22][^1_23][^1_24][^1_25][^1_26][^1_27][^1_28][^1_29][^1_3][^1_30][^1_31][^1_32][^1_33][^1_34][^1_35][^1_36][^1_37][^1_38][^1_39][^1_4][^1_40][^1_41][^1_42][^1_43][^1_44][^1_45][^1_46][^1_5][^1_6][^1_7][^1_8][^1_9]</span>

<div align="center">⁂</div>

[^1_1]: 2410.12593v3.pdf

[^1_2]: https://link.springer.com/10.1007/s40866-025-00260-6

[^1_3]: https://link.springer.com/10.1007/s13762-025-06850-2

[^1_4]: https://www.ijircst.org/view_abstract.php?title=RfGANNNet-2.0:-A-Hybrid-AI-Framework-Integrating-Random-Forests,-Spatio-Temporal-Graph-Convolution,-and-Physics-Guided-GANs-for-High-Resolution-Rainfall-Forecasting\&year=2025\&vol=13\&primary=QVJULTEzNzk=

[^1_5]: https://azojete.com.ng/index.php/azojete/article/view/1121

[^1_6]: https://dl.acm.org/doi/10.1145/3748777.3748812

[^1_7]: https://ieeexplore.ieee.org/document/10981815/

[^1_8]: https://link.springer.com/10.1007/s41324-025-00622-3

[^1_9]: https://www.mdpi.com/1424-8220/25/17/5560

[^1_10]: https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0012394400003660

[^1_11]: https://link.springer.com/10.1007/s00477-025-03123-9

[^1_12]: http://arxiv.org/pdf/2503.04528.pdf

[^1_13]: http://arxiv.org/pdf/2410.12593.pdf

[^1_14]: http://arxiv.org/pdf/2211.12509.pdf

[^1_15]: http://arxiv.org/pdf/2301.13629v4.pdf

[^1_16]: https://arxiv.org/pdf/2412.10912.pdf

[^1_17]: http://arxiv.org/pdf/2411.15893.pdf

[^1_18]: https://arxiv.org/pdf/2311.06190.pdf

[^1_19]: https://arxiv.org/pdf/2206.09113.pdf

[^1_20]: https://arxiv.org/html/2506.14831v2

[^1_21]: https://arxiv.org/pdf/2402.06326.pdf

[^1_22]: https://arxiv.org/abs/2510.21491

[^1_23]: https://arxiv.org/html/2510.00014v2

[^1_24]: https://arxiv.org/html/2310.02473v2

[^1_25]: https://arxiv.org/html/2510.00809v1

[^1_26]: https://arxiv.org/html/2507.12463v1

[^1_27]: https://arxiv.org/html/2402.06326v2

[^1_28]: https://arxiv.org/html/2601.18699v1

[^1_29]: https://arxiv.org/pdf/2510.00014.pdf

[^1_30]: https://arxiv.org/html/2508.13219v1

[^1_31]: https://arxiv.org/abs/2510.00809

[^1_32]: https://arxiv.org/pdf/2511.03799.pdf

[^1_33]: https://arxiv.org/html/2601.03464v1

[^1_34]: https://ar5iv.labs.arxiv.org/html/2007.07400

[^1_35]: https://proceedings.iclr.cc/paper_files/paper/2025/file/cb2266111eadcfa2c02187ace64e2183-Paper-Conference.pdf

[^1_36]: https://www.emergentmind.com/topics/temporal-prompted-approach

[^1_37]: https://openreview.net/pdf/bd45ae931d19dd167c7107f33d28c0b7feca2db3.pdf

[^1_38]: https://openreview.net/forum?id=YUNnVFlpjp

[^1_39]: https://www.ijcai.org/proceedings/2025/0372.pdf

[^1_40]: https://openreview.net/pdf?id=YUNnVFlpjp

[^1_41]: https://www.sciencedirect.com/science/article/pii/S0957417425018639

[^1_42]: https://aclanthology.org/2023.findings-acl.493.pdf

[^1_43]: https://www.themoonlight.io/en/review/are-time-series-foundation-models-susceptible-to-catastrophic-forgetting

[^1_44]: https://arxiv.org/pdf/2503.01580.pdf

[^1_45]: https://arxiv.org/html/2405.12452v2

[^1_46]: https://scholarworks.aub.edu.lb/items/9fd7e7b9-15ec-4df1-8fb5-e9735587c89f
