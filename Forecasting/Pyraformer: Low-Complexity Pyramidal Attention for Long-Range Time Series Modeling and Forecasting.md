# Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting

### 1. 핵심 주장과 주요 기여

**Pyraformer**는 시계열 예측에서 **장거리 의존성 캡처**와 **낮은 계산 복잡도**의 상충관계를 동시에 해결하는 획기적인 접근법을 제안합니다. 기존 방법들의 한계를 극복하기 위해, 이 논문의 핵심 주장은 다음과 같습니다:[1]

**주요 기여:**

1. **다중해상도 피라미드 그래프 구조**: 시계열을 여러 해상도 수준에서 표현하여 다양한 범위의 시간 의존성을 효율적으로 모델링합니다.[1]

2. **최적의 복잡도-성능 균형 달성**: 기존 Transformer의 $$O(L^2)$$ 복잡도를 선형 $$O(L)$$으로 감소시키면서도, 최대 신호 경로를 $$O(1)$$로 유지합니다.[1]

3. **실증적 우수성**: 단일 단계 및 다중 단계 예측 작업에서 기존 방법들을 능가하는 성능을 달성합니다.[1]

***

### 2. 핵심 문제와 해결 방법

#### 문제 정의

기존 시계열 예측 모델들은 근본적인 트레이드오프에 직면했습니다:[1]

| 방법 | 시간 복잡도 | 최대 신호 경로 길이 | 문제점 |
|------|-----------|-----------------|-------|
| CNN/RNN | $$O(L)$$ | $$O(L)$$ | 장거리 의존성 학습 어려움 |
| Full Attention | $$O(L^2)$$ | $$O(1)$$ | 매우 긴 수열 처리 불가 |
| LogTrans | $$O(L \log L)$$ | $$O(\log L)$$ | 중간 수준의 경로 길이 |
| **Pyraformer** | **$$O(L)$$** | **$$O(1)$$** | **양쪽 모두 최적** |

#### 제안 방법: 피라미드 어텐션 모듈 (PAM)

Pyraformer의 핵심은 **피라미드 어텐션 모듈(PAM)**입니다. 이는 기존의 완전 어텐션 메커니즘을 개선합니다.[1]

**기존 완전 어텐션:**

$$
y_i = \frac{\sum_{\ell=1}^{L} \exp(q_i k_{\ell}^T / \sqrt{D_K}) v_{\ell}}{\sum_{\ell=1}^{L} \exp(q_i k_{\ell}^T / \sqrt{D_K})}
$$

여기서 모든 쿼리-키 쌍을 계산하므로 $$O(L^2)$$ 복잡도가 발생합니다.[1]

**피라미드 어텐션:**

$$
y_i = \frac{\sum_{\ell \in N_{\ell}^{(s)}} \exp(q_i k_{\ell}^T / \sqrt{d_K}) v_{\ell}}{\sum_{\ell \in N_{\ell}^{(s)}} \exp(q_i k_{\ell}^T / \sqrt{d_K})}
$$

여기서 각 노드는 제한된 이웃 집합 $$N_{\ell}^{(s)}$$에만 어텐션합니다:[1]

$$
N_{\ell}^{(s)} = A_{\ell}^{(s)} \cup C_{\ell}^{(s)} \cup P_{\ell}^{(s)}
$$

이는 세 가지 연결 유형을 포함합니다:[1]

- **동일 스케일 연결** $$A_{\ell}^{(s)}$$: 인접한 A개의 노드
- **자식 노드 연결** $$C_{\ell}^{(s)}$$: C-진법 트리의 자식 C개
- **부모 노드 연결** $$P_{\ell}^{(s)}$$: 부모 노드 1개

#### 주요 이론적 결과

**명제 1 (복잡도):** 주어진 A와 L에 대해 피라미드 어텐션의 시간 및 공간 복잡도는 $$O(AL)$$이며, A가 L에 대해 상수일 때 $$O(L)$$로 감소합니다.[1]

**명제 2 (신호 경로):** A와 S가 고정되고 C가 다음을 만족할 때:

$$
\sqrt[S-1]{L} \geq C \geq \sqrt[S-1]{\frac{L}{(A-1)N/2 + 1}}
$$

피라미드 그래프의 최대 신호 경로 길이는 $$O(1)$$입니다.[1]

***

### 3. 모델 구조

**Pyraformer의 전체 아키텍처는 네 가지 주요 구성 요소로 이루어집니다:**[1]

#### 3.1 코스 스케일 생성 모듈 (CSCM)

다중 스케일 노드를 초기화하는 모듈입니다:[1]

1. 임베딩된 시계열에 순차적으로 커널 크기 C, 스트라이드 C인 합성곱 층을 적용
2. 각 스케일 s에서 길이 $$L/C^s$$의 수열 생성
3. 파라미터 감소를 위해 병목 구조(bottleneck) 사용

**이점:**[1]
- 파라미터 수: 병목 없음 3,147,264 → 병목 포함 328,704 (약 90% 감소)
- 성능: MSE 0.796 vs 0.808 (1.51% 차이만 발생)

#### 3.2 피라미드 어텐션 모듈 (PAM)

핵심 모듈로, 다음과 같이 작동합니다:[1]

1. **스케일 s의 노드** $$n_{\ell}^{(s)}$$는 세 가지 유형의 이웃에 어텐션:
   - 같은 스케일의 인접 노드 (A개)
   - 자식 노드 (C개, 스케일 s-1)
   - 부모 노드 (1개, 스케일 s+1)

2. **계층적 구조**: 단계적으로 스택되는 주의 층을 통해 정보 전파

#### 3.3 예측 모듈

**단일 단계 예측:**[1]
- 모든 스케일의 마지막 노드 특성을 수집
- 완전 연결 층으로 단일 값 예측

**다중 단계 예측 (두 가지 전략):**[1]

**전략 1 (더 우수한 성능):**
- 마지막 노드들을 모든 M개의 미래 시점으로 직접 매핑

**전략 2:**
- 2개의 완전 어텐션 층을 사용한 디코더
- 예측 토큰 $$F_p$$를 쿼리, 인코더 출력을 키/값으로 사용

***

### 4. 성능 향상 분석

#### 4.1 단일 단계 예측 결과

**표 2 분석:**[1]

| 데이터셋 | Pyraformer NRMSE | 최고 성능 대비 | Q-K 쌍 수 |
|--------|-------------------|-------------|----------|
| Electricity | 0.324 | 동등 | 17,648 |
| Wind | 0.161 | 최고 (LogTrans 0.173) | 20,176 |
| App Flow | 0.366 | 최고 (Longformer 0.377) | 20,176 |

**주요 발견:**[1]
1. **가장 적은 계산량**: 로그 스파스 어텐션 대비 **65.4%** 감소
2. **비교 우위**: Wind 데이터셋에서 희소 주의 메커니즘이 과적합을 방지하여 완전 어텐션 능가
3. **파라미터 효율성**: 약 5% 오버헤드만 존재

#### 4.2 장기 다중 단계 예측 결과

**표 3 분석 (주요 성과):**[1]

ETTh1 데이터셋에서 Informer 대비 성능:
- 예측 길이 168: **MSE 24.8% 감소** (1.075 → 0.808)
- 예측 길이 336: **MSE 28.9% 감소** (1.329 → 0.945)
- 예측 길이 720: **MSE 26.2% 감소** (1.384 → 1.022)

**계산 효율성:**[1]
- Reformer 대비 Q-K 쌍: 1,016,064 → 26,472 (**97.4% 감소**)

#### 4.3 시간 및 메모리 소비

**그림 4의 실증적 결과:**[1]

| 시퀀스 길이 | 메모리 사용량 |
|-----------|-----------|
| 5,800 | Full Attention: OOM, Pyraformer: 1.0 GB |
| 20,000 | Informer: OOM, Pyraformer: 1.91 GB |

**계산 시간:** 수열 길이 증가에 따른 완벽한 선형 확장 관찰됨[1]

#### 4.4 합성 데이터 실험

**다중 범위 의존성 모델링 능력 검증:**[1]

합성 데이터에서 $$f(t) = \beta_0 + \beta_1 \sin(\frac{2\pi}{24}t) + \beta_2 \sin(\frac{2\pi}{168}t) + \beta_3 \sin(\frac{2\pi}{720}t)$$

| 모델 | MSE |
|-----|-----|
| Full Attention | 3.550 |
| Informer | 7.546 |
| Reformer | 1.538 |
| **Pyraformer (6,6,6)** | **1.258** (18.2% 개선) |
| **Pyraformer (12,7,4)** | **1.176** (23.6% 개선) |

---

### 5. 일반화 성능 향상 메커니즘

#### 5.1 다중 스케일 표현의 정규화 효과

**다중 스케일 구조의 이점:**[1]

1. **암시적 정규화**: 서로 다른 해상도에서 동일한 패턴을 반복 학습하여 과적합 방지
2. **정보 병목**: CSCM의 병목 구조가 추가 정규화 역할 수행
3. **가중 샘플러 활용**: 각 윈도우의 평균값 기반 가중치 샘플링으로 난제 샘플 처리

#### 5.2 히스토리 길이의 영향

**표 9 분석 (ETTm1, 예측 길이 1344):**[1]

| 히스토리 길이 | MSE | MAE |
|------------|-----|-----|
| 84 | 1.234 | 0.856 |
| 168 | 1.226 | 0.868 |
| 336 | 1.108 | 0.835 |
| 672 | **1.057** | **0.806** |
| 1344 | 1.062 | 0.806 |

**결론:** 672 시점에서 최고 성능 달성 후 성능 개선 정체. 충분한 주기 정보 포함 후 추가 길이는 노이즈 증가[1]

#### 5.3 하이퍼파라미터 선택의 일반화에 미치는 영향

**표 7 분석 (A와 C의 영향, ETTh1):**[1]

| A | C=2 | C=3 | C=4 | C=5 |
|---|-----|-----|-----|-----|
| 3 | 1.035 | 1.029 | 1.001 | **0.999** ✓ |
| 9 | 1.029 | 1.009 | 1.028 | 1.005 |
| 13 | 1.003 | 1.056 | 1.027 | 1.017 |

**핵심 발견:**[1]
- A를 작게 유지 (3 또는 5)할 때 최고 성능
- C를 수열 길이 L과 함께 증가시키는 것이 최적
- 상위 스케일 노드가 전역 수용장을 확보한 후 추가 증가는 성능 향상 미미

***

### 6. 모델의 한계

#### 6.1 구조적 한계

1. **고정된 스케일 구조**: S와 N이 고정되어 있어 특정 시계열 특성에 완벽히 적응하기 어려움[1]

2. **휴리스틱 기반 하이퍼파라미터**: C 선택이 방정식 (5)의 제약에 의존하여 데이터로부터 학습되지 않음[1]

3. **CUDA 커널 구현 필수성**: 희소 어텐션이 표준 프레임워크에서 지원되지 않아 TVM 기반 커스텀 구현 필요[1]

#### 6.2 실증적 한계

1. **제한된 데이터셋**: 4개 데이터셋에서만 평가 (Wind, App Flow, Electricity, ETT)[1]

2. **비자기회귀 디코더의 성능**: 다중 단계 예측에서 자기회귀 방식이 아닌 동시 예측이 우수 → 에러 누적 학습 부족 가능성[1]

3. **도메인 특화성**: 장기 시계열에 특화되어 있으나 단기 예측이나 불규칙한 시계열에서의 성능 미평가

#### 6.3 이론적 한계

1. **O(1) 경로의 조건부 달성**: 명제 2의 조건 (방정식 5)이 매우 제한적이며 실제 적용에서 정확히 만족되지 않을 가능성[1]

2. **신호 경로 길이의 완전 이해 부족**: 최대 경로 O(1)이더라도 평균 경로나 분포는 분석되지 않음

***

### 7. 향후 연구에 미치는 영향과 고려사항

#### 7.1 긍정적 영향

**1. 아키텍처 혁신의 방향성:**[1]
- 다중 해상도 그래프 구조가 다른 도메인으로 확장될 가능성 (NLP, 컴퓨터 비전)
- 희소 주의 메커니즘의 새로운 설계 패러다임 제시

**2. 효율성 연구의 기준 설정:**[1]
- $$O(L)$$ 복잡도로 매우 긴 시계열 처리 가능 입증
- 프로덕션 환경에서의 실시간 예측 가능성 확대

**3. 멀티스케일 모델링의 중요성 인식:**[1]
- 시계열의 계층적 구조 학습이 성능 향상에 필수적임을 증명
- 도메인 지식(예: 일일/주간/월간 주기)의 통합 중요성 강조

#### 7.2 향후 연구 시 고려할 점

**1. 적응적 하이퍼파라미터 학습:**[1]
- 현재는 방정식 (5)의 제약에 따라 C를 휴리스틱하게 선택
- **향후:** 데이터로부터 최적 C를 학습하는 메커니즘 개발
- 논문의 향후 계획: "데이터로부터 하이퍼파라미터를 적응적으로 학습하는 방법 탐색"

**2. 도메인 확장 및 일반화:**[1]
- 현재: 시계열 예측에 특화
- **향후:** NLP, 컴퓨터 비전 등 다른 시퀀스 모델링 작업으로 확장
- 불규칙 시계열, 이상 탐지, 다변량 예측 등 다양한 응용

**3. 이론적 깊이 강화:**[1]
- 최대 경로 O(1)의 충분 조건 완화
- 평균 경로 길이, 정보 흐름 분석 등 이론적 분석 확대
- 일반화 성능에 대한 통계적 학습 이론 관점의 분석

**4. 실무 적용성 개선:**[1]
- TVM 커스텀 커널 없이도 작동하는 구현 방법 모색
- PyTorch, TensorFlow 등 표준 프레임워크의 희소 주의 지원 활용
- 프로덕션 환경에서의 안정성 및 재현성 검증

**5. 오프라인 학습에서 온라인 학습으로의 확장:**[1]
- 현재: 고정된 길이 시계열에 대한 배치 학습
- **향후:** 스트리밍 데이터에 대한 온라인 학습 및 점진적 업데이트 메커니즘

**6. 다른 시간-공간 구조와의 결합:**[1]
- 그래프 신경망(GNN)과의 결합을 통한 공간-시간 의존성 모델링
- 외생 변수(exogenous variables)의 효과적 통합

#### 7.3 일반화 성능 향상을 위한 구체적 전략

**당신의 연구 관심사 (일반화 성능)에 초점:**[1]

1. **도메인 적응(Domain Adaptation) 관점:**
   - 서로 다른 시계열 데이터셋 간 전이학습 가능성 탐색
   - 소수의 라벨 데이터로 새로운 도메인에 빠르게 적응

2. **데이터 증강(Data Augmentation):**
   - 다중 스케일 구조를 활용한 자체 감독 학습(Self-supervised Learning)
   - 다양한 해상도의 표현을 서로 일치시키는 정규화

3. **앙상블 및 다중 모델 결합:**
   - 다양한 하이퍼파라미터 조합의 Pyraformer 앙상블
   - 기존 방법(ARIMA, Prophet)과의 하이브리드 모델

4. **메타학습(Meta-Learning):**
   - 새로운 시계열에 빠르게 적응하기 위한 메타학습 프레임워크
   - Few-shot 학습을 통한 일반화 성능 향상

***

### 결론

Pyraformer는 **피라미드 어텐션 메커니즘**을 통해 시계열 예측의 오래된 문제를 우아하게 해결합니다. $$O(L)$$ 복잡도로 $$O(1)$$ 신호 경로를 달성함으로써 기술적으로 우수한 모델임을 입증했습니다. 특히 **다중 해상도 표현**이라는 핵심 아이디어는 단순하면서도 강력하며, 향후 시계열 분석뿐 아니라 다양한 시퀀스 모델링 분야에 깊은 영향을 미칠 것으로 예상됩니다.[1]

앞으로의 연구에서는 **적응적 하이퍼파라미터 학습**, **도메인 확장**, **온라인 학습 지원** 등을 통해 모델의 실용성과 일반화 성능을 더욱 강화할 수 있을 것입니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/ed5cfb09-de90-4129-ab50-00cd422b1e3f/Liu-2022-Pyraformer-Low-Complexity-Pyramidal-Attention-for-Long-Range-Tim...-vor.pdf)
