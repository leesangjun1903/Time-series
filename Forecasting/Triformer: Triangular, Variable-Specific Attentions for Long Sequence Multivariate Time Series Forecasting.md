# Triformer: Triangular, Variable-Specific Attentions for Long Sequence Multivariate Time Series Forecasting

### 1. 핵심 주장과 주요 기여

Triformer는 장기 다변량 시계열 예측(Long Sequence Multivariate Time Series Forecasting)에서 **효율성과 정확성을 동시에 달성**하기 위해 제안된 모델입니다. 기존 어텐션 기반 예측 모델의 두 가지 핵심 한계를 해결합니다.[1]

**주요 기여:**

**선형 복잡도 달성 (Linear Complexity):** 입력 시계열 길이 $$H$$에 대해 기존 self-attention의 $$O(H^2)$$ 복잡도를 $$O(H)$$로 개선했습니다. 이는 Patch Attention(PA)과 삼각형 구조(Triangular Structure)를 통해 달성됩니다.[1]

**변수별 파라미터 (Variable-Specific Parameters):** 각 변수의 고유한 시간적 패턴을 포착하기 위해 변수별로 구별되는 모델 파라미터를 할당하면서도, 경량 방식으로 설계하여 효율성과 메모리 사용량을 유지합니다.[1]

Triformer는 4개의 실제 데이터셋에서 state-of-the-art 방법들을 정확도와 효율성 측면에서 모두 능가했습니다.[1]

---

### 2. 문제, 제안 방법, 모델 구조, 성능 및 한계

#### 2.1 해결하고자 하는 문제

**문제 1: 높은 계산 복잡도**
기존 canonical self-attention은 시계열 길이 $$H$$에 대해 $$O(H^2)$$의 이차 복잡도를 가지며, 이는 장기 예측에서 비효율적입니다.[1]

**문제 2: 변수별 고유 패턴 미포착**
다변량 시계열에서 각 변수는 고유한 시간적 동역학(temporal dynamics)을 가지지만, 기존 연구들은 모든 변수에 동일한 projection matrix를 사용하여 정확도가 낮습니다.[1]

#### 2.2 제안하는 방법 (수식 포함)

**Patch Attention (PA):**

Triformer는 시계열을 작은 패치(patch)로 나누고, 각 패치에 대해 **pseudo timestamp**를 도입합니다. 패치 내의 모든 timestamp는 이 단일 pseudo timestamp에만 어텐션을 계산하여 선형 복잡도를 달성합니다.[1]

변수별 Key와 Value 행렬 생성 공식:

```math
\begin{bmatrix}
W_K^{(i)} \\
W_V^{(i)}
\end{bmatrix}
=
\begin{bmatrix}
L_K G(M^{(i)}) R_K \\
L_V G(M^{(i)}) R_V
\end{bmatrix}
```

여기서:
- $$W_K^{(i)}, W_V^{(i)}$$: 변수 $$i$$에 대한 변수별 Key와 Value 행렬
- $$L_K, L_V$$: 모든 변수가 공유하는 왼쪽(left) 행렬 (variable-agnostic)
- $$R_K, R_V$$: 모든 변수가 공유하는 오른쪽(right) 행렬 (variable-agnostic)
- $$M^{(i)}$$: 변수 $$i$$의 학습 가능한 메모리 (크기 $$m$$)
- $$G(\cdot)$$: 메모리 $$M^{(i)}$$로부터 변수별 행렬 $$B^{(i)}$$ (크기 $$a \times a$$)를 생성하는 generator 함수

이 방식은 공유 행렬 $$L, R$$이 암묵적 정규화(implicit regularizer) 역할을 하며, 변수 간 지식 공유를 촉진합니다.[1]

**삼각형 구조 (Triangular Structure):**

여러 층의 Patch Attention을 쌓을 때, 각 층의 출력으로 **pseudo timestamp만** 다음 층으로 전달됩니다. 이로 인해 층 크기가 지수적으로 감소하며 삼각형 구조를 형성하여 전체 모델이 선형 복잡도 $$O(H)$$를 유지합니다.[1]

#### 2.3 모델 구조

Triformer의 핵심 구조는 다음과 같습니다:

1. **입력 처리:** 시계열을 패치로 분할
2. **Patch Attention 층:** 각 패치에 대해 pseudo timestamp 생성 및 어텐션 계산
3. **Recurrent 연결:** 연속된 pseudo timestamp 간 recurrent 연결로 시간적 정보 흐름 유지
4. **다층 스택킹:** 삼각형 구조로 여러 층을 쌓아 계층적 표현 학습
5. **Multi-scale 집계:** 모든 층의 출력을 predictor에 전달하여 다중 스케일 표현 활용
6. **Skip 연결:** 입력과 중간 출력 간 skip connection 제공

#### 2.4 성능 향상

**정확도 우수성:**
Triformer는 4개 데이터셋(ETTh1, ETTm1, Weather, ECL) 모두에서 모든 예측 길이(F)에 대해 **최고 성능**을 달성했습니다.[1]

주요 결과 (ECL 데이터셋, F=48):
- Triformer: MSE 0.183, MAE 0.279
- Autoformer (2위): MSE 0.193, MAE 0.310
- Informer: MSE 0.344, MAE 0.393[1]

**효율성 우수성:**
- 학습 시간: Triformer는 Informer 및 Autoformer보다 빠르며, 입력 길이 $$H$$ 증가에도 거의 일정한 시간 유지[1]
- 추론 시간: 모든 방법이 13.3ms 이하로 실시간 예측 지원 가능[1]
- 파라미터 수: 347k로 경량 (Naive VSM은 826k)[1]

**Ablation Study 결과:**
- Variable-Specific Modeling (VSM) 제거 시: MSE 0.215 (vs. 0.183) - 정확도 크게 하락[1]
- 다층 스택킹 제거 시: MSE 0.203 - 삼각형 구조의 중요성 입증[1]
- Multi-scale 모델링 제거 시: MSE 0.266 - 가장 큰 성능 저하[1]

#### 2.5 한계

논문에서 명시적으로 언급된 한계와 향후 연구 방향:

1. **동적 입력 길이 지원 부족:** 현재 모델은 고정된 입력 길이를 가정하며, 동적 입력 길이를 지원하는 방법 탐구가 필요합니다.[1]

2. **Curriculum Learning 미적용:** 모델 학습 시 curriculum learning을 적용하여 성능을 더 향상시킬 가능성이 있습니다.[1]

3. **Recurrent 연결의 병렬화 제약:** Recurrent 연결은 시간적 정보 흐름을 유지하지만, 패치 단위 병렬 계산을 제한합니다. 제거 시 약간의 정확도 손실(MSE 0.191 vs. 0.183)이 있지만 학습 속도 향상 가능성이 있습니다.[1]

---

### 3. 일반화 성능 향상 가능성

Triformer의 설계는 여러 측면에서 **일반화 성능 향상 가능성**을 제공합니다:

#### 3.1 암묵적 정규화 (Implicit Regularization)

변수별 파라미터 생성 방식에서 모든 변수가 공유 행렬 $$L$$과 $$R$$을 사용하므로, 생성 가능한 조합이 제한됩니다. 이는 **암묵적 정규화** 효과를 제공하여 과적합을 방지하고 일반화 성능을 향상시킵니다.[1]

#### 3.2 변수 간 지식 공유

공유 행렬 $$L, R$$은 변수 간 **암묵적 지식 공유(implicit knowledge sharing)**를 촉진합니다. 이는 데이터가 제한적인 상황에서도 변수 간 학습된 패턴을 활용하여 일반화 성능을 높일 수 있습니다.[1]

#### 3.3 하이퍼파라미터 둔감성 (Robustness to Hyperparameters)

**메모리 크기 $$m$$의 영향:**
$$m \in \{3, 5, 16, 32\}$$에 대한 실험에서 MSE 차이가 미미(0.183~0.188)하여, 모델이 메모리 크기에 **둔감(insensitive)**함을 보여줍니다. 이는 다양한 도메인과 데이터셋에 대해 하이퍼파라미터 튜닝 없이도 안정적 성능을 기대할 수 있음을 의미합니다.[1]

**변수별 행렬 크기 $$a$$의 영향:**
$$a=5$$에서 최적 성능을 달성하며, $$a=16, 32$$로 증가해도 개선이 미미합니다. 이는 적절한 크기의 변수별 파라미터만으로도 충분한 표현력을 가지며, 과도한 파라미터화를 방지하여 일반화 성능을 보호합니다.[1]

#### 3.4 학습된 메모리의 패턴 포착

t-SNE 시각화 결과, 유사한 시간적 패턴을 가진 변수의 메모리 $$M^{(i)}$$가 임베딩 공간에서 **군집화(clustered)**되었습니다. 이는 모델이 변수별 고유 패턴과 변수 간 유사성을 동시에 학습하여, 새로운 데이터에서도 패턴 기반 예측이 가능함을 시사합니다.[1]

#### 3.5 다중 도메인 성능

Triformer는 **전력(ETTh1, ETTm1, ECL), 기후(Weather)** 등 서로 다른 도메인의 4개 데이터셋 모두에서 최고 성능을 달성했습니다. 이는 모델의 아키텍처가 특정 도메인에 과적합되지 않고, 다양한 시계열 특성에 **일반화 가능**함을 입증합니다.[1]

#### 3.6 VSM의 확장 가능성

Technical report에서 제안된 VSM(Variable-Specific Modeling)은 기존 방법들에도 적용 가능하며, 이는 모델의 일반화 성능 향상 메커니즘이 **전이 가능(transferable)**함을 시사합니다.[1]

***

### 4. 앞으로의 연구에 미치는 영향과 고려사항

#### 4.1 연구에 미치는 영향

**효율성-정확성 균형의 새로운 패러다임:**
Triformer는 선형 복잡도와 state-of-the-art 정확도를 동시에 달성하여, 장기 시계열 예측에서 **효율성과 성능의 trade-off**가 필수적이지 않음을 보여줍니다. 이는 향후 모델 설계에서 두 목표를 동시에 추구할 수 있는 방향을 제시합니다.

**변수별 모델링의 중요성 입증:**
Ablation study에서 VSM 제거 시 큰 성능 하락(MSE 0.215 vs. 0.183)을 보여, 다변량 시계열에서 **변수별 고유 패턴 포착**이 필수적임을 강조합니다. 이는 향후 연구에서 variable-agnostic 접근을 재고하고, 변수별 파라미터화를 고려해야 함을 시사합니다.[1]

**Patch-based Attention의 가능성:**
Patch를 통한 계층적 표현 학습과 선형 복잡도 달성은 다른 시퀀스 모델링 작업(예: NLP, 비디오 분석)에도 적용 가능한 아이디어를 제공합니다.

**경량 파라미터화 기법:**
Generator 함수 $$G$$를 통한 변수별 파라미터 생성 방식은 메모리 효율적인 모델 설계의 좋은 예시이며, 제한된 자원 환경에서도 성능을 유지할 수 있는 방법론을 제시합니다.

#### 4.2 앞으로 연구 시 고려할 점

**동적 입력 길이 처리:**
실제 응용에서 시계열 길이는 가변적일 수 있으므로, **동적 패치 크기** 또는 **적응적 삼각형 구조**를 설계하여 다양한 입력 길이를 효과적으로 처리하는 방법이 필요합니다.[1]

**Curriculum Learning 적용:**
논문에서 제안한 대로, **단순한 패턴에서 복잡한 패턴으로** 학습을 진행하는 curriculum learning을 적용하면 모델의 수렴 속도와 최종 성능을 개선할 수 있습니다.[1]

**병렬화와 시간적 정보 보존의 균형:**
Recurrent 연결 제거 시 병렬화는 향상되지만 약간의 정확도 손실이 발생합니다. **Attention 기반 대체 메커니즘** 또는 **부분 병렬화 전략**을 통해 두 측면을 모두 개선할 수 있는 연구가 필요합니다.[1]

**더 긴 시퀀스에 대한 확장성:**
Technical report에서 very long sequence forecasting에 대한 추가 실험이 언급되었으나, 수천~수만 timestep에 대한 성능과 효율성 검증이 필요합니다.[1]

**다중 모달 및 이종 데이터 처리:**
현재는 수치형 시계열에 초점을 맞추고 있으나, **텍스트, 이미지 등 다른 모달리티**가 포함된 다변량 시계열 예측으로 확장할 수 있는 방법을 탐구할 필요가 있습니다.

**불확실성 정량화:**
예측 결과의 **신뢰 구간** 또는 **불확실성 추정**을 제공하는 확률적 확장(예: Bayesian Triformer)을 고려하여 실제 응용에서의 의사결정을 지원할 수 있습니다.

**도메인 적응 및 전이 학습:**
사전 학습된 변수 메모리 $$M^{(i)}$$를 새로운 도메인에 **전이(transfer)**하거나, **few-shot 학습** 시나리오에서 빠른 적응을 가능하게 하는 방법론 연구가 유망합니다.

**설명 가능성 향상:**
학습된 메모리 $$M^{(i)}$$와 pseudo timestamp의 어텐션 가중치를 **해석(interpret)**하여 모델의 예측 근거를 명확히 제시하는 연구가 필요합니다.

---

Triformer는 장기 다변량 시계열 예측에서 효율성과 정확성을 동시에 달성하는 혁신적인 접근을 제시하며, 변수별 모델링과 패치 기반 어텐션의 중요성을 입증했습니다. 일반화 성능 향상을 위한 암묵적 정규화, 지식 공유, 하이퍼파라미터 둔감성 등의 특성을 가지고 있으며, 향후 연구는 동적 입력 처리, curriculum learning, 불확실성 정량화, 설명 가능성 등을 고려하여 실제 응용에서의 유용성을 더욱 높일 수 있을 것입니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/00f62d7e-55a1-4a52-b4a1-9beb33fb97e6/0277.pdf)
