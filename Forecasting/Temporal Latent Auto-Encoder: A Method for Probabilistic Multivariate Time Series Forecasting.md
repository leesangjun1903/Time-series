# Temporal Latent Auto-Encoder: A Method for Probabilistic Multivariate Time Series Forecasting

## 1. 핵심 주장과 주요 기여

**Temporal Latent Auto-Encoder (TLAE)**는 고차원 다변량 시계열의 확률적 예측을 위한 혁신적인 딥러닝 프레임워크입니다. 이 논문은 기존 접근법들의 세 가지 핵심 한계를 극복합니다:[1]

**주요 기여**

**비선형 인수분해(Nonlinear Factorization)** - 기존의 선형 행렬 분해 방식(TRMF, DeepGLO)을 비선형 오토인코더로 대체하여, 시계열 간 비선형 관계를 포착할 수 있게 되었습니다. 이는 소매 수요 예측에서 상대 가격비율과 같은 비선형 상관관계를 모델링할 수 있게 합니다.[1]

**End-to-End 학습** - 인코더, 잠재공간 시계열 모델(LSTM), 디코더를 통합된 손실함수로 함께 학습하여, 기존 방법들의 휴리스틱한 교차 최적화 문제를 해결했습니다. 이는 확률적 경사하강법(SGD)을 통한 효율적인 최적화를 가능하게 합니다.[1]

**확률적 분포 모델링** - 잠재공간에 가우시안 분포를 주입하고 비선형 디코더를 통해 변환함으로써, 복잡한 결합 예측 분포를 암묵적으로 모델링합니다. 이는 VAE와 유사하지만, 시계열 모델과 결합되어 있고 잠재평균에 제약이 없다는 차이가 있습니다.[1]

**스케일러빌리티** - 모델 복잡도가 $$O((nd + bd)L)$$로 입력 차원 $$n$$에 대해 선형적이며, 입력공간에서 직접 모델링하는 $$O(n^2)$$ 복잡도보다 훨씬 효율적입니다. 이는 수백만 개의 시계열로 확장 가능하게 합니다.[1]

**성능 향상** - 실험 결과, Traffic 및 Electricity 데이터셋에서 기존 글로벌 인수분해 방법 대비 최대 50% 성능 향상을 달성했으며, 일부 데이터셋에서는 지역 모델링을 사용하는 방법들보다도 우수한 성능을 보였습니다.[1]

## 2. 문제 정의, 제안 방법, 모델 구조, 성능 및 한계

### 문제 정의

**배경** - 고차원 다변량 시계열 $$Y \in \mathbb{R}^{n \times T}$$ (n개의 시계열, T개의 시점)이 주어졌을 때, 미래 τ 시점의 조건부 확률분포를 예측하는 것이 목표입니다:[1]

$$p(y_{T+1}, \ldots, y_{T+\tau} | y_{1:T}) = \prod_{i=1}^{\tau} p(y_{T+i} | y_{1:T+i-1})$$

**기존 방법의 한계**:[1]
- **다중출력 모델** (VAR, RNN, TCN): 시계열 수가 증가하면 과적합되고 확장성이 부족
- **다중작업 단변량 모델** (DeepAR, Transformer): 시계열 간 상관관계를 포착하지 못함
- **기존 인수분해 방법** (TRMF, DeepGLO): 선형 관계만 포착하고, 확률적 예측 불가능하며, end-to-end 학습 불가

### 제안 방법

**결정론적 모델 (Point Prediction)**

TLAE의 핵심 아이디어는 입력 시계열을 저차원 잠재공간으로 매핑한 후, 그 공간에서 예측을 수행하고 다시 원래 공간으로 복원하는 것입니다.[1]

**인코더-디코더 구조**:[1]
- 인코더: $$X = g_{\phi}(Y)$$, $$g: \mathbb{R}^n \rightarrow \mathbb{R}^d$$ (여기서 $$d \ll n$$)
- 디코더: $$\hat{Y} = f_{\theta}(X)$$, $$f: \mathbb{R}^d \rightarrow \mathbb{R}^n$$
- 잠재 시계열 모델: $$\hat{x}\_{i+1} = h_W(x_{i-L+1}, \ldots, x_i)$$ (LSTM 사용)

**손실 함수**:[1]

배치 데이터 $$Y_B = [y_1, \ldots, y_b]$$에 대한 손실함수는 재구성 손실과 잠재공간 예측 손실의 가중합입니다:

$$L_B(W, \phi, \theta) = \frac{1}{nb} \|\hat{Y}_B - Y_B\|_{\ell_p}^p + \lambda \frac{1}{d(b-L)} \sum_{i=L}^{b-1} \|x_{i+1} - h_W(x_{i-L+1}, \ldots, x_i)\|_{\ell_q}^q$$

여기서:
- 첫 번째 항: 입력공간에서의 재구성 손실 (실험에서는 $$\ell_1$$ 사용)
- 두 번째 항: 잠재공간에서의 시계열 예측 손실 (실험에서는 $$\ell_2$$ 사용)
- $$\lambda$$: 정규화 파라미터 (실험에서는 0.5 사용)

전체 목적함수는:[1]

$$\min_{W,\phi,\theta} L(W, \phi, \theta) = \frac{1}{|\mathcal{B}|} \sum_{B \in \mathcal{B}} L_B(W, \phi, \theta)$$

**확률론적 모델 (Probabilistic Prediction)**

**잠재공간 확률 모델**:[1]

잠재변수에 조건부 가우시안 분포를 부여합니다:

$$p(x_{i+1} | x_1, \ldots, x_i) = \mathcal{N}(x_{i+1}; \mu_i, \sigma_i^2), \quad i = L, \ldots, b$$

여기서:
- $$\mu_i = h_W^{(1)}(x_1, \ldots, x_i)$$: LSTM 출력 평균
- $$\sigma_i^2 = 1$$: 단위분산으로 고정 (과적합 방지)

**확률론적 손실함수**:[1]

$$L_B(\phi, \theta, W) = \frac{1}{nb} \|\hat{Y}_B - Y_B\|_{\ell_p}^p - \lambda \frac{1}{b-L} \sum_{i=L+1}^{b} \log \mathcal{N}(x_i; \mu_{i-1}, 1)$$

여기서:
- 첫 번째 항: 재구성 손실
- 두 번째 항: 음의 로그-가능도 (negative log-likelihood)
- $$\lambda = 0.005$$ (확률론적 실험에서 사용)

**Reparameterization Trick**:[1]

역전파를 위해 VAE에서 사용되는 재매개변수화 기법을 활용합니다:

$$\hat{x}_{i+1} = \mu_i + \epsilon, \quad \epsilon \sim \mathcal{N}(0, 1)$$

**예측 절차**:[1]

1. 과거 입력 $$[y_{T-L+1}, \ldots, y_{T}]$$를 인코더에 통과: $$x_i = g_{\phi}(y_i)$$
2. LSTM으로 잠재예측: $$\hat{x}_{T+1} \sim \mathcal{N}(\mu_T, 1)$$
3. 디코더로 복원: $$\hat{y}\_{T+1} = f_{\theta}(\hat{x}_{T+1})$$
4. τ 스텝만큼 반복하여 여러 샘플 생성 (예: 100개 샘플)

### 모델 구조

**네트워크 아키텍처**:[1]

**인코더 및 디코더** - Feed-forward network (FNN) 레이어들로 구성되며, 마지막 레이어를 제외하고 ReLU 활성화 함수 사용. 데이터셋별로 레이어 차원이 다름:[1]
- Traffic: 
- Electricity:  또는 
- Solar/Taxi: 

**잠재공간 시계열 모델** - 4층 LSTM, 각 층당 32 또는 64개의 hidden units 사용[1]

**배치 크기** - LSTM 타임스텝 L의 2배로 설정하여, 직접 전달되는 잠재변수와 LSTM을 통과하는 잠재변수의 균형 유지[1]

**학습 설정**:[1]
- Optimizer: Adam (학습률 0.0001)
- 손실함수: Y에 대해 $$\ell_1$$, X에 대해 $$\ell_2$$ (결정론적) 또는 음의 로그-가능도 (확률론적)
- Epochs: 데이터셋별로 20-1000 (Wiki: 20, Traffic: 1000)

### 성능 향상

**Point Prediction 결과 (WAPE/MAPE/SMAPE)**:[1]

| 모델 | Traffic | Electricity | Wiki |
|------|---------|-------------|------|
| TLAE | **0.117/0.137/0.108** | **0.080/0.152/0.120** | 0.334/0.447/0.434 |
| DeepGLO-TCN-MF | 0.226/0.284/0.247 | 0.106/0.525/0.188 | 0.433/1.59/0.686 |
| TRMF | 0.159/0.226/0.181 | 0.104/0.280/0.151 | 0.309/0.847/0.451 |

TLAE는 모든 글로벌 인수분해 방법을 능가하며, Traffic과 Electricity에서 최대 50% 성능 향상을 보였습니다.[1]

**Probabilistic Prediction 결과 (CRPS-Sum / MSE)**:[1]

| 모델 | Solar | Traffic | Taxi |
|------|-------|---------|------|
| TLAE | **0.124/6.8e2** | **0.069/4.4e-4** | **0.130/2.6e1** |
| GP-Copula | 0.337/9.8e2 | 0.078/6.9e-4 | 0.208/3.1e1 |
| VRNN | 0.133/7.3e2 | 0.181/8.7e-4 | 0.139/3.0e1 |

TLAE는 10개 데이터셋-메트릭 조합 중 7개에서 최고 성능을 달성했습니다.[1]

**Ablation Study - 비선형 인코더의 효과**:[1]

| 인코더 타입 | WAPE/MAPE/SMAPE |
|------------|-----------------|
| Linear FFN  | 0.117/0.134/0.109 |
| FFN with ReLU  | **0.106/0.120/0.093** |

비선형 인코더가 선형 인코더보다 유의미하게 우수한 성능을 보여, 비선형 임베딩의 중요성을 검증했습니다.[1]

### 한계점

**계산 비용** - GPU 없이 CPU만 사용할 경우 대규모 데이터셋(Traffic: 963 시계열)에서 1000 epochs 학습에 약 4일 소요. 그러나 GPU를 사용하면 수 시간으로 단축 가능.[1]

**하이퍼파라미터 튜닝 부족** - 논문에서는 제한적인 하이퍼파라미터 탐색만 수행했으며, 더 철저한 튜닝으로 성능 향상 가능성이 있음.[1]

**Wiki 데이터셋 성능** - 일부 메트릭에서 TCN (LeveledInit) 같은 지역 모델링 방법에 비해 낮은 성능. 이는 Wiki 데이터의 특성상 시계열 간 상관관계보다 개별 시계열의 특성이 더 중요할 수 있음을 시사합니다.[1]

**고정된 잠재분산** - 확률론적 모델에서 잠재공간 분산을 1로 고정했는데, 이를 학습 가능하게 만들면 더 유연한 분포 모델링이 가능할 수 있습니다.[1]

**외생변수 미포함** - 본 연구에서는 기본 시계열 데이터만 사용했으며, DeepAR이나 Transformer처럼 요일, 시간 등의 외생변수를 포함하면 성능이 더 향상될 수 있습니다. 논문에서는 3D 텐서 확장 방법을 제시했지만 실험하지는 않았습니다.[1]

## 3. 일반화 성능 향상 가능성

### 구조적 일반화 메커니즘

**저차원 잠재공간의 노이즈 제거 효과** - 고차원 입력을 저차원($$d \ll n$$)으로 압축함으로써 노이즈와 관련 없는 변동을 필터링하고 공통 패턴만 추출합니다. 이는 희소하거나 노이즈가 많은 데이터(예: 월 1회 판매)에서 더 정확한 예측을 가능하게 합니다.[1]

**교차 시계열 정보 활용** - 단변량 방법과 달리, 관련 시계열 간 정보를 공유하여 개별 시계열의 부족한 데이터를 보완합니다. 예를 들어, 소매에서 한 제품의 판매 증가가 관련 제품의 판매에 미치는 영향을 학습합니다.[1]

**비선형 관계 포착** - 비선형 인코더/디코더를 통해 시계열 간 복잡한 관계(역비례, 비선형 의존성)를 모델링할 수 있습니다. Ablation study에서 비선형 인코더가 선형 대비 약 10% 성능 향상을 보였습니다.[1]

### 실험적 일반화 증거

**다양한 도메인에서의 성능** - Traffic(교통), Electricity(전력), Solar(태양광), Taxi(택시), Wiki(웹 트래픽) 등 5개 다른 도메인에서 일관되게 우수한 성능 달성.[1]

**다양한 시계열 규모** - 137개(Solar)부터 115,084개(Wiki-large)까지 다양한 시계열 수에서 효과적으로 작동. 특히 Wiki-large에서 Wiki-small 대비 런타임 증가가 미미하여 스케일러빌리티 입증.[1]

**소형 잠재차원** - Traffic에서 16차원, Electricity에서 32차원의 잠재공간만으로 우수한 성능 달성. 이는 DeepGLO의 64차원보다 작으며, 더 간결하고 일반화된 표현을 학습했음을 의미합니다.[1]

### 일반화 성능 향상 전략

**하이퍼파라미터 민감도 분석**:[1]
- **배치 크기**: L+1에서 3L로 증가시 MAPE가 0.4에서 0.2로 감소 (예측 능력 향상)
- **정규화 파라미터 λ**: 1e-4 ~ 1e-2 범위에서 최적 성능 (너무 작으면 과적합, 너무 크면 과소적합)
- **잠재차원**: 16~32 차원에서 성능 안정화 (더 큰 차원은 이득 미미)

**적절한 균형**:[1]
- 배치 크기를 LSTM 타임스텝의 2배로 설정하여 직접 재구성과 예측 손실의 균형 유지
- $$\ell_1$$ 손실을 사용하여 이상치(outlier)에 강건하게 만듦

**확장 방향**:[1]
- **고급 아키텍처**: LSTM 대신 TCN이나 Transformer 사용 시 성능 향상 가능
- **외생변수 통합**: 3D 텐서로 확장하여 날씨, 가격, 이벤트 등 추가 정보 활용
- **지역-글로벌 결합**: DeepGLO처럼 글로벌 잠재예측과 지역 단변량 모델 결합
- **시계열 인코더/디코더**: 인코더/디코더에도 시계열 모델(RNN, TCN) 사용하여 비정상적(non-stationary) 관계 포착

**다양한 분포 학습 능력**:[1]
- 실험 결과, 2D 결합분포 시각화에서 다봉(multi-modal), 비가우시안(non-Gaussian), 비선형 관계를 성공적으로 포착
- 예측 구간(prediction interval)이 시간과 시계열에 따라 변하는 이분산성(heteroskedasticity) 분포 모델링 성공

## 4. 향후 연구에 미치는 영향 및 고려사항

### 학술적 영향

**글로벌 인수분해 방법론의 패러다임 전환** - TRMF와 DeepGLO의 선형 제약을 극복하고, 비선형 딥러닝 기반 인수분해의 새로운 표준을 제시했습니다. 이는 수백만 개 시계열로 확장 가능한 확률론적 예측의 가능성을 열었습니다.[1]

**확률론적 예측의 새로운 접근** - 저차원 잠재공간에서 단순한 가우시안 노이즈를 주입하고 비선형 디코더로 변환하여 복잡한 결합분포를 모델링하는 방법은, 고차원 공분산 행렬의 저랭크 근사나 normalizing flow보다 스케일러블합니다.[1]

**End-to-End 학습의 우수성** - 교차 최적화 없이 통합 손실함수로 학습하여 효율성과 성능을 동시에 달성한 것은, 향후 다변량 예측 모델 설계의 중요한 지침이 됩니다.[1]

### 산업 응용 가능성

**소매 수요 예측** - 수백만 제품-매장 조합의 판매 예측에서 교차 제품 효과를 포착하고, 희소한 판매 데이터의 불확실성을 정량화할 수 있습니다.[1]

**에너지 및 교통 관리** - 실험에서 입증된 것처럼, 공간적 상관관계가 있는 전력 소비나 교통 흐름 예측에 효과적입니다.[1]

**금융 리스크 관리** - 주식 가격 간 비선형 의존성을 모델링하고 포트폴리오 리스크를 정량화하는 데 활용 가능합니다.[1]

### 향후 연구 방향

**모델 아키텍처 개선**:[1]
- **Transformer 통합**: 잠재공간에 Transformer를 적용하여 장기 의존성 포착 향상
- **시계열 인코더/디코더**: 현재 FNN 대신 CNN, RNN, TCN을 사용하여 비정상적 관계 모델링
- **Attention 메커니즘**: 시계열 간 동적 가중치 학습으로 더 유연한 관계 포착

**외생변수 통합**:[1]
- 3D 텐서 확장 실험으로 날씨, 가격, 이벤트, 계절성 등 추가 정보 활용
- CNN 스타일 아키텍처로 시계열-특징 간 관계 효율적 학습

**지역-글로벌 하이브리드**:[1]
- TLAE의 글로벌 잠재예측을 DeepGLO처럼 지역 단변량 모델의 외생변수로 활용
- 단변량 방법(DeepAR, Transformer)과 상호 보완적 결합

**이론적 분석**:
- 일반화 오차 한계(generalization error bound) 증명
- 최적 잠재차원 선택 이론 개발
- 수렴 보장 및 안정성 분석

### 연구 시 고려사항

**하이퍼파라미터 튜닝** - 잠재차원, 배치 크기, 정규화 파라미터, 학습 에포크 수에 대한 철저한 그리드 서치가 필요합니다. 논문에서는 제한적 탐색만 수행했으므로 추가 향상 가능성이 있습니다.[1]

**계산 자원 관리** - GPU 활용을 통해 학습 시간을 수 시간으로 단축할 수 있으나, 대규모 데이터셋(수백만 시계열)에서는 여전히 상당한 자원이 필요합니다.[1]

**데이터 특성 고려** - Wiki 데이터셋처럼 시계열 간 상관관계가 약한 경우, 단변량 방법이 더 효과적일 수 있으므로 사전 분석이 필요합니다.[1]

**확률론적 검증** - CRPS, quantile loss 등 다양한 확률론적 메트릭으로 예측 분포의 품질을 종합적으로 평가해야 합니다.[1]

**비선형성 검증** - Ablation study를 통해 특정 도메인에서 비선형 인코더의 필요성을 확인해야 합니다. 선형 관계가 지배적인 경우 더 단순한 모델이 충분할 수 있습니다.[1]

**재현성 확보** - 논문에서 3회 실행의 평균과 표준편차를 보고했듯이, 무작위 초기화에 따른 변동성을 고려해야 합니다.[1]

TLAE는 고차원 다변량 시계열 예측의 스케일러빌리티, 표현력, 확률론적 모델링을 동시에 달성한 획기적인 방법론으로, 향후 연구와 산업 응용에서 중요한 기준점이 될 것입니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/38e922f1-c121-44bf-97c3-25c38630eeed/2101.10460v1.pdf)
