# End-to-End Learning of Coherent Probabilistic Forecasts for Hierarchical Time Series

### 1. 핵심 주장과 주요 기여의 간결한 요약[1]

이 논문은 계층적 시계열(hierarchical time series)에 대한 일관성 있는 확률적 예측을 생성하기 위한 새로운 접근법을 제시합니다. 기존 방법들이 독립적인 기저 예측(base forecast)을 먼저 생성한 후 사후 처리(post-processing) 단계에서 조정하는 두 단계 절차를 따른 것과 달리, 이 연구의 핵심 혁신은 **학습과 조정을 하나의 통합된 엔드-투-엔드 모델로 결합**하는 것입니다.[1]

**주요 기여:**
- 계층 구조의 모든 시계열에서 동시에 학습하는 글로벌 모델 제안[1]
- 재매개변수화 기법(reparameterization trick)을 활용하여 샘플링 과정의 미분 가능성 확보[1]
- 조정 과정을 폐쇄형 해를 가진 볼록 최적화 문제로 구현[1]
- 확률적이면서도 일관성 있는(coherent) 예측을 사후 처리 없이 직접 생성[1]
- 그룹화된, 시간적, 교차 시간적 계층 구조를 포함한 일반적인 집계 제약 조건 지원[1]

---

### 2. 문제 정의, 제안 방법, 모델 구조, 성능 및 한계

#### 2.1 문제 정의[1]

계층적 시계열 예측의 핵심 과제는 두 가지입니다:

1. **일관성 보장(Coherency)**: 상위 수준의 예측값이 하위 수준 예측값들의 합과 일치해야 함[1]
2. **정보 활용**: 계층 구조 내 모든 수준의 정보를 동시에 활용하면서 정확한 예측 생성[1]

기존 접근법의 문제점은:
- 각 시계열에 대해 독립적으로 모델 학습, 정보 손실[1]
- 학습된 모델 매개변수를 고려하지 않고 기저 예측 수정[1]
- 대부분의 기존 방법이 점 예측(point forecast)만 생성, 확률적 예측(probabilistic forecast) 미지원[1]

#### 2.2 제안 방법의 수학적 표현[1]

**계층적 시계열의 정의:**

시간 $t$에서의 계층적 시계열을 $y_t \in \mathbb{R}^n$이라 하면, 이를 상위 수준($a_t$)과 하위 수준($b_t$)으로 분할:

$$
y_t = Sb_t \quad \Leftrightarrow \quad \begin{pmatrix} a_t \\ b_t \end{pmatrix} = \begin{pmatrix} S_{sum} \\ I_m \end{pmatrix} b_t
$$

여기서 $S$는 집계 행렬, $S_{sum}$은 합산 행렬, $I_m$은 $m \times m$ 항등 행렬입니다.[1]

동등한 표현:

$$
Ay_t = 0
$$

여기서 $A := [I_r | -S_{sum}] \in \{0,1\}^{r \times n}$이고, $r$은 집계된 시계열의 개수입니다.[1]

**일관성 정의:**[1]

점 예측 $\hat{y}_{T+h}$가 일관성 있다는 것:

$$
\hat{y}_{T+h} \in \mathcal{S} := \{y | y \in \text{null}(A)\}
$$

일관성 오차: $\hat{r}\_{T+h} = A\hat{y}_{T+h}$

**핵심 최적화 문제 - 샘플 투영(Sample Projection):**[1]

기저 예측 샘플 $\{\bar{y}_t\}$를 일관성 있는 부분공간으로 투영:

$$
\hat{y}_t = \arg \min_{y \in \mathbb{R}^n} \|y - \bar{y}_t\|_2^2 \quad \text{s.t.} \quad Ay = 0
$$

**폐쇄형 해:**

투영 행렬을 사용한 직접 계산:

$$
M := I - A^\top(AA^\top)^{-1}A
$$

따라서:

$$
\hat{y}_t = M\bar{y}_t \in \mathcal{S}
$$

이 행렬 $M$은 시간 불변이므로 훈련 전에 미리 계산할 수 있습니다.[1]

#### 2.3 모델 구조[1]

논문은 **DeepVAR**을 기본 다변량 예측자로 사용합니다:

**Deep VAR 모델:**

시간 $t$에서의 예측 분포 매개변수는 RNN의 출력:

$$
\Theta_t = \Psi(x_t, y_{t-1}, h_{t-1}; \Phi)
$$

여기서:
- $\Psi$: RNN의 재귀 함수
- $\Phi$: 전역 공유 매개변수
- $h_{t-1}$: 숨겨진 상태
- $\Theta_t = \{\mu_t, \Sigma_t\}$: 평균과 공분산 행렬 (가우시안 가정)[1]

대각 공분산 행렬 사용:
모델은 하위 수준 시계열 간의 비선형 관계를 더 잘 포착하기 위해 선택적으로 정규화 흐름(normalizing flow)을 통한 비선형 변환 지원.[1]

**재매개변수화 기법 기반 샘플링:**[1]

표준 정규분포에서의 샘플 $z \sim \mathcal{N}(0, I)$를 사용하여 예측 샘플 생성:

$$
\bar{y}_t = \mu_t + \Sigma_t^{1/2}z
$$

이 표현은 샘플링 과정을 네트워크 매개변수에 대해 미분 가능하게 만듭니다.[1]

**전체 모델 구조 (그림 기반):**[1]

1. 입력: 계층적 시계열 $y_t$, 시간 변수 $x_t$
2. DeepVAR 예측자: 분포 매개변수 $\Theta_t$ 생성
3. 샘플링: 재매개변수화를 통해 기저 예측 샘플 $\{\bar{y}_t\}$ 획득
4. 투영: 폐쇄형 투영 연산자 $M$ 적용하여 일관성 있는 샘플 $\{\hat{y}_t\}$ 생성
5. 손실 계산: 일관성 있는 샘플로부터 충분 통계량(sufficient statistics) $\Theta_t^c$ 계산하여 손실 정의[1]

#### 2.4 훈련 절차[1]

기존 DeepVAR 훈련과의 주요 차이:

**표준 DeepVAR 로그 가능도:**

$$
\ell(\Phi) = \sum_{t=1}^{T} p(y_t; \Theta_t)
$$

**계층적 모델의 로그 가능도:**

$$
\ell(\Phi) = \sum_{t=1}^{T} p(y_t; \Theta_t^c)
$$

여기서 $\Theta_t^c$는 일관성 있는 샘플 집합 $\{\hat{y}_t\}$에서 계산된 충분 통계량입니다.[1]

**핵심 특징:**[1]

- 로그 가능도 외에도 정량 손실(quantile loss), 연속 순위 확률 점수(CRPS) 등 다양한 손실 함수 활용 가능
- 각 네트워크 매개변수의 기울기 자동 계산 가능 (끝-끝 미분 가능성)

#### 2.5 성능 향상[1]

**실험 결과 (표 3 기준):**

| 데이터셋 | Hier-E2E (제안) | 최고 경쟁 방법 | 개선도 |
|---------|------------------|----------------|--------|
| Labour | 0.0340 | 0.0393 (PERMBU-MINT) | 13.5% ↓ |
| Traffic | 0.0376 | 0.0466 (ARIMA-ERM) | 19.3% ↓ |
| Tourism | 0.0834 | 0.0771 (PERMBU-MINT) | - |
| Tourism-L | 0.1520 | 0.1609 (ARIMA-MinT-shr) | 5.5% ↓ |
| Wiki | 0.2038 | 0.2206 (ARIMA-ERM) | 7.6% ↓ |

4개 데이터셋에서 명시적인 개선을 보임 (CRPS 기준, 낮을수록 우수).[1]

**성능 향상의 원인:**[1]

1. **글로벌 학습의 이점**: 모든 시계열을 동시에 학습하므로 정보 공유로 인한 성능 개선
2. **일관성 강화 학습**: 훈련 단계에서 일관성을 직접 적용하므로 기저 예측이 이미 더 정확
3. **계층 구조 활용**: 상위 수준 패턴으로부터 하위 수준의 희소한 시계열 학습 개선

**집계 수준별 성능 일관성:**[1]

제안된 방법은 25개 총 수준 중:
- 12개 수준에서 최고 성능 (굵은글씨)
- 13개 수준에서 2위 성능 (이탤릭)
- 다른 방법들은 특정 수준에서만 우수 성능 보임

#### 2.6 한계[1]

**1. 가우시안 가정:**
- 모델이 대각 공분산 행렬을 가정하므로, 하위 수준 시계열 간의 복잡한 비선형 관계 표현에 제한
- 해결책: 정규화 흐름을 사용한 비선형 변환 제안했으나 실제 구현 미흡

**2. 계산 복잡성:**
- 많은 시계열이 있을 경우 RNN 기반 모델의 계산 비용 증가

**3. 희소 데이터 문제:**
- 하위 수준 시계열이 매우 희소할 경우, 글로벌 학습의 이점이 제한될 수 있음

**4. 투영 행렬 계산:**
- $AA^\top$의 역행렬이 존재해야 함 (계층 구조의 경우 보장이나, 일반적인 제약의 경우 추가 최적화 필요)

**5. Tourism 데이터셋 성능:**
- Tourism 데이터셋에서 PERMBU-MINT에 비해 성능이 약간 떨어짐 (0.0834 vs 0.0771)
- 특정 데이터 특성에 대한 일반화 한계 시사

---

### 3. 모델의 일반화 성능 향상 가능성[1]

#### 3.1 현재 일반화 성능 분석[1]

**학습 효율성 개선:**
- DeepVAR 단독 사용 (0.0382) vs Hier-E2E (0.0340) on Labour: 11% 개선
- 이는 글로벌 학습으로부터의 일반화 성능 향상을 시사

**교차 수준 일관성:**
제안된 방법이 모든 수준에서 일관되게 좋은 성능을 유지하는 것은 계층 구조의 제약이 정규화(regularization) 역할을 한다는 의미입니다.[1]

#### 3.2 일반화 성능 향상을 위한 가능한 방향[1]

**1. 비선형 변환 활용:**

논문에서 제시한 정규화 흐름(normalizing flows)의 활용을 더 적극화:

$$
y_t = f^{-1}(z_t; \theta_f)
$$

여기서 $f$는 학습 가능한 비선형 변환, $z_t$는 잠재 변수입니다. 이를 통해:
- 비가우시안 데이터 분포에 대한 일반화 개선[1]
- 하위 수준 시계열 간의 복잡한 의존 구조 학습 가능[1]

**2. 계층 구조 기반 정규화:**

투영 행렬을 정규화 항으로 통합:

$$
\mathcal{L}(\Phi) = \sum_{t=1}^{T} p(y_t; \Theta_t^c) + \lambda \|A\bar{y}_t\|_2^2
$$

여기서 $\lambda$는 정규화 가중치입니다. 이는:
- 훈련 단계에서 부분적 일관성 강화[1]
- 모델이 계층 구조를 더 잘 학습하도록 유도[1]

**3. 멀티태스크 학습:**

다양한 수준에 대한 별도의 헤드 네트워크를 추가:

$$
\mu_t^{(l)}, \Sigma_t^{(l)} = \text{Head}_l(\text{DeepVAR}(\cdot))
$$

여기서 $l$은 계층 수준입니다. 이는:
- 각 수준의 특성에 맞는 분포 학습[1]
- 계층 간 정보 공유로 인한 일반화 개선[1]

**4. 데이터 수준의 증강:**

적응적 데이터 증강 (Adaptive Data Augmentation):
- 희소한 하위 시계열에 대해 계층 구조를 활용한 합성 데이터 생성[1]
- 부트스트랩 방식의 재샘플링을 통한 데이터 다양성 증가[1]

#### 3.3 실증적 일반화 성능 지표[1]

**교차 검증 성능 (표 4 기준):**

표 4에서 제시된 집계 수준별 성능 분석은 다음을 시사합니다:

- **Labour**: 모든 4개 수준에서 PERMBU-MINT보다 우수
- **Traffic**: 수준 1-3에서 우수, 수준 4에서 ARIMA-ERM에 비해 열위 (과적합 가능성)
- **Tourism**: 수준 1-2에서 우수, 수준 3-4에서 열위 (희소 하위 시계열의 어려움)
- **Wiki**: 수준 1-4에서 일관되게 우수, 수준 5에서만 열위

**일반화 문제의 원인:**
1. 예측 수평(forecast horizon)이 길어질수록 일관성 유지 어려움[1]
2. 매우 희소한 하위 시계열에서의 불안정한 예측[1]

***

### 4. 논문의 영향과 향후 연구 고려사항

#### 4.1 학문적 영향[1]

**1. 패러다임 전환:**
- 두 단계 절차 → 엔드-투-엔드 학습으로의 전환
- 확률적 예측의 실용화: 리스크 관리와 의사 결정 개선에 직접 활용 가능[1]

**2. 기술적 혁신:**
- 재매개변수화 기법의 실용적 적용 확대[1]
- 미분 가능한 볼록 최적화 계층(DCL)의 신경망 통합 가능성 제시[1]

**3. 일반화 가능성:**
- 제안된 방법론이 DeepVAR뿐 아니라 다른 다변량 예측 모델과도 결합 가능[1]
- 다양한 손실 함수 적용 가능 (CRPS, 정량 손실 등)[1]

#### 4.2 실무적 영향[1]

**1. 산업 응용:**
- 소매 판매 예측 (다단계 계층)
- 전력 수요/공급 예측 (지역별, 시간대별 계층)
- 교통 흐름 예측 (도로, 지역, 광역 수준)

**2. 의사 결정 개선:**
- 일관된 확률적 예측으로 인한 신뢰성 향상[1]
- 다양한 시나리오 계획 가능[1]

#### 4.3 향후 연구 시 고려할 점[1]

**1. 비가우시안 데이터 처리 강화:**
- 현재 제시된 정규화 흐름의 실제 구현 및 검증[1]
- Gamma, Beta, Student-t 분포 등의 체계적 평가[1]
- 혼합 분포(mixture distributions) 지원 확대[1]

**2. 대규모 계층 구조 확장성:**
- 수천 개 이상의 시계열을 가진 초대규모 데이터셋에 대한 성능 평가[1]
- 계산 효율성 개선 (분산 처리, 근사 기법)[1]

**3. 고급 제약 조건 통합:**
- 비음수 제약(non-negativity) 직접 통합[1]
- 사용자 정의 비선형 제약 조건 지원[1]

**4. 적응적 일관성 균형:**
- 정확성과 일관성 간의 트레이드오프 자동 최적화[1]
- 데이터 또는 애플리케이션별 맞춤형 가중치 학습[1]

**5. 다중 계층 구조:**
- 지리적, 시간적, 제품 범주 등 여러 계층 구조가 교차하는 복잡한 시나리오[1]
- 그룹화된 계층(grouped hierarchies) 및 교차 시간적 계층(cross-temporal hierarchies)의 체계적 평가[1]

**6. 불확실성 정량화:**
- 예측 구간의 신뢰도 평가[1]
- 캘리브레이션(calibration) 개선 방법[1]

**7. 비교 연구 확대:**
- 최근 발표된 다른 엔드-투-엔드 방법들과의 비교 (예: Han et al., 2021)[1]
- 다양한 신경망 아키텍처(Transformer, Graph Neural Networks 등)와의 결합 연구[1]

#### 4.4 방법론적 개선 제안[1]

**1. 위계적 손실 함수:**

서로 다른 수준에 다른 가중치를 부여하는 가중 손실:

$$
\mathcal{L}(\Phi) = \sum_{l=1}^{L} w_l \sum_{t=1}^{T} p(y_t^{(l)}; \Theta_t^{c,(l)})
$$

여기서 $w_l$은 수준별 가중치, $L$은 총 수준 수입니다.[1]

**2. 적응적 정규화:**

훈련 진행도에 따라 조정되는 정규화 항:

$$
\mathcal{L}(\Phi) = \sum_{t=1}^{T} p(y_t; \Theta_t^c) + \lambda(epoch) \|A\bar{y}_t\|_2^2
$$

여기서 $\lambda(epoch)$는 에포크 진행에 따라 점진적으로 증가합니다.[1]

---

### 결론

"End-to-End Learning of Coherent Probabilistic Forecasts for Hierarchical Time Series"는 계층적 시계열 예측 분야에서 **패러다임적 전환**을 이룬 중요한 연구입니다. 기존의 두 단계 절차를 단일 엔드-투-엔드 모델로 통합함으로써 정확성과 일관성을 동시에 달성했습니다.[1]

이 논문의 가장 큰 기여는 기술적 혁신(재매개변수화, 미분 가능 투영)뿐 아니라, **학습과 제약 조건을 통합하는 새로운 패러다임**을 제시함에 있습니다. 향후 연구는 비가우시안 데이터 처리, 대규모 계층 구조 확장성, 다양한 신경망 아키텍처와의 결합 등을 통해 이 방법론의 영향력을 더욱 확대할 수 있을 것입니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/4a3502b4-2adf-49db-97d4-c7062060eddb/rangapuram21a.pdf)
