# Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting

### 1. 핵심 주장 및 주요 기여

**핵심 주장**: 기존 Transformer 기반 시계열 예측 모델들은 세 가지 근본적인 문제에 직면해 있습니다. 첫째, 장기 시계열의 복잡한 시간적 패턴으로 인해 신뢰할 수 있는 의존성 발견이 어렵습니다. 둘째, Transformer의 이차 복잡도로 인한 계산 병목이 존재합니다. 셋째, 기존 희소 주의(sparse attention) 메커니즘들은 점 단위 표현 집계에만 의존하여 정보 활용률이 저하됩니다.[1]

**주요 기여**:

Autoformer는 두 가지 혁신적인 설계를 제시합니다. 첫째, **점진적 분해 아키텍처(Progressive Decomposition Architecture)**로, 기존의 사전 처리 분해와 달리 심층 모델 내부에 분해 블록을 임베딩하여 예측 과정 전체에서 추세와 계절 성분을 단계적으로 분리합니다. 둘째, **자기상관 메커니즘(Auto-Correlation Mechanism)**으로, 시계열의 주기성을 기반으로 부분 시계열 수준의 의존성을 발견하고 집계하여 $$O(L \log L)$$ 복잡도를 달성하면서도 정보 활용률을 극대화합니다. 실험 결과, 6개 벤치마크에서 38% 평균 MSE 감소를 달성했으며, 에너지, 교통, 경제, 날씨, 질병 등 5가지 실제 응용 분야에서 최첨단 성능을 보입니다.[1]

---

### 2. 문제 정의 및 제안 방법

#### 2.1 문제 정의

장기 시계열 예측(Long-term forecasting) 문제는 과거 I개의 시간 단계를 입력받아 미래 O개의 시간 단계(O가 상대적으로 큼)를 예측하는 작업입니다. 이는 극한 기상 조기 경보, 장기 에너지 소비 계획 등 실제 응용에서 중요한 수요입니다.[1]

**주요 문제점**:

1. **복잡한 시간적 패턴**: 장기 미래의 시계열은 추세(trend), 계절성(seasonality), 랜덤 변동 등 다양한 성분이 얽혀 있어, 직접적인 의존성 발견이 신뢰하기 어렵습니다.[1]

2. **계산 효율성 병목**: 표준 Transformer의 자기 주의(self-attention)는 $$O(L^2)$$ 복잡도로 장기 시계열에 대해 계산 비용이 매우 높습니다.[1]

3. **정보 활용 병목**: 희소 주의 기법들(LogSparse, LSH, ProbSparse)은 점 단위의 의존성만 고려하므로, 효율성을 개선하면서도 정보 활용률을 희생합니다.[1]

#### 2.2 제안 방법: 점진적 분해 아키텍처

**시계열 분해 블록**:

기존의 시계열 분해는 사전 처리로만 사용되어 과거 시계열에만 적용 가능했습니다. Autoformer는 이를 혁신하여 모델 내부 연산으로 통합합니다. 이동 평균을 이용한 분해:[1]

$$
X_t = \text{AvgPool}(\text{Padding}(X))
$$

$$
X_s = X - X_t
$$

여기서 $$X_s$$는 계절(seasonal) 성분, $$X_t$$는 추세-순환(trend-cyclical) 성분입니다. 이를 $$X_s, X_t = \text{SeriesDecomp}(X)$$로 표기합니다.[1]

**인코더 구조**:

인코더는 계절 성분에 집중하며, 각 층에서 다음과 같이 작동합니다:

$$
S^{l,1}_{\text{en}}, \cdot = \text{SeriesDecomp}(\text{Auto-Correlation}(X^{l-1}_{\text{en}}) + X^{l-1}_{\text{en}})
$$

$$
S^{l,2}_{\text{en}}, \cdot = \text{SeriesDecomp}(\text{FeedForward}(S^{l,1}_{\text{en}}) + S^{l,1}_{\text{en}})
$$

각 분해 블록에서 추세는 제거되고(·으로 표시), 계절 성분만 다음 층으로 전달됩니다.[1]

**디코더 구조**:

디코더는 두 부분으로 구성됩니다: 추세-순환 성분의 축적 구조와 계절 성분의 단계적 정제입니다.[1]

$$
S^{l,1}_{\text{de}}, T^{l,1}_{\text{de}} = \text{SeriesDecomp}(\text{Auto-Correlation}(X^{l-1}_{\text{de}}) + X^{l-1}_{\text{de}})
$$

$$
S^{l,2}_{\text{de}}, T^{l,2}_{\text{de}} = \text{SeriesDecomp}(\text{Auto-Correlation}(S^{l,1}_{\text{de}}, X^N_{\text{en}}) + S^{l,1}_{\text{de}})
$$

$$
S^{l,3}_{\text{de}}, T^{l,3}_{\text{de}} = \text{SeriesDecomp}(\text{FeedForward}(S^{l,2}_{\text{de}}) + S^{l,2}_{\text{de}})
$$

$$
T^l_{\text{de}} = T^{l-1}_{\text{de}} + W^{l,1} * T^{l,1}_{\text{de}} + W^{l,2} * T^{l,2}_{\text{de}} + W^{l,3} * T^{l,3}_{\text{de}}
$$

최종 예측은 $$W_S \cdot X^M_{\text{de}} + T^M_{\text{de}}$$입니다.[1]

#### 2.3 자기상관 메커니즘

**주기 기반 의존성 발견**:

확률 과정 이론에서 영감을 받아, 시계열의 자기상관을 계산합니다:[1]

$$
R_{XX}(\tau) = \lim_{L \to \infty} \frac{1}{L} \sum_{t=1}^{L} X_t X_{t-\tau}
$$

여기서 $$\tau$$는 시간 지연(time delay)이며, $$R_{XX}(\tau)$$는 해당 주기의 신뢰도입니다. 쿼리 Q와 키 K 사이의 자기상관을 계산하여 상위 k개의 주기를 선택합니다.[1]

**시간 지연 집계**:

선택된 주기 $$\tau_1, \cdots, \tau_k$$를 기반으로 값(Value)을 이동시켜 동일한 주기 위치의 부분 시계열을 정렬합니다:[1]

$$
\tau_1, \cdots, \tau_k = \arg \text{Topk}_{\tau \in \{1, \cdots, L\}}(R_{Q,K}(\tau))
$$

$$
\hat{R}_{Q,K}(\tau_1), \cdots, \hat{R}_{Q,K}(\tau_k) = \text{SoftMax}(R_{Q,K}(\tau_1), \cdots, R_{Q,K}(\tau_k))
$$

$$
\text{Auto-Correlation}(Q, K, V) = \sum_{i=1}^{k} \text{Roll}(V, \tau_i) \odot \hat{R}_{Q,K}(\tau_i)
$$

여기서 $$\text{Roll}(X, \tau)$$는 시계열을 $$\tau$$만큼 순환 이동하는 연산이고, $$\odot$$는 원소별 곱셈입니다.[1]

다중 헤드 버전은:

$$
\text{MultiHead}(Q, K, V) = W_{\text{output}} * \text{Concat}(\text{head}_1, \cdots, \text{head}_h)
$$

여기서 $$\text{head}_i = \text{Auto-Correlation}(Q_i, K_i, V_i)$$입니다.[1]

**복잡도 분석**:

Fast Fourier Transform(FFT)을 이용하여 모든 지연의 자기상관을 한 번에 계산하며:[1]

$$
S_{XX}(f) = \mathcal{F}(X_t) \mathcal{F}^*(X_t)
$$

$$
R_{XX}(\tau) = \mathcal{F}^{-1}(S_{XX}(f))
$$

$$k = \lfloor c \times \log L \rfloor$$개의 주기를 집계하므로, 전체 복잡도는 $$O(L \log L)$$입니다.[1]

***

### 3. 모델 구조

#### 3.1 전체 아키텍처

Autoformer는 표준 인코더-디코더 구조를 따르되, 내부 연산이 완전히 혁신되었습니다:[1]

- **인코더** (N=2 층): 계절 성분 모델링에 집중, Auto-Correlation과 피드포워드 층이 각각 분해 블록으로 감싸임
- **디코더** (M=1 층): 추세-순환 성분의 누적 구조 + 계절 성분의 점진적 정제 + 인코더-디코더 크로스 Auto-Correlation

#### 3.2 입력 초기화

인코더는 과거 I개 시간 단계를 입력으로 받습니다. 디코더는 두 개의 초기화된 성분을 받습니다:[1]

$$
X^{\text{en}}_s, X^{\text{en}}_t = \text{SeriesDecomp}(X_{\text{en}}[I/2:I])
$$

$$
X^{\text{de}}_s = \text{Concat}(X^{\text{en}}_s, X_0), \quad X^{\text{de}}_t = \text{Concat}(X^{\text{en}}_t, X_{\text{Mean}})
$$

계절 성분은 0으로, 추세 성분은 평균값으로 채웁니다. 이는 최근의 계절성을 반영하면서도 미래 추세 예측을 위한 기초를 제공합니다.[1]

#### 3.3 핵심 설계 선택

1. **점진적 분해**: 각 디코더 층에서 추세를 추출하여 최종적으로 누적하므로, 점진적인 추세 정제가 가능합니다.[1]

2. **부분 시계열 수준 집계**: 자기상관은 점 단위가 아닌 주기 기반 부분 시계열을 집계하므로, 희소 주의보다 정보 활용도가 높습니다.[1]

3. **복잡한 계절성 포착**: FFT 기반 자기상관은 원본 시계열과 다른 깊은 표현에서 학습된 주기를 발견할 수 있어, 복잡한 계절 패턴을 자동으로 모델링합니다.[1]

---

### 4. 성능 향상 및 실험 결과

#### 4.1 주요 성능 개선

**다변량 예측**:

Autoformer는 6개 벤치마크의 모든 예측 길이 설정에서 최첨단 성능을 달성했습니다:[1]

- **ETT 데이터셋** (input-96-predict-336): 74% MSE 감소 (1.334 → 0.339)
- **Electricity**: 18% 감소 (0.280 → 0.231)
- **Exchange**: 61% 감소 (1.357 → 0.509)
- **Traffic**: 15% 감소 (0.733 → 0.622)
- **Weather**: 21% 감소 (0.455 → 0.359)
- **ILI** (input-36-predict-60): 43% 감소 (4.882 → 2.770)

**평균 38% MSE 감소**를 달성했습니다.[1]

| 데이터셋 | 예측 길이 | Autoformer | Informer | 개선율 |
|---------|---------|-----------|---------|-------|
| ETT | 336 | 0.339 | 1.363 | 74% |
| Electricity | 336 | 0.231 | 0.300 | 18% |
| Exchange | 336 | 0.509 | 1.672 | 61% |
| Traffic | 336 | 0.622 | 0.777 | 15% |

**단변량 예측**:

더 광범위한 기준선(N-BEATS, DeepAR, Prophet, ARIMA)과의 비교에서도 우수한 성능을 보였습니다. 특히 교환율 데이터처럼 명확한 주기성이 없는 데이터셋에서도 17% (0.611 → 0.508) MSE 감소를 달성했습니다.[1]

#### 4.2 장기 안정성

Autoformer는 예측 길이가 증가함에 따라 성능 저하가 완만합니다. 이는 기존 방법들이 예측이 길어질수록 급격하게 성능이 저하되는 것과 대비되며, 실제 응용에서 중요한 특성입니다.[1]

#### 4.3 소거 연구(Ablation Studies)

**분해 아키텍처의 효과**:

표 3에서 분해 블록 없이는 예측 길이가 증가할수록 성능이 급격히 저하되지만, 제안된 점진적 분해를 적용하면 안정적인 성능을 유지합니다. 또한 사전 분해(separate prediction) 방식보다 우수합니다.[1]

| 모델 | Predict-96 | Predict-192 | Predict-336 | Predict-720 |
|-----|-----------|-----------|-----------|-----------|
| 분해 없음 | 0.604 | 1.060 | 1.413 | 2.672 |
| 분해 (Separate) | 0.311 | 0.760 | 0.665 | 3.200 |
| 분해 (점진적) | 0.204 | 0.266 | 0.375 | 0.537 |

**자기상관 vs 자기 주의**:

표 4에서 Autoformer의 자기상관이 모든 설정에서 Full Attention, LogSparse, LSH, ProbSparse 주의를 능가합니다. 특히 긴 입력 길이(I=336)에서 다른 방법들은 메모리 부족 오류가 발생하는 반면, Autoformer는 안정적으로 작동합니다.[1]

**복잡한 계절성 모델링**:

Figure 6에서 보듯이, Autoformer가 학습하는 주기는 실제 시계열의 주기와 일치합니다:[1]
- Electricity (시간 기준): 일일 주기
- Exchange (일일 기준): 월간, 분기, 연간 주기
- Traffic (시간 기준): 24시간, 168시간(1주) 주기
- Weather (10분 기준): 일일 주기

#### 4.4 계산 효율성

Figure 7에서 Autoformer는 $$O(L \log L)$$ 복잡도로 메모리와 실행 시간 모두에서 자기 주의 기반 모델들을 능가합니다. 특히 긴 시계열에서 이점이 더욱 두드러집니다.[1]

***

### 5. 일반화 성능 및 강건성

#### 5.1 일반화 능력 향상

**데이터 특성에 무관한 성능**:

Autoformer는 명확한 주기성이 있는 데이터셋(ETT, Electricity, Traffic)뿐만 아니라, **주기성이 약하거나 불규칙한 데이터셋(Exchange, ILI)**에서도 우수한 성능을 보입니다. 이는 Auto-Correlation 메커니즘이 선형 회귀나 시계열 분해와 달리 깊은 표현에서 숨겨진 주기성을 자동으로 발견할 수 있음을 시사합니다.[1]

**전이 가능성**:

표 3의 소거 연구에서 제안된 점진적 분해 아키텍처를 다른 모델(Transformer, Informer, LogTrans, Reformer)에 적용하면 모든 모델의 성능이 향상됩니다. 예를 들어:[1]
- Transformer: 1.413 → 0.375 (73% 개선, predict-336)
- Informer: 1.363 → 0.481 (65% 개선)
- LogTrans: 1.334 → 0.362 (73% 개선)
- Reformer: 1.549 → 0.366 (76% 개선)

이는 점진적 분해가 일반적인 원칙이며 다양한 모델에 효과적임을 보여줍니다.

#### 5.2 초매개변수 민감도

표 6에서 $$c$$ 값(자기상관에서 선택할 주기 개수 $$k = \lfloor c \times \log L \rfloor$$)에 대한 민감도 분석이 제시됩니다. 일반적으로 $$c \in $$ 범위에서 성능이 안정적이며, 주기성이 명확한 데이터셋은 더 큰 $$c$$ 값(성능과 효율의 균형)을, 주기성이 약한 데이터셋은 더 작은 $$c$$ 값을 선호합니다.[1]

#### 5.3 입력 길이의 영향

표 7에서 입력 길이 I의 영향을 분석합니다:[1]
- **주기성이 있는 데이터** (ETT): 입력 길이 96으로도 충분
- **주기성이 약한 데이터** (ILI): 더 긴 입력이 필요 (36 이상)

이는 모델이 데이터 특성에 따라 적응적으로 정보를 활용함을 시사합니다.

#### 5.4 강건성

**극한 상황 처리**:

COVID-19 데이터셋 사례 연구(Appendix F)에서 Autoformer는 훈련 데이터가 제한적인 상황에서도 피크와 골짜기를 정확히 예측하며, 전염병 방제에 필수적인 극한값과 장기 추세 예측에서 우수합니다.[1]

**실패 사례 분석**:

논문은 "극한 경우 분석(Model Robustness)" 섹션에서 다음을 명시합니다: 데이터가 완전히 랜덤이거나 극도로 약한 시간적 일관성을 가진 경우, Autoformer를 포함한 모든 모델이 성능 저하를 보일 수 있습니다. 이는 현재 예측 모델의 근본적인 한계입니다.[1]

***

### 6. 한계(Limitations)

#### 6.1 설계 관련 한계

1. **이동 평균의 제약**: 분해에 사용되는 이동 평균 커널 크기(k=25)는 고정되어 있습니다. 적응적 커널 크기 결정이나 다양한 분해 알고리즘의 탐색이 향후 연구 방향입니다.[1]

2. **주기 발견의 한계**: 자기상관 기반 주기 발견은 명확한 주기성이 존재할 때 우수하지만, 매우 복잡하거나 중첩된 주기를 가진 시계열에서는 한계가 있을 수 있습니다.[1]

3. **긴 시계열의 메모리 비용**: 비록 $$O(L \log L)$$의 선형 복잡도이지만, 초기 분석에서는 매우 긴 시계열(L > 8,000)에 대한 평가가 부족합니다.[1]

#### 6.2 실험 관련 한계

1. **모델 크기 비교 부공정성**: Autoformer는 2개 인코더 + 1개 디코더 층으로 제한되는 반면, 일부 비교 기준선은 더 많은 층을 사용합니다. 공정한 매개변수 수 기준의 비교가 필요합니다.[1]

2. **데이터셋 한정성**: 6개 벤치마크가 시계열 예측 커뮤니티에서 널리 사용되지만, 도메인 다양성 확대(예: 금융, 기후, 의료)가 도움이 될 수 있습니다.

3. **통계적 유의성**: 표 10의 표준 편차를 보면, 일부 개선이 오류 범위 내에 있을 수 있습니다. 더 엄격한 통계 검정이 필요합니다.[1]

#### 6.3 방법론적 한계

1. **분해의 인과성**: 점진적 분해가 인코더와 디코더에서 독립적으로 작동하므로, 추세와 계절 성분 간의 상호 작용이 완전히 모델링되지 않을 수 있습니다.[1]

2. **다변량 시계열의 채널 간 상호작용**: Auto-Correlation이 각 채널 독립적으로 작동하므로, 서로 다른 변수 간의 복잡한 상호작용이 제한적으로 모델링됩니다.[1]

3. **외생 변수 미지원**: 현재 Autoformer는 순수 시계열 정보만 활용하며, 외생 변수(exogenous variables)를 통합하는 메커니즘이 없습니다.[1]

---

### 7. 향후 연구에 미치는 영향 및 고려사항

#### 7.1 학문적 영향

**1. 시계열 분해의 재해석**:

이 논문은 전통적인 시계열 분해가 단순 전처리가 아니라 심층 신경망의 핵심 구성 요소가 될 수 있음을 보여줍니다. 향후 연구는 다양한 분해 방식(STL, Hodrick-Prescott, Christiano-Fitzgerald 필터)의 학습 가능한 버전을 탐색할 수 있습니다.[1]

**2. 주기성 기반 주의 메커니즘**:

Auto-Correlation이 점 단위 주의의 대안으로 제시되었으며, 이는 자연어 처리나 컴퓨터 비전의 주의 메커니즘을 재고하는 계기가 될 수 있습니다. 예를 들어, 자연어의 문법적 구조나 이미지의 반복 패턴도 이러한 방식으로 모델링될 수 있을 것입니다.[1]

**3. 멀티스케일 시계열 모델링**:

점진적 분해 아키텍처는 다양한 시간 스케일에서의 패턴을 동시에 모델링하는 기초를 제공하며, 멀티스케일 또는 계층적 시계열 분석으로 확장될 수 있습니다.[1]

#### 7.2 실무 적용 시 고려사항

**1. 초매개변수 튜닝**:

실무에서는 데이터의 주기성 특성에 따라 $$c$$ 값(자기상관 주기 개수)과 입력 길이를 신중하게 선택해야 합니다:[1]
- 명확한 주기성: $$c = 2-3$$, 입력 길이 = 2-3 주기
- 약한 주기성: $$c = 1-2$$, 입력 길이 = 더 김 (예: 2개월)

**2. 계산 자원 최적화**:

$$O(L \log L)$$ 복잡도에도 불구하고, FFT 계산과 Roll 연산은 GPU 최적화가 필수적입니다. 논문의 "Speedup Version" (Algorithm 3-4)을 이용하여 배치 정규화 스타일의 최적화가 가능합니다.[1]

**3. 도메인 적응**:

전이 학습을 통해 사전 훈련된 Autoformer를 특정 도메인(에너지, 교통 등)에 파인 튜닝할 수 있으며, 표 3의 결과가 시사하듯이 각 모델에 분해 블록을 추가하면 성능이 향상됩니다.[1]

#### 7.3 향후 연구 방향

**1. 다변량 의존성 모델링**:

현재 Auto-Correlation은 각 채널을 독립적으로 처리합니다. 향후 연구는 채널 간 교차 상관(cross-correlation)을 고려한 멀티채널 자기상관을 설계할 수 있습니다.[1]

**2. 적응적 분해**:

고정된 이동 평균 커널 크기 대신, 각 시점과 채널별로 적응적으로 결정하는 메커니즘을 도입하면 복잡한 시계열에 더 잘 적응할 수 있을 것입니다.[1]

**3. 불확실성 정량화**:

현재 Autoformer는 점 추정만 제공합니다. 향후 연구는 베이지안 접근법이나 확률적 모델을 통합하여 예측 불확실성을 정량화할 수 있습니다.[1]

**4. 외생 변수 통합**:

날씨(강우, 기온), 휴일, 특수 이벤트 등 외생 변수를 모델에 통합하면 실무 예측 성능을 크게 향상시킬 수 있습니다.[1]

**5. 극단 값 처리**:

COVID-19 사례 연구가 극단 상황에서의 우수성을 보여주지만, 이상 탐지(anomaly detection)와의 통합 연구가 필요합니다.[1]

***

### 8. 결론

Autoformer는 두 가지 핵심 혁신을 통해 장기 시계열 예측에서 획기적인 성과를 달성했습니다: **점진적 분해 아키텍처**로 복잡한 시간적 패턴을 단계적으로 분해하고, **자기상관 메커니즘**으로 주기성 기반의 효율적이면서도 정보 활용도 높은 의존성 발견을 가능하게 했습니다.[1]

실험 결과는 38% 평균 성능 향상과 더불어 강건성, 전이 가능성, 그리고 실무 적용 가능성을 모두 보여줍니다. 특히 비주기적 데이터셋에서도 우수한 성능을 달성하고, 컴퓨팅 효율성을 개선했다는 점이 중요합니다.[1]

향후 연구는 주로 다변량 의존성 확장, 적응적 분해 메커니즘, 외생 변수 통합 등에 집중될 것으로 예상되며, 이러한 개선사항들이 실무 예측 시스템의 성능을 더욱 향상시킬 것입니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/f4511b48-62f0-4e47-a3f4-6b8da46dc1bc/2106.13008v5.pdf)
