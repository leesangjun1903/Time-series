<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# 1. 당신은 AI 분야의 연구자입니다. "TimeKAN: KAN-based Frequency Decomposition Learning Architecture for Long-term Time Series Forecasting"의 핵심 주장과 주요 기여를 간결하게 요약하세요.

2. 이 논문이 "해결하고자 하는 문제, 제안하는 방법(수식 포함), 모델 구조, 성능 향상 및 한계"를 중심으로 자세하게 설명하세요.
3.특히 "모델의 일반화 성능 향상 가능성"과 관련된 내용을 중점적으로 다뤄 주세요.
4.요약 후에는 "해당 논문이 앞으로의 연구에 미치는 영향과 앞으로 연구 시 고려할 점"도 자세히 기술해 주세요.
“2020년 이후 관련 최신 연구 비교 분석”
수식을 LaTeX 코드로 답변 형식을 맞춰주고, LaTeX 렌더링을 위해 마크다운에 달러 기호를 사용해 주세요.

이제 수집된 정보를 바탕으로 상세한 분석 답변을 작성하겠습니다.

***

# TimeKAN: KAN 기반 주파수 분해 학습 아키텍처 심층 분석


***

## 1. 핵심 주장 및 주요 기여 요약

TimeKAN은 **ICLR 2025**에 발표된 논문으로, 실세계 시계열 데이터에 혼재된 다중 주파수 성분을 효과적으로 분리하고, Kolmogorov-Arnold Network(KAN)의 유연성을 활용하여 각 주파수 대역에 특화된 패턴 학습을 수행하는 경량 예측 아키텍처입니다. 핵심 주장은 **"균일한 모델링 방식이 아닌, 주파수 대역별 특성에 맞는 서로 다른 복잡도의 KAN을 적용함으로써 예측 정확도와 경량성을 동시에 달성할 수 있다"**는 것입니다.[^1_1]

주요 기여를 정리하면 다음과 같습니다:

- **주파수 분해 관점의 재정립:** 트렌드/계절성 분해가 아닌, 다중 주파수 성분 분리(Decomposition-Learning-Mixing) 프레임워크를 제안[^1_1]
- **M-KAN 블록 설계:** KAN의 유연성을 극대화하기 위해, 저주파에는 저차(low-order), 고주파에는 고차(high-order) Chebyshev KAN을 적용하는 다중 차수 KAN 설계[^1_1]
- **극단적 경량성:** 동일 성능 대비 파라미터 수가 PatchTST 대비 최대 295배 적으며 SOTA 수준의 예측 성능 달성[^1_1]

***

## 2. 해결하고자 하는 문제

### 문제 정의

실세계 다변량 시계열 $\mathbf{X} \in \mathbb{R}^{N \times T}$는 여러 주파수 성분이 뒤섞여 있습니다. 기존 분해 기법들(Autoformer, DLinear, TimeMixer 등)은 단순한 **계절-트렌드 분해**나 **다중 스케일 분해**에 그쳤는데, 이는 두 가지 문제를 낳습니다:[^1_1]

1. **주파수 간 정보 밀도 차이 무시:** 저주파 성분은 장기 주기 변화, 고주파 성분은 급격한 이벤트를 나타내며 복잡도가 다른데, 이를 동일한 모델로 처리하면 부정확한 패턴 표현이 발생[^1_1]
2. **KAN의 잠재력 미활용:** 기존 연구들은 KAN을 단순히 MLP의 대체재로만 사용하여 KAN의 핵심 유연성(차수 조절)을 활용하지 못함[^1_1]

***

## 3. 제안하는 방법 및 수식

### 3.1 전체 아키텍처 (Decomposition-Learning-Mixing)

TimeKAN은 세 핵심 블록으로 구성됩니다: **CFD → M-KAN → Frequency Mixing**.[^1_1]

**Step 1. 계층적 시퀀스 전처리 (Hierarchical Sequence Preprocessing)**

이동평균으로 원시 시계열 $X$에서 고주파 성분을 단계적으로 제거하여 다중 레벨 시퀀스 $\{x_1, \cdots, x_k\}$를 생성합니다:

$x_i = \text{AvgPool}(\text{Padding}(x_{i-1})) \tag{1}$

이후 각 레벨 시퀀스를 고차원 임베딩 공간으로 투영합니다:

$x_i = \text{Linear}(x_i), \quad x_i \in \mathbb{R}^{\frac{T}{d^{i-1}} \times D} \tag{2}$

**Step 2. 계단식 주파수 분해 (CFD Block)**

FFT를 이용한 Frequency Upsampling으로 $i$번째 주파수 성분의 표현 $\hat{x}_i$를 추출합니다:

$\hat{x}_i = \text{IFFT}(\text{Padding}(\text{FFT}(x_{i+1}))) \tag{3}$

그리고 잔차(residual)를 통해 $i$번째 주파수 성분 $f_i$를 분리합니다:

$f_i = x_i - \hat{x}_i \tag{4}$

**Step 3. 다중 차수 KAN 표현 학습 (M-KAN Block)**

**Depthwise Convolution** (시간 의존성 포착):

$f_{i,1} = \text{Conv}_{D \rightarrow D}(f_i, \text{group}=D) \tag{5}$

**Multi-order ChebyshevKAN** (채널별 주파수 표현 학습):

$T_n(x) = \cos(n \arccos(x)) \tag{6}$

$\phi_o(x) = \sum_{j=1}^{D} \sum_{i=0}^{n} \Theta_{o,j,i} T_i(\tanh(x_j)) \tag{7}$

여기서 $\Theta \in \mathbb{R}^{D \times D \times (n+1)}$은 학습 가능한 계수입니다. 주파수 레벨 $i$에 대한 KAN 차수는 하한 $b$를 기준으로 **저주파일수록 낮은 차수, 고주파일수록 높은 차수**를 적용합니다:

$f_{i,2} = \text{KAN}(f_i, \text{order}=b+k-i) \tag{8}$

M-KAN 블록의 최종 출력:

$\hat{f}_i = f_{i,1} + f_{i,2} \tag{9}$

**Step 4. 주파수 믹싱 (Frequency Mixing)**

학습된 주파수 성분들을 재결합하여 다음 CFD 블록의 입력으로 복원합니다:

$x_i = \text{IFFT}(\text{Padding}(\text{FFT}(x_{i+1}))) + f_i \tag{10}$

최종 예측:

$X_O = \text{Linear}(x_1) \tag{11}$

***

## 4. 모델 구조 요약

TimeKAN의 전체 파이프라인은 아래와 같이 요약됩니다:[^1_1]


| 단계 | 구성 요소 | 역할 |
| :-- | :-- | :-- |
| 전처리 | AvgPool + Linear Embedding | 다중 레벨 시퀀스 생성 및 차원 투영 |
| 분해 | CFD Block (FFT → ZeroPad → IFFT + 잔차) | 각 주파수 대역의 성분 추출 |
| 학습 | M-KAN (DepthwiseConv + ChebyshevKAN) | 주파수별 시간 패턴 및 의존성 학습 |
| 재결합 | Frequency Mixing (IFFT + f_i) | 주파수 성분 재통합 |
| 출력 | Linear Layer | 예측값 생성 |

**계산 복잡도:** FFT가 $O(L \log L)$, Depthwise Conv가 $O(L)$, M-KAN이 $O(L)$로 전체 복잡도는 $O(ML \log L)$ (M: 변수 수).[^1_1]

***

## 5. 성능 향상

### 5.1 예측 정확도

6개 실세계 데이터셋(ETTh1, ETTh2, ETTm1, ETTm2, Weather, Electricity)에서의 실험 결과, TimeKAN은 **22개의 1위 MAE 지표**를 기록하며 iTransformer, TimeMixer, PatchTST 등 SOTA 모델을 압도했습니다. Electricity 데이터셋에서만 iTransformer가 1위를 차지했는데, 이는 해당 데이터셋이 고차원 변수 간 의존성이 중요하기 때문입니다.[^1_1]

### 5.2 경량성 비교

Weather 데이터셋 기준 TimeKAN의 파라미터는 **20,940개**로 TimeMixer(104,430개)의 약 20%, iTransformer(4.83M)의 0.43%에 불과합니다. Electricity 데이터셋에서 PatchTST 대비 파라미터 수는 **295분의 1**, MACs는 **118분의 1** 수준입니다.[^1_1]

### 5.3 주요 에블레이션 결과 (Ablation Study)

- **Frequency Upsampling vs. 대체 업샘플링:** Linear Mapping/Interpolation/Transposed Convolution 모두 ETTh1 MSE 기준 0.001~0.034 포인트 성능 저하 발생 → FFT 기반 주파수 보존 업샘플링이 핵심[^1_1]
- **Multi-order KANs vs. 고정 차수:** 고정 저차(order 2) 또는 고정 고차(order 5) KAN 모두 Multi-order KAN보다 성능 열세 → 차수 조절이 주파수 특성 적응에 유효[^1_1]
- **Depthwise Conv vs. Self-Attention:** Depthwise Conv가 채널 간 간섭 없이 시간 의존성만을 독립적으로 포착하여 Self-Attention보다 우수[^1_1]

***

## 6. 모델 일반화 성능 향상 가능성

### 6.1 룩백 윈도우 확장 실험

TimeKAN은 **룩백 윈도우 길이가 증가할수록 일관되게 MSE가 감소**하는 특성을 보입니다. ETTm2와 Weather 데이터셋에서 $T \in \{48, 96, 192, 336, 512, 720\}$으로 변화시킨 실험에서, 다른 모델들(iTransformer, PatchTST)과 달리 TimeKAN은 monotonically decreasing MSE를 보였습니다. 이는 주파수 기반 모델의 특성상 **더 긴 시퀀스일수록 유효 주파수 성분이 풍부해지기 때문**입니다.[^1_1]

### 6.2 Electricity 데이터셋의 성능 제약과 극복

$T=96$에서 Electricity의 평균 유효 주파수 수는 **10.69개**에 불과하지만, $T=512$로 확장 시 **19.74개**로 증가합니다. 실제로 $T=512$에서 TimeKAN은 Electricity 데이터셋에서도 TimeMixer와 MOMENT를 추월하며 1위를 기록했습니다. 이는 데이터 특성에 맞게 입력 길이를 조절하면 **도메인 전반에서 일반화 성능이 향상**될 수 있음을 시사합니다.[^1_1]

### 6.3 통계적 강건성 (Statistical Robustness)

3개의 서로 다른 시드(seed)로 반복 실험한 결과, 대부분의 데이터셋에서 **99% 신뢰도**로 TimeMixer 대비 우수성이 검증되었습니다. ETTh1의 경우 `TimeKAN: 0.422±0.004` vs `TimeMixer: 0.462±0.006` (MSE 기준)으로 낮은 분산과 높은 신뢰도를 보입니다.[^1_1]

### 6.4 주파수 대역 수의 영향

3~4개의 주파수 대역 분할이 대부분의 데이터셋에서 최적 성능을 보였습니다. 2개 이하는 주파수 중첩(overlap)이 과다하고, 5개 이상은 대역당 정보량이 부족해지는 trade-off가 존재합니다. 이는 **데이터셋의 주파수 다양성에 따라 하이퍼파라미터를 적응적으로 조절할 필요성**을 보여줍니다.[^1_1]

***

## 7. 2020년 이후 관련 연구 비교 분석

| 모델 | 연도 | 핵심 방식 | 분해 방법 | 학습 구조 | 특징 |
| :-- | :-- | :-- | :-- | :-- | :-- |
| Autoformer | 2021 | 자기상관 기반 분해 | 계절-트렌드 | Transformer | 장기 의존성 포착 [^1_1] |
| FEDformer | 2022 | 주파수 향상 분해 | Fourier/Wavelet | Transformer | 주파수 도메인 처리 [^1_1] |
| DLinear | 2023 | 단순 선형 분해 | 계절-트렌드 | 단일 Linear | 극도 경량화 [^1_1] |
| PatchTST | 2023 | Patch 임베딩 | 없음 | Transformer | 채널 독립성 [^1_1] |
| TimeMixer | 2024 | 다중 스케일 혼합 | 계절-트렌드 + 스케일 | MLP | 스케일 간 상호작용 [^1_1] |
| iTransformer | 2024 | 변수 단위 토큰화 | 없음 | Inverted Transformer | 변수 간 의존성 [^1_1] |
| **TimeKAN** | **2025** | **다중 주파수 분해** | **FFT 기반 계단식** | **KAN (Chebyshev)** | **경량성 + 주파수 적응** [^1_1] |
| HaKAN | 2026 | Hahn 다항식 KAN | Patch 기반 | KAN + Residual | 글로벌·로컬 패턴 [^1_2] |
| TFKAN | 2025 | 시간-주파수 이중 도메인 | FFT + 시간 도메인 | 이중 브랜치 KAN | 도메인 이기종성 처리 [^1_3] |

**주요 추세:**

- **2021~2023:** Transformer 기반 모델이 주류였지만, 과도한 파라미터 수와 메모리 소비가 한계[^1_1]
- **2023~2024:** MLP 기반 경량 모델(DLinear, TimeMixer)이 Transformer를 능가하는 현상 등장[^1_1]
- **2024~2025:** KAN을 활용한 시계열 예측(T-KAN, MT-KAN, TKAN, TimeKAN 등)이 급증하며 MLP의 유망한 대안으로 부상[^1_4][^1_5]
- **2025~2026:** 주파수 도메인과 KAN을 결합한 TFKAN, HaKAN 등 후속 연구 활발[^1_3][^1_2]

***

## 8. 향후 연구에 미치는 영향과 고려 사항

### 연구에 미치는 영향

TimeKAN은 KAN을 단순한 MLP 대체재로 사용하는 것을 넘어서, **KAN의 차수 조절 능력을 도메인 구조(주파수)에 맞게 설계하는 원칙**을 제시했습니다. 이는 이후 TFKAN, HaKAN, TSKANMixer  등 KAN 기반 시계열 예측 연구의 설계 철학에 직접적인 영향을 미쳤습니다. 또한 비파라미터(non-parametric) Frequency Upsampling을 핵심 구성 요소로 확립함으로써, **주파수 무결성 보존 업샘플링**이 분해-학습-재결합 파이프라인에서 필수임을 실증했습니다.[^1_6][^1_2][^1_3][^1_1]

### 향후 연구 시 고려할 점

1. **채널 간 의존성 통합:** 현재 TimeKAN은 variate-independent 전략을 채택하고 있어, 고차원 변수(Electricity 등)에서 iTransformer에 비해 열세를 보입니다. 채널 간 cross-attention이나 graph-based 모듈과의 결합이 중요한 연구 방향입니다.[^1_1]
2. **적응적 주파수 대역 분할:** 현재는 주파수 대역 수 $k$가 사전 고정됩니다. 데이터 스펙트럼에 따라 **대역 수와 이동평균 윈도우를 자동으로 결정**하는 adaptive decomposition 연구가 필요합니다.[^1_1]
3. **비정상(non-stationary) 시계열 대응:** 현재 FFT 기반 분해는 정상성(stationarity)을 암묵적으로 가정합니다. 비정상 시계열에 대한 **시변(time-varying) 주파수 분해** (예: wavelet 기반 또는 EMD 방식)와의 결합이 요구됩니다.[^1_7]
4. **더 긴 룩백 윈도우의 체계적 활용:** $T=512$ 이상의 입력에서 TimeKAN이 의미 있는 이득을 얻는 것이 확인되었으나, 이에 따른 **메모리 효율화**($O(ML \log L)$ 복잡도 관리)와 훈련 전략이 추가로 연구되어야 합니다.[^1_1]
5. **KAN 기저 함수의 다양화:** ChebyshevKAN은 B-spline 대비 효율적이지만, Hahn 다항식 이나 Radial Basis Function  등 다양한 기저 함수의 시계열 적합성에 대한 체계적 비교 연구가 필요합니다.[^1_8][^1_2]
6. **Foundation Model과의 통합:** Time-FFM, MOMENT 등 시계열 파운데이션 모델과 TimeKAN의 경량 주파수 분해를 결합하는 hybrid 접근이 zero-shot 및 few-shot 일반화 성능을 크게 향상시킬 가능성이 있습니다.[^1_1]
7. **이상 탐지·분류로의 확장:** 주파수 분해 표현은 예측 외에도 KAN-AD  등이 보여주듯 이상 탐지나 분류 태스크에서도 활용 가능성이 높으며, TimeKAN의 표현 학습 구조를 이러한 태스크에 적용하는 연구도 의미 있습니다.[^1_9]
<span style="display:none">[^1_10][^1_11][^1_12][^1_13][^1_14][^1_15][^1_16][^1_17][^1_18][^1_19][^1_20][^1_21][^1_22][^1_23][^1_24][^1_25][^1_26][^1_27][^1_28][^1_29][^1_30][^1_31][^1_32][^1_33][^1_34][^1_35][^1_36][^1_37][^1_38][^1_39][^1_40][^1_41][^1_42][^1_43][^1_44]</span>

<div align="center">⁂</div>

[^1_1]: 2502.06910v2.pdf

[^1_2]: https://arxiv.org/abs/2601.18837

[^1_3]: https://arxiv.org/html/2506.12696v1

[^1_4]: https://arxiv.org/abs/2406.02496

[^1_5]: https://arxiv.org/abs/2405.07344

[^1_6]: https://arxiv.org/pdf/2502.18410.pdf

[^1_7]: https://arxiv.org/html/2510.10145v1

[^1_8]: https://arxiv.org/pdf/2601.18837.pdf

[^1_9]: https://arxiv.org/abs/2411.00278

[^1_10]: https://arxiv.org/html/2502.06910v1

[^1_11]: https://www.semanticscholar.org/paper/Kolmogorov-Arnold-networks-for-time-series-a-review-Yamak-Li/44e39edc11dc6ab7ac768f1d01dfc362a137f319

[^1_12]: https://arxiv.org/pdf/2404.19756.pdf

[^1_13]: https://arxiv.org/html/2407.12415v1

[^1_14]: https://www.arxiv.org/pdf/2410.02081.pdf

[^1_15]: https://www.arxiv.org/pdf/2510.16940.pdf

[^1_16]: https://arxiv.org/html/2601.15669v1

[^1_17]: https://www.semanticscholar.org/paper/Kolmogorov-Arnold-Networks-(KAN)-for-Time-Series-Dong-Zheng/b0992e50f0360fe60fe1436d6a748acf4c259345

[^1_18]: https://pubs.acs.org/doi/10.1021/acs.est.4c11113

[^1_19]: https://arxiv.org/abs/2502.18410

[^1_20]: https://link.springer.com/10.1007/s10586-025-05574-9

[^1_21]: https://arxiv.org/abs/2510.16940

[^1_22]: https://www.mdpi.com/2227-7390/12/19/3022

[^1_23]: https://arxiv.org/abs/2509.02967

[^1_24]: https://ieeexplore.ieee.org/document/11100692/

[^1_25]: https://www.ssrn.com/abstract=4825654

[^1_26]: http://arxiv.org/pdf/2405.08790.pdf

[^1_27]: http://arxiv.org/pdf/2502.06910.pdf

[^1_28]: http://arxiv.org/pdf/2406.17890.pdf

[^1_29]: http://arxiv.org/pdf/2406.02496.pdf

[^1_30]: http://arxiv.org/pdf/2408.11306.pdf

[^1_31]: http://arxiv.org/pdf/2411.00278.pdf

[^1_32]: https://arxiv.org/pdf/2406.02486.pdf

[^1_33]: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5220118

[^1_34]: https://www.datasciencewithmarco.com/blog/kolmogorov-arnold-networks-kans-for-time-series-forecasting

[^1_35]: https://axi.lims.ac.uk/paper/2406.02496

[^1_36]: https://www.nature.com/articles/s41598-024-52240-y

[^1_37]: https://www.semanticscholar.org/paper/Kolmogorov-Arnold-Networks-(KANs)-for-Time-Series-Vaca-Rubio-Blanco/081eb8781725e560f597b01c624fe65618c3c0f8

[^1_38]: https://ar5iv.labs.arxiv.org/html/2311.04522

[^1_39]: https://openreview.net/forum?id=LWQ4zu9SdQ

[^1_40]: https://www.sciencedirect.com/science/article/abs/pii/S0950705124006208

[^1_41]: https://icml.cc/virtual/2025/poster/45584

[^1_42]: https://arxiv.org/pdf/2410.02070v1.pdf

[^1_43]: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4825654

[^1_44]: https://5g-stardust.eu/wp-content/uploads/sites/97/2025/01/GLOBECOM_WS_KANs-1.pdf

