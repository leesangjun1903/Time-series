# A Transformer-based Framework for Multivariate Time Series Representation Learning

### 1. 핵심 주장 및 주요 기여

이 논문의 핵심 주장은 **다변량 시계열(Multivariate Time Series, MTS) 데이터의 비지도 표현 학습을 위해 처음으로 Transformer 인코더 기반 프레임워크를 제시**한다는 것입니다. 논문은 다음과 같은 주요 기여를 제시합니다:[1]

- **최초의 Transformer 기반 MTS 비지도 학습 프레임워크**: 회귀, 분류, 예측, 결측값 대체 등 여러 하위 작업에 활용 가능한 범용적 방법론 제시[1]

- **지도 학습을 초과하는 성능**: 추가 레이블 데이터 없이 비지도 사전학습(Unsupervised Pre-training)이 순수 지도 학습보다 뛰어난 성능을 제공하는 **최초의 경우** 입증[1]

- **제한된 학습 데이터에서의 우수한 성능**: 수백 개 정도의 샘플로도 기존 최고 성능 방법들을 상당한 차이로 앞지르는 **전례 없는 성과** 달성[1]

- **효율적인 계산 자원 활용**: 수십만 개 정도의 파라미터로 CPU에서도 실용적으로 학습 가능하며, GPU 활용 시 가장 빠른 기존 방법들과 유사한 속도 달성[1]

***

### 2. 해결하고자 하는 문제

**주요 문제점**:[1]

1. **레이블 부족의 심각성**: 빅데이터 시대임에도 불구하고 다변량 시계열 데이터의 레이블된 샘플이 매우 제한적이며, 전문가 주석에는 높은 시간과 비용이 소모됨

2. **심층 학습의 한계**: 컴퓨터 비전이나 NLP와 달리 시계열 분야에서는 비심층 방법(TS-CHIEF, HIVE-COTE, ROCKET)들이 여전히 최고 성능을 유지하고 있음[1]

3. **기존 비지도 학습 방법의 제한성**: 대부분의 MTS 비지도 학습이 입력 재구성 목표의 오토인코더 기반(MLP 또는 LSTM)으로 제한되어 있어 확장성과 성능이 부족함

***

### 3. 제안하는 방법 (수식 포함)

#### 3.1 기본 모델 구조

입력 시계열 $$X \in \mathbb{R}^{w \times m}$$ (길이 w, 변수 개수 m)를 특징 벡터 시퀀스로 표현합니다:[1]

$$ X = [x_1, x_2, \ldots, x_w], \quad x_t \in \mathbb{R}^m $$

각 시간 단계의 특징 벡터를 정규화한 후 모델 차원 d로 선형 사영합니다:[1]

$$ u_t = W_p x_t + b_p $$

여기서 $$W_p \in \mathbb{R}^{d \times m}$$, $$b_p \in \mathbb{R}^d$$는 학습 가능한 파라미터입니다.

**위치 인코딩**(Positional Encoding): Transformer의 순열 불변성을 극복하기 위해 학습 가능한 위치 인코딩을 추가합니다:[1]

$$ U' = U + W_{\text{pos}} $$

여기서 $$W_{\text{pos}} \in \mathbb{R}^{w \times d}$$는 학습 가능한 위치 인코딩입니다. 원본 Transformer의 결정론적 사인파 인코딩 대신 학습 가능한 인코딩을 사용하면 모든 데이터셋에서 더 나은 성능을 보입니다.[1]

**정규화 방식**: NLP에서는 레이어 정규화(LayerNorm)가 표준이지만, 시계열 데이터의 이상치 완화를 위해 배치 정규화(BatchNorm)를 사용합니다. 실험 결과 배치 정규화가 상당한 성능 이점을 제공합니다.[1]

#### 3.2 회귀 및 분류

모든 시간 단계의 최종 표현 벡터 $$z_t \in \mathbb{R}^d$$를 연결하여 단일 벡터를 생성합니다:[1]

$$ \bar{z} = [z_1; \ldots; z_w] \in \mathbb{R}^{d \cdot w} $$

이를 선형 출력 층에 입력하여 예측값을 계산합니다:[1]

$$ \hat{y} = W_o \bar{z} + b_o $$

여기서 $$W_o \in \mathbb{R}^{n \times (d \cdot w)}$$, $$b_o \in \mathbb{R}^n$$이고, n은 회귀의 경우 일반적으로 1, 분류의 경우 클래스 개수입니다.

**손실 함수**: 회귀의 경우 제곱 오차, 분류의 경우 크로스 엔트로피를 사용합니다.

#### 3.3 비지도 사전학습: 입력 노이징

비지도 사전학습의 핵심은 **입력 노이징(Input Denoising) 작업**으로, 입력의 일부를 0으로 마스킹하고 모델이 원본 값을 복원하도록 학습합니다.[1]

**마스킹 생성**: 이진 노이즈 마스크 $$M \in \mathbb{R}^{w \times m}$$를 생성합니다:[1]

$$ \tilde{X} = M \odot X $$

각 변수의 마스크 열에서 평균적으로 비율 r이 0으로 설정됩니다. 각 마스킹된 세그먼트(0의 시퀀스)의 길이는 평균 $$l_m = 3$$인 기하 분포를 따르며, 뒤이어 마스킹되지 않은 세그먼트(1의 시퀀스)의 평균 길이는 $$l_u = \frac{1-r}{r} l_m$$입니다. 실험에서 $$r = 0.15$$로 설정했습니다.[1]

**BERT의 "cloze" 마스킹과의 차이**: BERT는 전체 특징 벡터를 특수 토큰으로 대체하지만, 이 논문은 각 변수 차원에서만 값을 마스킹합니다. 이는 모델이 인접한 세그먼트와 다른 변수의 동시 값에 모두 주의를 기울이도록 유도하여 변수 간 종속성 학습을 촉진합니다.[1]

**손실 함수**: 최종 표현 벡터 $$z_t$$의 선형 계층으로부터 각 시간 단계에서 노이즈 제거된 입력 벡터를 추정합니다:[1]

$$ \hat{x}_t = W_o z_t + b_o $$

**중요한 점**: 마스킹된 위치의 예측만 손실 계산에 포함됩니다:[1]

$$ L_{\text{MSE}} = \frac{1}{|M|} \sum_{(t,i) \in M} (\hat{x}(t,i) - x(t,i))^2 $$

여기서 $$M = \{(t,i) : m_{t,i} = 0\}$$는 마스킹된 위치의 인덱스 집합입니다. 이는 데노이징 오토인코더와 다른데, 후자는 전체 입력의 재구성 손실을 사용합니다.[1]

---

### 4. 모델 구조의 주요 특징

#### 4.1 Transformer 인코더의 장점

논문은 Transformer가 시계열 데이터에 특히 적합한 이유를 명시합니다:[1]

1. **장거리 의존성 학습**: RNN의 위치 의존적 편향 없이 긴 문맥을 동시에 고려하며, RNN의 소실 기울기 문제(Vanishing Gradient Problem)에도 불구하지 않습니다.

2. **다중 주의 헤드**: 여러 표현 부분공간에서 다양한 관련성 측면을 학습합니다. 예를 들어 주파수 성분 $$1/T_1$$과 $$1/T_2$$를 갖는 신호에서 한 헤드는 인접한 시점, 다른 헤드는 $$T_1$$ 또는 $$T_2$$ 주기 전의 시점들에 주의할 수 있습니다.

3. **계층적 주의 재분배**: 각 인코더 층을 통과하면서 더 추상적인 입력 표현을 바탕으로 주의가 재분배되어, RNN의 단일 주의 분포보다 더 강력한 표현을 학습합니다.

#### 4.2 시계열 데이터 특화 설계

**시퀀스 길이 변동성 처리**: 최대 길이 w을 기준으로 짧은 샘플을 패딩하고, 패딩 마스크를 사용하여 소프트맥스 계산 전 패딩 위치에 큰 음수를 더합니다. 이는 모델이 패딩 위치를 완전히 무시하면서도 대규모 미니배치 병렬 처리를 가능하게 합니다.[1]

**배치 정규화 선택**: 시계열의 이상치 완화 장점과 NLP 대비 크기 변동이 작다는 특성을 감안하여 배치 정규화를 채택합니다. 실험에서 특정 데이터셋에 따라 배치 정규화가 레이어 정규화 대비 상당한 성능 이점을 제공함을 보입니다.[1]

---

### 5. 성능 향상 분석

#### 5.1 회귀 작업 성능

6개 다변량 시계열 회귀 데이터셋에서 평균 순위 **1.33**을 달성하여 다른 모든 방법을 압도합니다:[1]

| 비교 대상 | 평균 순위 |
|----------|---------|
| TST (제안 방법) | 1.33 |
| XGBoost | 3.5 |
| ROCKET | 5.67 |
| Inception | 5.67 |

- **RMSE 개선**: 평균적으로 모든 모델의 평균 RMSE 대비 30% 감소, XGBoost(두 번째 최고 성능)대비 약 16% 감소[1]
- **절대 개선폭**: 데이터셋별로 약 4%~36%의 향상[1]

#### 5.2 분류 작업 성능

11개 다변량 시계열 분류 데이터셋에서:[1]

- **최고 성능**: 11개 중 7개 데이터셋에서 최고 성능 달성, 평균 순위 **1.7**
- ROCKET: 3개 데이터셋에서 최고, 평균 순위 2.3
- 평균 정확도: 0.748 (ROCKET 0.725, XGBoost 0.659)[1]

**특성 분석**: ROCKET이 우수한 3개 데이셋은 모두 매우 낮은 차원(3차원)이었으며, 본 논문의 모델은 고차원 데이터셋(FaceDetection, HeartBeat, InsectWingBeat, PEMS-SF)에서 특히 우수합니다.[1]

#### 5.3 비지도 사전학습의 이점

**중요한 발견**: 추가 미레이블 데이터 없이도 동일 학습 샘플을 비지도 목표로 재사용하는 것만으로 순수 지도 학습을 초과하는 성능을 달성합니다:[1]

- **회귀**: 6개 중 3개 데이셋에서 사전학습 모델이 순수 지도 학습 모델을 능가
- **분류**: 11개 중 8개 데이셋에서 사전학습 모델이 우수하며, 때로는 상당한 차이 발생[1]

**부분 레이블 시나리오**: BeijingPM25Quality 데이셋(12.5k 샘플)에서:[1]

- 레이블 가용성이 증가해도 사전학습 모델이 일관되게 순수 지도 학습을 능가
- 레이블 10~20%만 사용해도 사전학습 여부에 따라 성능 격차 발생 (동일 레이블 비율에서도 사전학습이 우수)

***

### 6. 모델의 일반화 성능 향상 가능성

#### 6.1 현재 강점

**고차원 및 대규모 데이터에서의 우수성**:[1]

- InsectWingBeat 데이터셋: 30,000 샘플, 200 차원, 불규칙한 길이
  - 본 모델: 정확도 0.689
  - 차선: XGBoost 0.369 (47% 개선)
- PEMS-SF: 963 차원의 극고차원 데이터에서 우수한 성능

**소규모 샘플에서의 강건성**: 11개 분류 데이셋 중 8개가 수백 개 정도의 샘플을 가지고 있음에도 일관된 높은 성능 달성[1]

#### 6.2 현재 약점 및 개선 방향

**저차원 시계열의 약점**: ROCKET이 우수한 3개 데이셋(EthanolConcentration, Handwriting, UWaveGestureLibrary)은 모두 3차원입니다.[1]

**원인 분석**:[1]

- 저차원 표현 공간의 주의 메커니즘 문제
- 위치 인코딩의 간섭

**제안된 해결책**: 1D 컨볼루셔널 계층을 사용하여 저차원 입력 특징에서 더 의미 있는 표현을 추출합니다.[1]

#### 6.3 계산 복잡성과 확장성

**현재 제약**: 자기 주의의 시간 복잡성이 $$O(w^2)$$, 파라미터 복잡성이 $$O(w)$$로 시퀀스 길이에 따라 증가합니다.[1]

**확장 방법**: 다음 대체 자기 주의 스킴들이 $$O(w^2)$$ 복잡성을 크게 감소시킬 수 있습니다:[1]

- 희소 주의 패턴(Sparse Attention)
- 반복 메커니즘
- 압축된 (전역-국소) 주의

#### 6.4 전이 학습 가능성

**미개척 영역**:[1]

- **결측값 대체(Imputation)**: 비지도 사전학습 후 테스트 셋의 노이징 재구성 작업에서 MSE ≈ 0 달성 가능. 정성적 결과는 높은 성능을 보이지만 정량적 비교는 향후 작업으로 미뤄짐
- **시계열 예측(Forecasting)**: 마스킹 패턴 변경만으로 구현 가능 (예: 모든 변수의 마지막 부분 동시 마스킹). 슬라이딩 윈도우로 장시간 시계열 확장 가능

**표현 활용**: 추출된 표현 $$z_t$$는 다음에 직접 활용 가능합니다:[1]

- 시계열 유사성 평가
- 클러스터링 및 시각화
- 개별 시간 단계별 선택적 가중치 적용으로 시계열 진화 시각화

***

### 7. 한계 분석

#### 7.1 모델 수준의 한계

1. **저차원 성능**: 저차원 시계열에서 ROCKET보다 약할 수 있음
2. **장시간 시계열**: $$O(w^2)$$ 복잡성으로 매우 긴 시계열 처리 시 계산 비용 증가

#### 7.2 실험적 한계

1. **하이퍼파라미터 튜닝**: 각 데이터셋별로 별도의 하이퍼파라미터 튜닝이 필요하여 실무 적용 복잡성[1]
   - 권장 기본 하이퍼파라미터(Table 14): dim_model=128, dim_feedforward=256, num_heads=16, num_encoder_blocks=3[1]
   
2. **제한된 벤치마크**: 6개 회귀, 11개 분류 데이셋만 평가 (더 다양한 도메인 필요)

3. **미해결 작업**: 결측값 대체와 예측은 정성적 결과만 제시, 정량적 비교 미흡[1]

---

### 8. 앞으로의 연구 영향 및 고려사항

#### 8.1 학계에 미치는 영향

1. **패러다임 전환**: 시계열 분야에서 비심층 학습 방법의 우위에 도전하는 최초의 성공적인 심층 학습 접근법 제시

2. **비지도 학습의 가치 입증**: 추가 레이블 없이도 비지도 사전학습이 순수 지도 학습을 능가할 수 있음을 입증하여 NLP의 성공 경험을 시계열 분야에 적용

3. **Transformer 아키텍처의 보편성 강화**: NLP와 음악 이외의 시계열 도메인에서의 Transformer 효능 입증

#### 8.2 실무 적용 고려사항

1. **계산 효율성**: GPU 활용으로 ROCKET, XGBoost과 유사한 학습 시간 달성 가능하지만, CPU 전용 환경에서는 상당히 느림[1]

2. **하이퍼파라미터 선택**: 
   - 기본 설정(Table 14)으로 모든 데이터셋에 합리적인 성능 제공
   - 최고 성능을 위해서는 데이터셋 특성(차원, 길이, 샘플 수)에 따른 튜닝 권장[1]

3. **데이터 특성별 적용성**:
   - **고차원(100+)**: 강 추천
   - **저차원(<10)**: ROCKET 고려
   - **수백~수천 샘플**: 강력한 성능 기대

#### 8.3 향후 연구 방향

1. **희소 주의 메커니즘 통합**: 장시간 시계열 처리 능력 향상

2. **저차원 시계열 개선**: 1D 컨볼루셔널 계층 활용으로 성능 제고

3. **결측값 대체 및 예측 정량 평가**: 현재 정성적 결과의 체계적 벤치마킹

4. **도메인 특화 사전학습**: 의료, 금융, 에너지 등 특정 도메인에서의 전이 학습 잠재력 탐색

5. **다중 작업 학습 프레임워크**: 회귀, 분류, 예측을 통합하는 다중 목표 사전학습

***

## 결론

이 논문은 **처음으로 다변량 시계열의 비지도 표현 학습을 위한 실용적이고 효과적인 Transformer 프레임워크**를 제시합니다. 비지도 사전학습이 순수 지도 학습을 능가하고, 제한된 레이블 데이터 환경에서 지도 학습 방법들을 크게 앞지르는 성과는 시계열 머신러닝의 새로운 방향을 제시합니다. 고차원 데이터와 소규모 샘플 환경에서의 강건성은 실무 적용 가능성을 높입니다. 다만 저차원 시계열, 장시간 시계열 처리, 결측값 대체 및 예측 작업의 체계적 검증은 향후 연구 과제입니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/914ede76-b698-4549-8323-352f6d6631b2/2010.02803v3.pdf)
