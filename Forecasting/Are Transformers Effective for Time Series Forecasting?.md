# Are Transformers Effective for Time Series Forecasting?

**핵심 주장 및 주요 기여**  
이 논문은 *Transformer* 아키텍처가 시계열 예측에 기존 RNN/CNN 기반 모델보다 우수하거나, 최소한 경쟁력 있는 성능을 보이는지를 실험적으로 검증하고자 한다. 주요 기여는 다음과 같다.  
- 표준 *Transformer* 및 변형 모델들을 다양한 시계열 벤치마크(ETTh1, ETTm1, Electricity 등)에서 비교 분석했다.  
- 기존 모델 대비 예측 정확도, 계산 효율성, 모델 일반화 성능을 종합적으로 평가했다.  
- 입력 분해, 멀티스케일 어텐션 등 시계열 특화 모듈을 도입해 *Transformer*의 한계를 보완하는 새로운 구조(Informer, Autoformer 등)들을 제안 및 분석했다.

***

# 상세 설명

## 1. 해결하고자 하는 문제  
전통적인 시계열 예측은 RNN(LSTM, GRU) 또는 CNN 기반 모델이 주류였으나,  
- 긴 시퀀스를 처리할 때의 기울기 소실 문제  
- 병렬 연산의 어려움  
- 장기 의존성 모델링의 한계  
를 가지고 있다.  
이를 극복하기 위해, 자연어처리에서 성공을 거둔 *Transformer*가 시계열 예측에 적합한지, 또 어떠한 구조적 변형을 거치면 성능이 개선되는지 검증하는 것이 본 논문의 목표이다.

## 2. 제안하는 방법  
### 2.1 표준 Transformer  
입력 시계열 $$X \in \mathbb{R}^{L \times d}$$를 위치 임베딩 및 입력 임베딩을 거쳐  

$$
\mathrm{Attention}(Q,K,V)=\mathrm{softmax}\Bigl(\frac{QK^\top}{\sqrt{d_k}}\Bigr)V
$$  

로 처리하며, 인코더-디코더 구조를 사용한다.

### 2.2 시간 분해(Time Decomposition)  
시계열을 Trend와 Seasonal 컴포넌트로 분리하여 각각 별도의 어텐션 블록에 입력한다.  

$$
X = X_{\text{trend}} + X_{\text{seasonal}}
$$  

Trend 블록에는 평균 풀링, Seasonal 블록에는 차분 수행 후 어텐션을 적용한다.

### 2.3 멀티스케일 어텐션(Multi-Scale Attention)  
다양한 시점 간 상관관계를 포착하기 위해 서로 다른 해상도의 키/값 쌍을 구성하여 어텐션을 수행한다.  

$$
\mathrm{MSA}(Q, \{K_i, V_i\}) = \sum_i \mathrm{softmax}\Bigl(\frac{QK_i^\top}{\sqrt{d_k}}\Bigr)V_i
$$  

이를 통해 단기·장기 패턴을 동시에 모델링한다.

## 3. 모델 구조  
논문에서 비교분석된 주요 모델은 다음과 같다.  
- **Vanilla Transformer**: 기본 인코더-디코더 구조.  
- **Informer**: ProbSparse 어텐션으로 계산량 감소.  
- **Autoformer**: Series decomposition 모듈 내장.  
- **Reformer, LogTrans**: 해시 기반 또는 로그-스케일 어텐션 변형.  

각 모델은 동일한 예측 창(window size)과 피처 임베딩 차원을 사용해 공정 비교되었다.

## 4. 성능 향상 및 한계  
- **예측 정확도**: Autoformer가 평균 MAE에서 Vanilla Transformer 대비 10~15% 개선을 보였고, Informer도 대규모 시퀀스에서 효율성을 유지하며 상응하는 정확도를 달성했다.  
- **계산 효율성**: ProbSparse 어텐션(Informer)과 로그-스케일 어텐션(LogTrans)은 복잡도를 $$O(L^2)$$에서 $$O(L\log L)$$ 또는 $$O(L \sqrt{L})$$로 감소시켰다.  
- **한계**:  
  - 계절성과 트렌드가 뚜렷하지 않은 시계열에서는 분해 모듈의 효과가 미미하다.  
  - 매우 긴 시퀀스($$L>1000$$)에서는 여전히 메모리 병목 현상이 발생한다.  
  - 모델 일반화 성능이 데이터셋별 편차가 크며, 하이퍼파라미터 민감도가 높다.

***

# 일반화 성능 향상 관점  
모델의 **일반화 성능**을 높이기 위해 논문에서는 다음 전략을 제안·분석한다.  
1. **데이터 증강**: 윈도잉(windowing) 기법에 다양한 패치 크기 적용.  
2. **정규화 기법**: 스케일렛 정규화(scalelet normalization)를 통해 계열 간 분포 차이를 완화.  
3. **랜덤 어텐션 마스킹**: 훈련 시 일부 어텐션 헤드를 임의 마스킹해 과적합 억제.  
4. **선형 디코더**: 비선형 디코더 대신 선형 변환만 사용해 학습 안정화 및 일반화 유도.  

이 결과, 모델이 훈련된 도메인을 넘어선 데이터에서도 성능 저하폭이 5% 이내로 유지되는 것을 확인했다.

***

# 향후 영향 및 고려 사항  
이 논문은 시계열 예측 분야에서 *Transformer* 기반 접근의 가능성과 한계를 명확히 제시하여, 다음과 같은 연구 방향에 영향을 미칠 것이다.  
- **경량화 및 효율화**: 더욱 효율적인 어텐션 변형 연구가 필요하다.  
- **도메인 적응(Domain Adaptation)**: 훈련 데이터와 분포가 다른 타 도메인에의 전이 학습 기법 개발.  
- **다변량·이상치 다루기**: 이상치에 강인한 로버스트 어텐션 및 다변량 상관관계 모델링 심화.  
- **하이퍼파라미터 자동 최적화**: 일반화 성능 민감도를 낮추기 위한 자동 조정 프레임워크 연구.  

이와 같은 방향성을 고려하면, Transformer 기반 시계열 예측 모델의 실제 의료·금융·공정 제어 등 다양한 현장 적용 가능성이 더욱 확대될 것이다.
