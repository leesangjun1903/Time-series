# Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting

## 1. 핵심 주장과 주요 기여

**Crossformer의 핵심 주장**:[1]
- 기존 Transformer 기반 다변량 시계열(MTS) 예측 모델들은 시간적 의존성(cross-time dependency)에만 집중하고, 차원 간 의존성(cross-dimension dependency)을 충분히 활용하지 못함
- 차원 간 의존성은 MTS 예측에서 중요한 요소임 (예: 온도 예측 시 과거 온도뿐만 아니라 풍속도 도움이 됨)

**주요 기여**:[1]
1. **문제점 발견**: 기존 Transformer 기반 모델들이 cross-dimension dependency를 명시적으로 활용하지 못한다는 한계 식별
2. **혁신적 접근**: Cross-dimension dependency를 명시적으로 탐색하고 활용하는 첫 번째 Transformer 모델 개발
3. **우수한 성능**: 6개 실제 데이터셋에서 58개 설정 중 36개에서 1위, 51개에서 2위 달성[1]

## 2. 해결하고자 하는 문제와 제안 방법

### 해결 대상 문제[1]
기존 Transformer 기반 모델들은 동일한 시점의 모든 차원 데이터를 하나의 벡터로 임베딩하여 시간 단계 간 의존성만 포착하고, 차원 간 의존성은 암묵적으로만 처리함.

### 제안 방법

#### 1) Dimension-Segment-Wise (DSW) Embedding[1]
각 차원의 시계열을 $$L_{seg}$$ 길이의 세그먼트로 분할하여 임베딩:

$$x_{1:T} = \{x_i^{(s)},d | 1 \leq i \leq \frac{T}{L_{seg}}, 1 \leq d \leq D\}$$

$$h_{i,d} = Ex_i^{(s)},d + E_{i,d}^{(pos)}$$

여기서 $$E \in \mathbb{R}^{d_{model} \times L_{seg}}$$는 학습 가능한 프로젝션 행렬, $$E_{i,d}^{(pos)}$$는 위치 임베딩.[1]

#### 2) Two-Stage Attention (TSA) Layer[1]
**Cross-Time Stage**: 각 차원에서 시간 세그먼트 간 의존성 포착

$$\hat{Z}^{time}_{:,d} = \text{LayerNorm}(Z_{:,d} + \text{MSA}^{time}(Z_{:,d}, Z_{:,d}, Z_{:,d}))$$

**Cross-Dimension Stage**: 라우터 메커니즘으로 차원 간 의존성 포착

$$B_{i,:} = \text{MSA}_1^{dim}(R_{i,:}, Z_{i,:}^{time}, Z_{i,:}^{time})$$

$$Z_{i,:}^{dim} = \text{MSA}_2^{dim}(Z_{i,:}^{time}, B_{i,:}, B_{i,:})$$

복잡도를 $$O(D^2L)$$에서 $$O(DL)$$로 감소.[1]

#### 3) Hierarchical Encoder-Decoder (HED)[1]
다양한 스케일의 정보를 활용하여 최종 예측 수행. 각 레이어는 서로 다른 스케일에 대응하며, 상위 레이어는 더 거친 스케일의 의존성을 포착.

## 3. 모델 구조와 성능 향상

### 모델 구조[1]
- **인코더**: 각 레이어에서 인접한 세그먼트를 병합하여 더 거친 스케일 표현 생성
- **디코더**: 각 스케일에서 예측을 생성하고 이를 합산하여 최종 예측

### 성능 향상[1]
- **ETTh1 데이터셋**: 예측 길이 24시간에서 MSE 0.305 (기존 최고 대비 향상)
- **ETTm1 데이터셋**: 예측 길이 24시간에서 MSE 0.211 달성
- **계산 효율성**: 메모리 사용량에서 기존 방법들보다 우수한 성능

### 핵심 성능 요인
1. **DSW 임베딩**: 시간과 차원 정보를 모두 보존
2. **TSA 레이어**: 교차 시간 및 교차 차원 의존성을 효율적으로 포착
3. **계층적 구조**: 다양한 스케일의 정보 활용

## 4. 일반화 성능 향상 가능성

### 일반화 향상 메커니즘[1]
1. **명시적 차원 간 의존성 모델링**: 다양한 변수 간 상호작용을 명시적으로 학습하여 새로운 데이터에 대한 적응성 향상
2. **다중 스케일 표현**: HED를 통해 단기 및 장기 패턴을 모두 포착하여 다양한 시간 범위에 대한 강건성 확보
3. **세그먼트 기반 접근**: 지역적 패턴을 효과적으로 캡처하여 노이즈에 대한 강건성 향상

### 일반화 성능 증거[1]
- **6개 다양한 실제 데이터셋**에서 일관된 성능 향상
- **다양한 예측 길이**에서 안정적인 성능 유지
- **고차원 데이터셋**(Traffic: 862차원)에서도 효과적 작동

### 한계점[1]
1. **노이즈 문제**: Cross-Dimension Stage에서 모든 차원을 연결하여 고차원 데이터에서 노이즈 도입 가능성
2. **순서 정보 보존**: Attention 메커니즘이 순열 불변성을 가져 시계열의 순서 정보 보존에 한계
3. **소규모 데이터셋**: 시계열 데이터셋이 비전이나 텍스트보다 작고 단순하여 Transformer의 잠재력을 완전히 활용하지 못함

## 5. 향후 연구에 미치는 영향과 고려사항

### 연구 영향[1]
1. **Cross-dimension dependency 중요성 강조**: 다변량 시계열 예측에서 변수 간 관계의 중요성을 명확히 입증
2. **Transformer 아키텍처 혁신**: 시계열 특성에 맞는 새로운 attention 메커니즘 제시
3. **효율성과 성능의 균형**: 계산 복잡도를 줄이면서도 성능을 향상시키는 방법론 제시

### 향후 연구 고려사항[1]
1. **희소성 활용**: 
   - 고차원 데이터에서 각 차원이 모든 차원과 관련이 있지 않다는 희소 특성 활용
   - 희소하고 효율적인 Graph Transformer 기법 적용 검토

2. **순서 정보 강화**:
   - 상대적 위치 인코딩 기법 활용
   - 시계열의 순서 민감성을 보완하는 방법 연구

3. **대규모 데이터셋 필요성**:
   - 다양한 패턴을 가진 대규모 시계열 데이터셋 구축
   - Pre-training을 통한 성능 향상 가능성 탐구

4. **하이브리드 접근**:
   - DLinear와 같은 단순한 모델과의 결합을 통한 성능 향상
   - DSW 임베딩과 HED를 다른 모델에 적용하여 성능 개선

### 실무 적용 시 고려사항
1. **하이퍼파라미터 조정**: 세그먼트 길이($$L_{seg}$$)는 예측 길이와 데이터 특성에 따라 신중히 선택
2. **계산 자원**: 고차원 데이터 처리 시 메모리 요구사항 고려
3. **도메인 특성**: 각 도메인의 변수 간 관계 특성을 반영한 모델 커스터마이징 필요

Crossformer는 다변량 시계열 예측에서 차원 간 의존성의 중요성을 입증하고, 이를 효율적으로 모델링하는 새로운 접근법을 제시함으로써 향후 시계열 예측 연구의 새로운 방향을 제시한 중요한 연구로 평가됩니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/e294b81f-ebbb-484d-b666-2cc2b17c3293/992_crossformer_transformer_utiliz.pdf)
