<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# 1. 당신은 AI 분야의 연구자입니다. "DEPTS: Deep Expansion Learning for Periodic Time Series Forecasting"의 핵심 주장과 주요 기여를 간결하게 요약하세요.

2. 이 논문이 "해결하고자 하는 문제, 제안하는 방법(수식 포함), 모델 구조, 성능 향상 및 한계"를 중심으로 자세하게 설명하세요.
3.특히 "모델의 일반화 성능 향상 가능성"과 관련된 내용을 중점적으로 다뤄 주세요.
4.요약 후에는 "해당 논문이 앞으로의 연구에 미치는 영향과 앞으로 연구 시 고려할 점"도 자세히 기술해 주세요.
“2020년 이후 관련 최신 연구 비교 분석”
수식을 LaTeX 코드로 답변 형식을 맞춰주고, LaTeX 렌더링을 위해 마크다운에 달러 기호를 사용해 주세요.
3. 핵심 주장 및 주요 기여 (간결 요약)

- DEPTS는 “주기 상태(periodic state)”라는 숨은 변수를 도입해, 국소 시계열 신호와 잠재 주기 성분 사이의 복잡한 의존성을 층별(residual) 확장으로 학습하는 PTS(Periodic Time Series) 전용 딥러닝 프레임워크이다.[^1_1][^1_2]
- 잔차 확장 모듈(expansion module)과 매개변수화된 코사인 기반 주기 모듈(periodicity module)을 결합하여 다양한 주기를 데이터 주도적으로 추출·초기화(DCT 기반)함으로써, 기존 N-BEATS 등보다 전기 부하, 교통량, 에너지 생산 등 다양한 데이터셋에서 최대 20% 수준의 오차 감소를 달성한다.[^1_2][^1_1]

***

## 2. 문제 정의, 제안 방법(수식), 모델 구조, 성능 및 한계

### 2.1 해결하고자 하는 문제

1) 일반 시계열 AR(autoregressive) 공식
기존 딥러닝 시계열 모델은 보통 다음 Markov 가정 기반 AR 구조를 쓴다.

$$
x_{t:t+H} = F_\Theta(x_{t-L:t}) + \varepsilon_{t:t+H},
$$

여기서 $x_{t-L:t} = [x_{t-L},\dots,x_{t-1}]$, $x_{t:t+H} = [x_t,\dots,x_{t+H-1}]$, $\varepsilon$는 독립 가우시안 노이즈이다.[^1_1]

2) PTS(Periodic Time Series)에 대한 수정
주기성을 반영하면 시간 인덱스 $t$에 의해 고정되는 계절성(예: 하루, 주, 연 주기)을 조건으로 넣어야 한다. 이를 위해

$$
x_{t:t+H} = F'_\Theta(x_{t-L:t}, t) + \varepsilon_{t:t+H}
$$

와 같은 형태로 일반화한다.[^1_1]

문제는

- 복잡하고 비선형적인 주기 의존성(단순 additive/multiplicative seasonality로는 부족)
- 다중 주기(일/주/연 등)와 서로 다른 진폭·위상을 동시에 추정해야 하는 난제
를 기존 모델들이 잘 다루지 못한다는 점이다.[^1_2][^1_1]

DEPTS의 목표는

- “복잡한 주기 의존성”과
- “여러 주기의 조합”
을 동시에 모델링하는, PTS에 특화된 딥러닝 아키텍처를 제안하는 것이다.[^1_2][^1_1]

***

### 2.2 제안하는 수식적 공식과 방법

#### 2.2.1 Decoupled formulation

DEPTS는 시간 $t$의 잠재 주기 상태를 스칼라 $z_t$로 도입해 다음과 같이 디커플링한다.

$$
x_{t:t+H} = f_\theta\big(x_{t-L:t}, z_{t-L:t+H}\big) + \varepsilon_{t:t+H}, \quad 
z_t = g_\phi(t).
$$

여기서

- $f_\theta : \mathbb{R}^L \times \mathbb{R}^{L+H} \to \mathbb{R}^H$ : expansion module
- $g_\phi : \mathbb{R} \to \mathbb{R}$ : periodicity module
이다.[^1_1][^1_2]

이 식은

- $x_{t-L:t}$ : 국소 모멘텀(local momentum)
- $z_{t-L:t+H}$ : 전역 주기성(global periodicity)
이 합성되어 미래 값을 만든다는 구조를 명시적으로 분리한다.


#### 2.2.2 Expansion module $f_\theta$

각 층 $\ell = 1,\dots,N$에서 세 가지 잔차량을 유지한다.

- $z^{(\ell)}_{t-L:t+H}$ : 주기 상태의 잔차
- $x^{(\ell)}_{t-L:t}$ : 입력 시계열의 잔차
- $\hat{x}^{(\ell)}_{t:t+H}$ : 누적 예측(backcast–forecast 구조)

초기값은

$$
x^{(0)}_{t-L:t} = x_{t-L:t},\quad
z^{(0)}_{t-L:t+H} = z_{t-L:t+H},\quad
\hat{x}^{(0)}_{t:t+H} = 0.
$$

이다.[^1_1]

(1) 주기 블록 $f^p_{\theta_p^{(\ell)}}$

$$
v^{(\ell)}_{t-L:t+H} = f^p_{\theta_p^{(\ell)}}\big(z^{(\ell-1)}_{t-L:t+H}\big),
$$

이를 backcast/forecast로 분해해

$$
v^{(\ell)}_{t-L:t}, \quad v^{(\ell)}_{t:t+H}
$$

를 얻는다. 그 다음

$$
z^{(\ell)}_{t-L:t+H} = z^{(\ell-1)}_{t-L:t+H} - v^{(\ell)}_{t-L:t+H}
$$

로 갱신하여, 다음 층의 주기 블록이 “아직 설명되지 않은 주기 잔차”에 집중하도록 한다.[^1_1]

(2) 로컬 블록 $f^l_{\theta_l^{(\ell)}}$
입력에서 방금 사용한 주기 성분을 제거한

$$
\tilde{x}^{(\ell)}_{t-L:t} = x^{(\ell-1)}_{t-L:t} - v^{(\ell)}_{t-L:t}
$$

를 로컬 블록에 넣고

$$
u^{(\ell)}_{t-L:t},\; u^{(\ell)}_{t:t+H} = f^l_{\theta_l^{(\ell)}}\big(\tilde{x}^{(\ell)}_{t-L:t}\big)
$$

를 얻는다. 그 후

$$
x^{(\ell)}_{t-L:t} = x^{(\ell-1)}_{t-L:t} - v^{(\ell)}_{t-L:t} - u^{(\ell)}_{t-L:t}
$$

로 갱신해, 이 층에서 설명한 로컬/주기 패턴을 모두 제거한 잔차를 다음 층으로 넘긴다.[^1_1]

(3) 누적 예측 갱신

$$
\hat{x}^{(\ell)}_{t:t+H} = \hat{x}^{(\ell-1)}_{t:t+H} + u^{(\ell)}_{t:t+H} + v^{(\ell)}_{t:t+H}.
$$

(4) 전체 확장식
최종 예측은 $\hat{x}_{t:t+H} = \hat{x}^{(N)}_{t:t+H}$이고, 세 잔차의 총합 분해는

$$
\begin{aligned}
z_{t-L:t+H} 
&= \sum_{\ell=1}^N v^{(\ell)}_{t-L:t+H} + z^{(N)}_{t-L:t+H}, \\
x_{t-L:t} 
&= \sum_{\ell=1}^N \big(u^{(\ell)}_{t-L:t} + v^{(\ell)}_{t-L:t}\big) + x^{(N)}_{t-L:t}, \\
\hat{x}_{t:t+H}
&= \sum_{\ell=1}^N \big(u^{(\ell)}_{t:t+H} + v^{(\ell)}_{t:t+H}\big).
\end{aligned}
$$

여기서 $z^{(N)}$, $x^{(N)}$은 “예측에 필요 없는 잔차”로 해석한다.[^1_1]

이 구조 덕분에

- $\sum_\ell v^{(\ell)}_{t:t+H}$ : **순수 주기 기반 예측**
- $\sum_\ell u^{(\ell)}_{t:t+H}$ : **주기 효과 제거 후 로컬 모멘텀 기반 예측**
으로 해석 가능하다.[^1_1]

내부 블록 구조는

- 로컬 블록: N-BEATS 스타일의 4-layer FC + backcast/forecast linear basis[^1_2][^1_1]
- 주기 블록: 단일 FC + 두 개의 linear projection (backcast/forecast)
로 구성된다.[^1_1]


#### 2.2.3 Periodicity module $g_\phi$

주기 상태는 코사인 함수 합으로 매개변수화한다.

$$
g_\phi(t) = A_0 + \sum_{k=1}^K A_k \cos(2\pi F_k t + P_k),
$$

여기서 $A_k$는 진폭, $F_k$는 주파수, $P_k$는 위상, $K$는 최대 주기 개수이다.[^1_2][^1_1]

이때 $\phi = \{A_0, A_k, F_k, P_k\}_{k=1}^K$를 직접 랜덤 초기화하면 심하게 비등Convex이고 local optimum이 많아 학습이 불안정하기 때문에, 저자들은 2단계 최적화 형태의 초기화 전략을 설계한다.[^1_1]

1단계: surrogate $g^M_\phi$ 도입
마스크 $M_k \in \{0,1\}$로 일부 주기를 끄고 켜는

$$
g^M_\phi(t) = A_0 + \sum_{k=1}^K M_k A_k \cos(2\pi F_k t + P_k)
$$

를 정의한다.[^1_1]

2단계: 이상적인(그러나 비현실적으로 비싼) 최적화

$$
\begin{aligned}
\phi^* &= \arg\min_\phi L_{D_{\text{train}}} \big(g_\phi(t)\big), \\
M^* &= \arg\min_{\|M\|_0 \le J} L_{D_{\text{val}}} \big(g^M_{\phi^*}(t)\big),
\end{aligned}
$$

여기서 $J$는 선택할 최대 주기 개수, $L$은 MSE 혹은 DTW 기반 discrepancy이다.[^1_1]

실제 구현에서는 이 문제를 근사하기 위해 DCT 기반 알고리즘(Algorithm 1)을 사용한다.

- DCT(Discrete Cosine Transform)를 PTS train 구간에 적용해, 가장 큰 진폭을 갖는 상위 $K$ 개의 코사인 basis를 선택하고 그 계수로 $\tilde\phi^*$를 설정.[^1_2][^1_1]
- validation 구간에서 greedy하게 마스크 $M_k$를 선택해, validation discrepancy를 줄이는 $J$개의 주기를 선택해 $\tilde M^*$를 얻는다.[^1_1]

결과적으로

- 초기 $\phi$는 DCT로 추출된 안정적인 주기 스펙트럼으로 설정되고,
- joint training 단계에서는 $\phi$와 $\theta$를 end-to-end로 미세 조정한다(단, 선택된 주기 subset은 고정).[^1_1]

이 초기화는 일반화 향상과 local minimum 회피에 매우 중요한 요소로, ablation에서 rand init 대비 일관된 향상으로 나타난다.[^1_1]

***

### 2.3 성능 향상 결과

#### 2.3.1 Synthetic 실험

저자들은

$$
x_t = f_c(l_t, p_t)
$$

형태의 합성 데이터를 만들고,

- $l_t$: AR(3) 기반 local momentum
- $p_t \sim \mathcal{N}(z_t, \sigma_p)$, $z_t$는 다중 코사인 합
으로 구성한 뒤,

$$
f_c(l_t, p_t) \in \{l_t + p_t,\ (l_t + p_t)^2,\ (l_t + p_t)^3\}
$$

로 선형/2차/3차 의존성을 구성한다.[^1_1]

N-BEATS와 DEPTS를 비교한 결과,

- 모든 경우에서 DEPTS가 nd 에서 유의한 개선을 보이며,
- 비선형성이 높아질수록 (linear → quadratic → cubic) N-BEATS 대비 오류 감소가 7% → 11%로 증가한다.[^1_1]

이는 “복잡한 주기·비선형 조합”을 명시적으로 분리해 모델링하는 것이 일반화에 유리함을 시사한다.

#### 2.3.2 Real-world 실험

사용 데이터셋: ELECTRICITY, TRAFFIC, M4(HOURLY) + 저자들이 구축한 장기 PTS CAISO, NP.[^1_2][^1_1]

평가지표:

$$
\mathrm{nd} = \frac{\frac{1}{|\Omega|}\sum_{(i,t)\in\Omega} |x_t^i - \hat{x}_t^i|}{\frac{1}{|\Omega|}\sum_{(i,t)\in\Omega} |x_t^i|},\quad
\mathrm{nrmse} = \frac{\sqrt{\frac{1}{|\Omega|}\sum_{(i,t)\in\Omega} (x_t^i - \hat{x}_t^i)^2}}{\frac{1}{|\Omega|}\sum_{(i,t)\in\Omega} |x_t^i|}.
$$

[^1_1]

주요 결과(대표값):

- ELECTRICITY: N-BEATS nd 0.064 vs DEPTS 0.060 (split 2014-09-01), 다른 split에서는 0.171→0.139 등 최대 18.7% 감소.[^1_1]
- TRAFFIC: N-BEATS nd 0.114 vs DEPTS 0.111, 평균 약 3.5% 감소.[^1_1]
- M4(HOURLY): 0.023 → 0.021, 약 8.7% 감소.[^1_1]
- CAISO: N-BEATS nd 0.026–0.031 vs DEPTS 0.020–0.029, 평균 13.3% 감소, 특정 split(2020-10-01)에선 23.1% 감소.[^1_1]
- NP: 평균 9.9% 감소, 일부 split에서 15% 이상 개선.[^1_1]

Ablation 결과에서

- 주기 모듈 제거(NoPeriod), 랜덤 초기화(RandInit), 마스크 고정(FixPeriod), MultiVar(단순 공변량 입력) 모두 DEPTS 본 모델보다 일관되게 성능이 떨어진다.[^1_1]
- 특히 M4(HOURLY) 같이 짧은 시계열에서 DEPTS가 다른 변형보다 큰 폭의 개선을 보이며, 안정적인 주기 추출과 residual 확장이 짧은 데이터에서도 일반화를 돕는다는 근거를 제공한다.[^1_1]


#### 2.3.3 한계

논문과 후속 survey에서 드러나는 한계는 다음과 같다.[^1_3][^1_4][^1_1]

- 단변량 PTS에 초점을 맞추고 있어, 다변량 복합 주기·상관 구조(예: spatio-temporal)는 직접 다루지 않는다.
- $g_\phi(t)$를 순수 코사인 합으로 제한했기 때문에, 비정상·시간변화 주기(time-varying frequency)나 비정형 계절성에는 제한적일 수 있다.
- hyperparameter $J$ (선택할 주기 개수), $K$ (후보 수)에 민감하며, 데이터셋·split마다 tuning이 필요하다.[^1_1]
- 장기 horizon에서 Transformer 계열(예: Autoformer, TimesNet, CycleNet 등)이 도입한 sub-series/patch 기반 구조 대비 얼마나 경쟁력 있는지는 비교가 제한적이다(주요 비교는 N-BEATS, PARMA 등에 국한).[^1_4][^1_5]

***

## 3. 모델의 일반화 성능 향상 관점에서의 분석

### 3.1 구조적 일반화 메커니즘

1) Decoupled formulation에 의한 inductive bias

$$
x_{t:t+H} = f_\theta(x_{t-L:t}, z_{t-L:t+H}),\quad z_t = g_\phi(t)
$$

와 같이, “주기성을 시간 축의 함수”로 외재화하고, 로컬 모멘텀은 주기 성분 제거 후 residual로 처리한다.[^1_1]

이 방식은

- raw input에서 seasonality와 trend/short-term dynamics를 분리하는 고전 decomposition(예: STL, MSTL)과 유사한 inductive bias를 딥러닝에 통합한 것이라 볼 수 있다.[^1_6][^1_3]
- 주기적 패턴은 $g_\phi$에 농축되므로, $f_\theta$는 상대적으로 덜 복잡한 “비주기적” 패턴에 집중할 수 있어 overfitting 위험을 줄인다.

2) Triply residual expansion의 효과
세 잔차(주기 상태, 입력, 예측)에 대해 layer-wise residual을 쌓는 것은

- 각 층이 “아직 설명되지 않는 주기/로컬 패턴”에 집중하도록 하고,
- 깊어진 네트워크에서도 gradient 흐름이 안정적이게 한다.[^1_1]

Ablation에서

- DEPTS-1 (입력에서 주기 backcast 제거 없이 로컬 블록 학습)
- DEPTS-2 (주기 기반 forecast 제거)
- DEPTS-3 (주기 상태 residual 제거)
모두 본 모델보다 성능이 떨어지는 것으로 나타나, 세 residual branch를 모두 유지하는 것이 generalization에 유리함을 보인다.[^1_1]

3) DCT 기반 주기 초기화와 마스크 선택

- DCT로부터 주파수–진폭 스펙트럼을 얻고 상위 $K$개의 basis를 선택하는 것은, classic Fourier 분석에서 “주요 주기만 남기고 잡음을 제거하는” low-dimensional prior를 도입하는 것과 같다.[^1_7][^1_1]
- validation loss를 기준으로 greedy하게 $J$개만 선택하는 것은, 복잡도를 explicit하게 제어하여 데이터셋별로 가장 잘 일반화되는 주기 subset을 선택하는 “구조적 정규화”에 해당한다.[^1_1]

Ablation 결과에서

- RandInit는 종종 overfitting 혹은 나쁜 optimum에 빠져 성능이 떨어지고,
- FixPeriod(초기화 후 주기를 고정)도 joint tuning에 비해 성능이 낮으며,
- 최종 DEPTS(초기화 + joint tuning)는 두 방법보다 consistently 우수하다.[^1_1]

이는

- 좋은 초기화 + 제한된 자유도 + joint optimization
조합이 일반화에 핵심적이라는 것을 실증적으로 보여준다.

4) 데이터셋별 adaptive 주기 선택

실험에서 $J$를 변화시키면 성능이 크게 바뀌며, dataset마다 최적 $J$가 다르다.[^1_1]

- 작은 $J$: underfitting (주요 주기를 충분히 표현 못함).
- 큰 $J$: overfitting (노이즈까지 주기 패턴으로 흡수).

validation 기반 tuning을 통해 각 데이터셋에 맞는 주기 복잡도를 선택함으로써, 평균적으로 N-BEATS 대비 3.5–13.3%의 nd 감소를 달성한다.[^1_2][^1_1]

### 3.2 해석 가능성과 일반화의 관계

DEPTS는

- $\sum_\ell v^{(\ell)}_{t:t+H}$: 전역 주기성 기반 예측
- $\sum_\ell u^{(\ell)}_{t:t+H}$: 로컬 모멘텀 기반 예측
을 분리해 보여주며, 실제 사례에서
- 강한 주기성을 가진 전력 부하 시계열에서는 periodic component가 예측의 대부분을 담당하고,
- 노이즈가 크거나 구조 변화가 잦은 시계열에서는 periodic component가 coarse skeleton만 만들고 local component가 세부를 보정하는 패턴을 보인다.[^1_1]

이러한 해석 가능성은

- 어떤 데이터에 대해 모델이 무엇을 학습했는지 확인하고,
- domain knowledge(예: 특정 주기의 의미)를 이용해 주기 선택, horizon별 신뢰도 평가, covariate 설계 등을 조정할 수 있게 하여, 결과적으로 실무에서의 generalization을 강화하는 도구로 작동한다.

***

## 4. 2020년 이후 관련 최신 연구와의 비교 분석

DEPTS 이후, “주기/주파수 구성 요소를 명시적으로 모델링하는” 딥러닝 시계열 연구가 크게 늘어났다. DEPTS는 이 흐름의 초기 work로 자주 인용된다.[^1_8][^1_3][^1_4]

### 4.1 주기·주파수 명시적 모델링 계열

1) Autoformer, TimesNet, FilterNet, TimeFlex 등

- Autoformer는 Auto-Correlation 메커니즘과 FFT 기반 time-delay similarity를 이용해 sub-series level에서 주기적 의존성을 포착한다.[^1_4]
- TimesNet은 1D 시계열을 period-aware 2D representation으로 변환해 다중 스케일 주기를 계층적으로 학습한다.[^1_4]
- FilterNet은 learnable frequency filter를 통해 특정 주파수 대역을 selective하게 통과/감쇠시켜, 주파수 기반 feature selection을 수행한다.[^1_9]
- TimeFlex는 trend를 time-domain에서, seasonality를 Fourier transform으로 처리하는 모듈형 구조로 설계되어, DEPTS와 유사하게 “구성 요소별 다른 처리 방식”을 취한다.[^1_10]

DEPTS와의 차이점:

- DEPTS는 latent scalar $z_t$와 residual expansion에 집중하는 반면,
- 위 모델들은 대부분 Transformer/Conv 기반 구조에 주파수 도메인 연산(FFT, wavelet, filter)을 내장한다.
- TimesNet, FilterNet, TimeFlex 등은 다변량·장기 forecasting에서도 SOTA에 가깝지만, PTS 해석 가능성 측면에서 DEPTS처럼 “local vs periodic contribution”을 직접적으로 분해해 주지는 않는다.[^1_10][^1_4]

2) D-PAD, CycleNet, Periodicity Decoupling Framework 등

- D-PAD는 deep–shallow multi-frequency disentangling 구조를 통해, 여러 구성 성분에 흩어진 다양한 주파수 패턴을 단계적으로 추출·결합한다.[^1_11]
- CycleNet은 주기 구조를 explicit하게 모델링하여 장기 forecasting 성능을 향상시키고, 다양한 cycle length를 네트워크 내에서 학습하도록 설계한다.[^1_12][^1_5]
- Periodicity Decoupling Framework는 multi-periodic decoupling block을 통해 서로 다른 period의 패턴을 분리·재조합한다.[^1_13]

DEPTS와의 관계:

- 이들 모델은 DEPTS와 마찬가지로 “다중 주기 분해”를 핵심 아이디어로 공유하나,
- DEPTS는 코사인 기반 주기 함수와 DCT 초기화를 통해 latent periodic state를 직접 추정하는 반면,
- D-PAD/ CycleNet/ Periodicity Decoupling은 주파수 혹은 component 레벨에서의 분해–재구성 모듈을 설계해, 보다 일반적인 multi-frequency disentangling을 목표로 한다.[^1_12][^1_11][^1_13]

survey 논문들(예: Deep Time Series Models Survey, Universal TS Representation Learning)은 DEPTS를 “basis expansion 및 주기성 모듈을 결합한 초기 work”로 분류하며, 이후 주파수/주기 기반 모델들의 하나의 출발점으로 소개한다.[^1_3][^1_8][^1_4]

### 4.2 일반화 측면에서의 비교

- DEPTS는 PTS에 특화된 구조적 inductive bias 덕분에, 전형적인 전력·교통·에너지 생산처럼 강한 계절성과 상대적으로 안정적인 주기를 지닌 도메인에서 매우 강력한 성능을 보인다.[^1_1]
- 그러나 non-periodic 또는 매우 non-stationary한 시계열, 또는 복잡한 multivariate 관계를 가진 데이터에서는, Transformer 기반 general-purpose 모델(예: TFT, PatchTST, LM-기반 TS representation)이나 RNN/Graph 기반 모델이 더 robust한 경우도 보고된다.[^1_14][^1_15][^1_4]
- 최근 SparseTSF, TimeFlex 등은 cross-period downsampling, trend/seasonal 분리 등으로 “모델 파라미터 수를 줄이면서도 장기 의존성·다중 주기”를 포착하는 방향을 제안하여, DEPTS보다 더 경량화된 yet strong-generalization 모델로 발전하고 있다.[^1_16][^1_10]

요약하면,

- DEPTS는 “주기성이 강한 단변량 혹은 소수 변수 PTS”에 특히 잘 맞는 구조화된 모델,
- 이후 연구들은 이를 일반화해 다변량·장기 forecasting·representation learning으로 확장하거나, Transformer/Conv 기반과 결합해 더 일반적인 환경에서의 generalization을 추구하고 있다.[^1_8][^1_4]

***

## 5. 앞으로의 연구 영향과 향후 고려점

### 5.1 DEPTS가 끼친 영향

1) PTS 특화 아키텍처의 정당화

- DEPTS는 “PTS는 일반 시계열과 다른 inductive bias를 요구한다”는 점을 실험적으로 보여주어, 이후 주기·주파수 명시적 모델링 연구에 근거를 제공했다.[^1_3][^1_4][^1_1]

2) 주파수/주기 도메인과 딥러닝의 결합

- DCT 기반 초기화, 코사인 기반 periodic latent state 등은, 이후 FilterNet, TimeFlex, CycleNet, D-PAD와 같은 주파수-aware 네트워크 설계에 직·간접적 영향을 준다.[^1_11][^1_9][^1_12][^1_4]

3) 해석 가능한 시계열 딥러닝

- forecast를 local vs periodic으로 분해하고, 각 주기의 $(A_k, F_k, P_k)$에 의미를 부여한 것은, “설명 가능한” TS 모델 설계의 대표적인 사례로 자주 인용된다.[^1_3][^1_1]


### 5.2 향후 연구 시 고려할 점 (일반화 관점 중심)

연구 관점에서 다음 방향들이 유망하다.

1) 다변량·공간적 구조로의 확장

- 현재 DEPTS는 주로 단변량 PTS에 맞춰 설계되었다. 향후에는
    - 다변량 주기 상태 $z_t \in \mathbb{R}^d$
    - 그래프 구조(전력망, 교통망 등) 상의 shared/region-specific periodicity
를 고려한 $g_\phi$와 residual expansion의 공동 설계가 필요하다.[^1_15][^1_10][^1_1]

2) 비정상·시간변화 주기의 모델링

- $g_\phi(t)$를 고정된 주파수의 코사인 합으로 두면, 주파수가 서서히 변하거나 이벤트에 따라 바뀌는 real-world PTS에 취약할 수 있다.
- potential 방향:
    - time-varying frequency $F_k(t)$나 warping을 허용하는 adaptive basis,
    - wavelet 기반 혹은 spline 기반 주기 표현,
    - Bayesian nonparametric spectral methods와의 결합.[^1_17][^1_7][^1_3]

3) DCT 초기화의 이론적 분석과 대안

- DCT 기반 initialization이 generalization에 기여하는 구조적 이유(예: implicit regularization, bias–variance trade-off)를 이론적으로 분석하는 연구가 필요하다.[^1_18][^1_1]
- 대안으로,
    - learnable filter bank (FilterNet 스타일),
    - sparse spectral learning (SparseTSF, Deep Frequency Derivative Learning)
등을 통합해 “데이터별 최적 주파수 기반”을 더 자동으로 발견하는 방향이 있다.[^1_16][^1_18][^1_9]

4) Foundation TS model과의 결합

- 최근에는 LLM/TabPFN 기반 범용 예측기(TabPFN-v2 등)가 time series까지 아우르는 방향으로 발전 중이다.[^1_19][^1_14][^1_8]
- DEPTS의 periodic latent state $z_t$를
    - foundation model의 feature로 제공하거나,
    - foundation model이 학습한 representation에 대한 regularizer로 사용할 수 있다.
이는 도메인 특화 inductive bias와 범용 representation을 결합함으로써 보다 강한 out-of-distribution generalization을 기대할 수 있는 방향이다.

5) 모델 선택·적응과 data-centric 관점

- DEPTS 실험 결과에서, 주기 효과가 약한 split에서는 N-BEATS 대비 이득이 작거나 제한적이다.[^1_1]
- 실무에서는
    - 데이터의 forecastability/periodicity를 자동 측정하는 메트릭(예: forecastability measures, spectral entropy)을 먼저 산출하고,[^1_20][^1_8]
    - PTS로 판별되는 경우에만 DEPTS류 모델 또는 periodic-aware 모듈을 켜는 “adaptive model selection”이 필요하다.

6) 불확실성·distribution shift에 대한 일반화

- 현재 DEPTS는 주로 point forecasting과 nd/nrmse로 평가된다.
- 장기 horizon에서의 분산, distribution shift(예: 계절 구조 변화, 정책/기후 변화 등)에 대한 robust generalization을 위해
    - probabilistic forecasting (e.g., quantile, full predictive distribution),
    - continual/online learning, domain adaptation
과 결합한 주기 구조의 사용이 중요해질 것이다.[^1_21][^1_4]

***

정리하면, DEPTS는 “주기 상태를 latent variable로 분리하고 residual 확장으로 local/periodic 기여를 단계적으로 학습한다”는 점에서 PTS 전용 딥러닝의 중요한 출발점이며, 이후 주파수·주기-aware 아키텍처들이 이 아이디어를 다양한 형태로 일반화하고 있다. 향후 연구에서는 다변량·비정상 주기·foundation model과의 결합, data-centric 모델 선택 등을 통해 DEPTS 스타일의 inductive bias를 보다 넓은 시계열 환경에서 일반화하는 방향이 핵심이 될 것이다.[^1_8][^1_4][^1_3][^1_1]
<span style="display:none">[^1_22][^1_23][^1_24][^1_25][^1_26][^1_27][^1_28][^1_29][^1_30][^1_31][^1_32][^1_33][^1_34][^1_35][^1_36][^1_37][^1_38][^1_39][^1_40][^1_41][^1_42][^1_43][^1_44][^1_45][^1_46]</span>

<div align="center">⁂</div>

[^1_1]: 2203.07681v1.pdf

[^1_2]: https://arxiv.org/abs/2203.07681

[^1_3]: https://arxiv.org/html/2302.02173v6

[^1_4]: http://arxiv.org/pdf/2407.13278.pdf

[^1_5]: https://neurips.cc/virtual/2024/poster/94391

[^1_6]: https://arxiv.org/pdf/2107.13462.pdf

[^1_7]: https://arxiv.org/pdf/1905.12118.pdf

[^1_8]: https://arxiv.org/pdf/2401.03717.pdf

[^1_9]: https://www.semanticscholar.org/paper/DEPTS:-Deep-Expansion-Learning-for-Periodic-Time-Fan-Zheng/d62fdf39a501f79df7c514a023e02a4dd0bcc967

[^1_10]: https://arxiv.org/html/2506.08977v1

[^1_11]: https://arxiv.org/html/2403.17814v1

[^1_12]: https://arxiv.org/pdf/2409.18479.pdf

[^1_13]: https://openreview.net/forum?id=dp27P5HBBt

[^1_14]: https://arxiv.org/html/2405.02358v1

[^1_15]: https://peerj.com/articles/cs-3001/

[^1_16]: https://arxiv.org/pdf/2405.00946.pdf

[^1_17]: https://arxiv.org/pdf/1810.09996.pdf

[^1_18]: https://www.ijcai.org/proceedings/2024/0436.pdf

[^1_19]: https://arxiv.org/html/2501.02945v3

[^1_20]: https://arxiv.org/html/2507.13556v1

[^1_21]: https://www.sciencedirect.com/science/article/abs/pii/S0098135423004301

[^1_22]: https://arxiv.org/html/2407.13278v1

[^1_23]: https://arxiv.org/html/2405.13522v2

[^1_24]: https://arxiv.org/pdf/2308.01011.pdf

[^1_25]: https://arxiv.org/html/2407.13278v2

[^1_26]: https://arxiv.org/html/2410.15217v3

[^1_27]: https://arxiv.org/html/2505.24003v2

[^1_28]: https://arxiv.org/html/2411.06735v1

[^1_29]: https://davaoresearchjournal.ph/index.php/main/article/view/91

[^1_30]: https://www.semanticscholar.org/paper/8192e87470ab8c0b800ccdebcaf4f4dc26ec60a2

[^1_31]: https://journals.sagepub.com/doi/10.1177/03080226231172628

[^1_32]: https://www.ijsr.net/archive/v12i7/SR23704205842.pdf

[^1_33]: https://academic.oup.com/ofid/article/doi/10.1093/ofid/ofad500.1922/7447579

[^1_34]: https://ashpublications.org/blood/article/142/Supplement 1/5892/501032/The-Efficacy-and-Safety-of-the-Third-Generation

[^1_35]: https://www.aanda.org/10.1051/0004-6361/202347654

[^1_36]: https://ashpublications.org/blood/article/142/Supplement 1/7350/504825/Management-of-Bone-Marrow-Transplants-in

[^1_37]: http://doi.wiley.com/10.1002/14651858.CD009604.pub2

[^1_38]: https://onlinelibrary.wiley.com/doi/10.1002/cyto.a.24784

[^1_39]: https://arxiv.org/pdf/2203.07681.pdf

[^1_40]: https://arxiv.org/pdf/2209.01491.pdf

[^1_41]: https://liner.com/review/depts-deep-expansion-learning-for-periodic-time-series-forecasting

[^1_42]: https://www.sciencedirect.com/science/article/abs/pii/S0360835223006915

[^1_43]: http://arxiv.org/abs/2203.07681

[^1_44]: https://github.com/qingsongedu/awesome-AI-for-time-series-papers

[^1_45]: https://github.com/weifantt/DEPTS/tree/main?tab=readme-ov-file

[^1_46]: https://blog.csdn.net/szzheng/article/details/126955602

