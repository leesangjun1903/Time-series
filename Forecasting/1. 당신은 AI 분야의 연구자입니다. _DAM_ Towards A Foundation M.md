<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# 1. 당신은 AI 분야의 연구자입니다. "DAM: Towards A Foundation Model for Time Series Forecasting"의 핵심 주장과 주요 기여를 간결하게 요약하세요.

2. 이 논문이 "해결하고자 하는 문제, 제안하는 방법(수식 포함), 모델 구조, 성능 향상 및 한계"를 중심으로 자세하게 설명하세요.
3.특히 "모델의 일반화 성능 향상 가능성"과 관련된 내용을 중점적으로 다뤄 주세요.
4.요약 후에는 "해당 논문이 앞으로의 연구에 미치는 영향과 앞으로 연구 시 고려할 점"도 자세히 기술해 주세요.
“2020년 이후 관련 최신 연구 비교 분석”
수식을 LaTeX 코드로 답변 형식을 맞춰주고, LaTeX 렌더링을 위해 마크다운에 달러 기호를 사용해 주세요.
3. 핵심 주장과 주요 기여 (간결 요약)
DAM은 “단일 범용(time-series) 기초 모델”을 목표로, 불규칙 샘플링·다양한 해상도·다양한 예측 시점(가변 horizon)을 동시에 다룰 수 있는 시계열 **forecasting** 모델을 제안한다.[^1_1]
핵심 아이디어는 (1) 과거를 확률적으로 길게 샘플링하는 History Sampling Regime(HSR)과 (2) 연속 시간 $t$에 대해 값을 주는 basis function 조합 $f(t)$의 계수를 Transformer가 출력하게 하여, “하나의 단일 univariate 모델”로 여러 데이터셋에서 장기 예측·제로샷 전이·매우 장기 예측·결측 보간을 모두 잘 수행한다는 것이다.[^1_1]

***

## 1. 논문이 해결하고자 하는 문제와 핵심 방법

### 1) 문제 정의: Universal / Foundation TS Forecasting

- 여러 도메인(전력, 교통, 날씨, 클라우드 workload 등)의 시계열을 하나의 모델로 학습해, 새로운 데이터셋/도메인에도 잘 일반화하는 “universal forecasting” 문제.[^1_1]
- 기존 SoTA 모델의 한계:[^1_1]
    - 고정 길이, 규칙적 샘플 간격을 가정(regular time series).
    - 고정된 forecast horizon(예: 96, 192, 336, 720 step)에 대한 vector 출력.
    - 각 데이터셋·horizon 조합마다 별도 모델을 학습해야 하며, 훈련 범위를 벗어난 horizon, resolution, irregular sampling에 일반화가 약함.

이 논문은 “단일 모델”이 다양한 해상도·도메인·horizon에 대해 예측 가능한 foundation model을 지향하며, 특히 불규칙 샘플·결측·새로운 데이터셋에 대한 일반화 문제를 해결하려 한다.[^1_1]

### 2) 제안 방법 개요

DAM(Deep data-dependent Approximate Analytical Model)의 핵심 구성요소:[^1_1]

1. **History Sampling Regime (HSR)**
    - 과거 시점 $x$ (현재를 0이라 할 때 $x \le 0$)에 대해 긴 꼬리(long-tail) 분포를 정의하고, 그 분포로부터 과거 context 및 target 시점을 샘플링.[^1_1]
    - 분포:

$$
p_{\mathrm{hsr}}(x) \propto \frac{1}{1 + \left(\frac{x}{\sigma}\right)^2}, \quad x \in \{1,\dots,X\}
$$

와 같은 꼴의 long-tail 분포(논문에서는 $\sigma$를 HSR 폭으로 사용).[^1_1]
    - 최근 과거일수록 샘플 확률이 크지만, 먼 과거도 유의미한 확률로 샘플되어 “글로벌한 장기 패턴”을 저비용으로 관찰 가능.[^1_1]
    - 학습·추론 시 둘 다 HSR을 사용하며, context 길이와 $\sigma$는 사후에 조정(tuning) 가능.
2. **Basis function composition에 의한 연속 시간 예측**
    - 모델의 출력은 미래 시계열 값을 직접 주는 벡터가 아니라, 연속 시간 함수

$$
f(t; \theta)
$$

의 계수 $\theta$ (sine·cosine basis, affine 파라미터)이다.[^1_1]
    - 선택된 437개 주파수 $\{\omega\}$에 대해, 다음과 같은 basis 합성으로 예측:

$$
f(t; \theta) = \mathrm{IQR}\cdot \sum_{\omega} \left(\theta_{\omega,1}\sin(2\pi \omega t) + \theta_{\omega,2}\cos(2\pi \omega t)\right) + \mathrm{MED},
$$

여기서 $\mathrm{MED}$와 $\mathrm{IQR}$은 각 시계열에 대해 온라인으로 계산되는 중앙값과 사분위수 범위, $\theta_{\omega,1},\theta_{\omega,2}$는 DAM이 예측하는 basis 계수이다.[^1_1]
    - 시간 단위(초 기준)를 day, week, year 스케일에 맞춰서 주파수들을 구성해, 1분~10년까지의 period를 커버.[^1_1]
    - 이 구조 덕분에 horizon을 고정하지 않고 “임의의 시간 $t$”에 대해 예측 가능하며, 매우 장기 forecasting·과거 재구성(backcasting)·imputation에 공통 메커니즘을 사용한다.[^1_1]
3. **Transformer backbone + 두 종류의 토큰**
    - 입력:
        - HSR로 샘플된 time-value 쌍 $(t_i, v_i)$ → TV-token.
        - 초기 basis 계수(아래 설명) → B-token.
        - value 분포의 50 percentile 요약을 담는 별도 affine TV-token.[^1_1]
    - 구조(4-layer transformer):[^1_1]
        - TV-token에 대한 multi-head self-attention(MHSA).
        - B-token이 query, TV-token이 key·value인 cross-attention.
        - TV-token, affine-token, B-token 각각에 대해 별도의 FFN(feed-forward) 블록.
        - B-token 간 basis 차원에 작동하는 추가 FFN $FF_{B,\mathrm{cross}}$ (basis coefficient 공간에서의 혼합·갱신) – ablation에서 가장 중요한 컴포넌트로 확인.[^1_1]
        - Token Merging(ToME)을 이용해 TV-token 수를 단계적으로 줄여 효율 향상.[^1_1]
    - 출력: 최종 B-token을 선형 투영해 basis 계수 $\theta$와 affine 파라미터 $a,b$ 등을 얻고, 이로부터 $f(t)$를 평가해 forecast/backcast를 얻는다.[^1_1]
4. **Basis 초기화(Analytical initializer)**
    - HSR로 얻은 과거 샘플들에 대해, least-squares 기반의 선형 미분방정식 풀이(실질적으로는 Lomb-type least-squares)에 의해 basis 계수 $\theta_0$를 먼저 피팅.[^1_1]
    - 이 $\theta_0$를 B-token의 초기값으로 사용해, 학습 안정성과 수렴 속도를 높인다.[^1_1]
    - $\theta_0$만으로 과거는 잘 맞추지만 미래 extrapolation은 약하며, DAM이 학습을 통해 이를 보완해 superior forecasting을 달성한다.[^1_1]
5. **학습**
    - Huber loss를 사용해 HSR로 샘플한 past+future target에 대한 reconstruction+forecast를 동시에 학습.[^1_1]
    - target도 HSR에서 샘플하므로, 모델은 전 시간축 상에서 연속 함수 $f(t)$를 잘 맞추도록 훈련된다.
    - 손실 가중치를 시간에 따라 지수적으로 decay하여, overly distant target에 대한 영향은 줄이되 장기 패턴은 여전히 학습하도록 설계.[^1_1]

***

## 2. 모델 구조 상세

### 1) 입력·토큰화

- 입력 시계열: 단변량(univariate) $v(x)$, 시간 index $x$는 데이터의 기본 resolution 단위(예: 시간, 분).[^1_1]
- HSR로 선택된 인덱스 집합 $\{x_i\}$에서 시간 $t_i$ (일 단위로 scaling)와 값 $v_i = v(x_i)$를 추출.[^1_1]
- 이 $(t_i, v_i)$는 time embedding + value embedding을 통해 TV-token으로 변환된다.
- Basis 초기 계수 $\theta_0$는 주파수 축에 따라 embedding되어 B-token을 형성.[^1_1]
- 추가로, 값 분포의 50개의 quantile을 encoding한 TV-token이 존재하며, 전체 sequence에 대한 affine 조정을 담당한다.[^1_1]


### 2) Transformer backbone

각 레이어에서:[^1_1]

1. TV-token
    - LayerNorm → MHSA → Residual → LayerNorm → FFN(Linear–GeLU–Linear) → Residual.
2. Affine TV-token
    - TV-token과 유사한 FFN을 통해 업데이트, 전체 시계열 분포에 대한 global affine 보정을 학습.
3. B-token
    - Cross-attention: Query=B-token, Key/Value=TV-token → basis 계수가 어떤 과거 샘플에 주목해야 하는지 학습.[^1_1]
    - LayerNorm → FFN(Linear–GeLU–Linear) across token dimension.
    - 별도 FFN $FF_{B,\mathrm{cross}}$: basis coefficient 차원(437×2)에서 작동, 서로 다른 주파수 성분 간 상호작용과 혼합을 학습.[^1_1]
4. Token Merging(ToME)
    - 각 레이어에서 TV-token을 merge하여 토큰 수를 줄여, 긴 context에서도 계산량을 제어하며, 중요한 토큰만 유지.[^1_1]

마지막 레이어의 B-token은 선형 projection을 통해 $\theta$, affine 파라미터 $a,b$로 변환되고, 연속 함수 $f(t)$의 계수로 사용된다.[^1_1]

***

## 3. 성능: 장점과 한계

### 1) Long-term multivariate forecasting (within-dataset generalization)

- 학습: 25개 시계열 데이터셋(총 2280 series, 4,400만 sample)에서 단일 univariate DAM을 학습.[^1_1]
- 평가: 10개의 표준 multivariate benchmark(ETT 시리즈, ECL, Traffic, Weather, Exchange, Wind, USWeather)에 대해 4개 horizon(96, 192, 336, 720)에서 multivariate forecasting (각 채널별로 DAM을 돌려 예측) 수행.[^1_1]
- 비교 모델: PatchTST, DLinear, N-HiTS, Crossformer, Pyraformer, MICN 등 최신 SoTA.[^1_1]
- 결과:
    - 전체 80개 metric(10데이터셋 × 4 horizon × MSE/MAE) 중 DAM이 39개(1등), PatchTST가 28개로 그 다음.[^1_1]
    - 평균 MSE/MAE 측면에서도 DAM이 SoTA에 상응하거나 우수한 성능을 보이며, 특히 HSR tuning 후 일부 데이터셋에서 성능이 추가 향상.[^1_1]
    - 중요점: 경쟁 모델은 각 데이터셋·horizon 조합마다 별도 모델(총 40개 이상)을 학습한 반면, DAM은 하나의 단일 모델로 모든 조합을 처리한다는 점에서 “범용성 대비 성능”이 두드러진다.[^1_1]


### 2) Zero-shot / Fine-tuning on held-out datasets (out-of-distribution generalization)

- 8개 held-out dataset: Illness, Weekdays, UCIPower, Azure, MTemp, MWeb, MWeather, MMeters.[^1_1]
- Protocol:[^1_1]
    - Zero-shot: DAM은 추가 학습 없이, HSR 시간 scaling 정도만 validation으로 조정.
    - 비교: PatchTST, DLinear, N-HiTS의 zero-shot (10개 training dataset에서 학습된 모델을 그대로 적용), fine-tuning, from-scratch 학습.
- 결과:[^1_1]
    - Zero-shot setting에서 DAM이 16개 metric 중 14개에서 SoTA, 심지어 타 모델을 그 데이터셋에 대해 처음부터 학습한 경우보다 좋은 경우도 존재.
    - Fine-tuning(각 held-out dataset에 대해 short training) 시 DAM은 추가로 성능이 향상되며, 대부분의 경우 from-scratch baselines 대비 경쟁력 또는 우수 성능.
- 의미: “범용 모델 → 새로운 데이터셋으로의 제로샷/소량 파인튜닝”이라는 foundation model 패턴이 시계열 영역에서도 가능함을 실증.[^1_1]


### 3) Very long-term forecasting

- Weather, ETT, ECL, Traffic, USWeather, Wind 등 9개 데이터셋에서 “매우 긴 horizon”(예: Weather 5000 step ≈ 35일)을 테스트.[^1_1]
- PatchTST, DLinear를 해당 horizon에 맞게 다시 학습한 모델과 비교.[^1_1]
- 결과:
    - 평균적으로 DAM이 MSE 측면에서 더 낮고, 특히 장기적인 trend와 multi-scale 패턴을 안정적으로 유지하는 예측을 생성.[^1_1]
    - 길이 512의 regular context만 사용한 baselines는 horizon이 길어질수록 패턴이 망가지거나 평균으로 수렴하는 반면, DAM은 HSR로 먼 과거까지 활용하고 basis 구성이 horizon에 비의존적이라 장기 구조를 유지.[^1_1]


### 4) Imputation (held-out task)

- 별도 학습 없이, 초기 basis 계수 $\theta_0$만을 사용해 imputation 수행.[^1_1]
    - 즉, backbone은 사용하지 않고, 과거 데이터에 대해 analytical fit만으로 missing block을 보간.
- ETTh1/2, ETTm1/2, ECL, Traffic, Weather에서 12.5~50% 결측을 전체 column 마스킹 방식으로 주고, TimesNet과 비교.[^1_1]
- 결과: DAM(초기화만 사용)이 TimesNet보다 대부분의 데이터셋에서 낮은 MSE/MAE를 달성.[^1_1]
- 이는 basis representation 자체의 표현력이 상당히 높고, irregular sampling에서도 robust하게 작동함을 보여준다.


### 5) 한계점

- **Sharp 변화·spike가 많은 데이터셋에서의 한계**
    - Traffic, Electricity, Wind처럼 급격한 변화가 많은 신호는 smooth한 basis로 표현하기 어려워, PatchTST 등 일부 방법에 비해 성능이 약해질 수 있다.[^1_1]
- **훈련 비용**
    - 단일 모델이지만, 25개 데이터셋·100만 iteration 이상을 학습해야 하므로 학습 비용이 높다.[^1_1]
- **Univariate-only 구조**
    - 최근 채널 독립(univariate) 접근이 multivariate overfitting을 줄인다는 보고에 맞추어 univariate로 설계되어 있으며, cross-variable 상관 정보를 명시적으로 활용하지 못한다.[^1_1]
- **모델 크기·스케일링**
    - scaling law 실험에서는 파라미터 수 증가에 따라 성능이 더 좋아지는 경향을 보이나, 더 큰 모델·더 많은 데이터셋으로의 확장은 향후 과제.[^1_1]

***

## 4. 일반화 성능 향상 메커니즘(중점)

DAM의 “일반화 성능 향상 가능성”은 구조적 설계에서 나온다.[^1_1]

### 1) HSR로 인한 데이터 다양화·global context

- HSR는 매 step마다 “과거의 다양한 범위”를 랜덤으로 샘플하므로, 각 training batch는 시간 구간이 겹치더라도 서로 다른 subset을 본다.[^1_1]
- 이는 시계열의 strong temporal correlation(비-i.i.d.) 문제를 완화해, optimization 관점에서 데이터 다양성을 증가시키고 overfitting을 줄이는 효과가 있다.[^1_1]
- $\sigma$ 조절 및 context 길이 tuning을 통해, 데이터셋별로 “최근 vs 장기 패턴”의 비중을 조정함으로써, 다양한 도메인에서 최적 generalization 지점을 찾을 수 있다.[^1_1]


### 2) Horizon-agnostic basis representation

- 모델은 horizon-specific vector를 학습하는 대신, 연속 함수 $f(t)$의 basis 계수를 학습하므로, training horizon과 test horizon이 달라도 동일한 $\theta$로 예측이 가능하다.[^1_1]
- 이는 training domain에서 본 시간 스케일(예: 96~720 step)에 국한되지 않고, 훨씬 긴 horizon(수천 step)이나 다른 sampling 주기에도 자연스럽게 generalize할 수 있게 한다.


### 3) 多-데이터셋 joint training

- 25개 데이터셋에서 동시에 학습한다는 것은, 다양한 seasonality·trend·noise level·resolution에 대해 공통의 basis 표현과 attention 패턴을 학습한다는 의미이다.[^1_1]
- 이때 HSR가 irregular·variable-length 데이터를 허용하므로, 개별 데이터셋을 동일한 포맷으로 맞추기 위한 복잡한 전처리 없이도 joint training이 가능하다.[^1_1]
- 결국 DAM은 “날씨, 전력, 트래픽, 클라우드 workload 등 다양한 도메인에서 공통적으로 유의미한 frequency·timescale 패턴”을 포착하여, 새로운 도메인에도 zero-shot transfer가 잘 되는 foundation 모델 특성을 보인다.[^1_1]


### 4) Interpretable attention + basis → inductive bias

- 주파수 스펙트럼이 명시적이고, 각 basis coefficient에 대한 FFN과 cross-attention이 존재하므로, 모델이 “특정 period(예: daily, weekly, yearly)”에 강하게 응답하도록 자연스럽게 유도된다.[^1_1]
- 해석 결과(ETTh1 등)에서 실제 데이터의 일·주기 패턴과 basis coefficient의 peak가 일치하는 것을 보여, 모델이 의미 있는 물리적 주기를 학습했음을 시각적으로 확인.[^1_1]
- 이런 구조적 inductive bias는 data-scarce 도메인에서도 과적합 대신 의미 있는 장기 패턴을 추출하는 데 기여해, generalization을 돕는다.

***

## 5. 2020년 이후 관련 최신 연구와 비교

아래 표는 2020년 이후 주요 open-access 시계열 forecasting 연구와 DAM을 비교한 것이다.[^1_1]


| 모델 | 연도 | 핵심 아이디어 | 데이터 처리 가정 | Horizon 처리 | Universal / Foundation 관점 |
| :-- | :-- | :-- | :-- | :-- | :-- |
| N-BEATS / N-HiTS | 2020 / 2023 | Neural basis expansion, multi-scale block 구조 | 규칙적 샘플, 고정 길이 | 고정 horizon vector 출력 | 데이터셋별 개별 모델, multi-dataset foundation 아님.[^1_1] |
| DLinear | 2023 | Trend/seasonal 분해 + 단순 선형층 | 규칙적, 고정 길이 | 고정 horizon | 매우 간단, dataset 특화, zero-shot transfer는 제한적.[^1_1] |
| PatchTST | 2022 | univariate patch 토큰 + Transformer | 규칙적, 패치 단위 | 고정 horizon | strong SoTA지만 irregular·variable horizon·multi-dataset joint foundation 구조는 아님.[^1_1] |
| TimesNet | 2023 | 1D 시계열을 2D temporal map으로 변환, CNN 기반 multi-scale | 규칙적, 고정 길이 | 고정 horizon | Forecast + Imputation 등 general T/S task, 그러나 irregular sampling·universal multi-dataset focus는 아님.[^1_1] |
| Fedformer / Autoformer | 2021–2022 | 주파수/auto-correlation 기반 decomposition, transformer 개선 | 규칙적, 고정 길이 | 고정 horizon | multi-scale/frequency 도입, 그러나 단일 foundation model로 다수 dataset joint training + 제로샷 전이를 주목적으로 하진 않음.[^1_1] |
| ETSformer | 2022 | Exponential smoothing + attention | 규칙적 | 고정 horizon | 고전적 ETS와 transformer 결합, foundation 방향은 아님.[^1_1] |
| DAM | 2024 | HSR 기반 길게 샘플링 + 연속 시간 basis function + 단일 univariate transformer | 불규칙/결측 허용, variable-length | horizon-agnostic 연속 함수 $f(t)$ | 25개 dataset joint training, zero-shot/very-long forecasting, imputation까지 단일 모델로 수행하는 “foundation TS model” 지향.[^1_1] |

DAM의 차별점은 **불규칙 샘플·장기 컨텍스트·가변 horizon·다수 데이터셋 joint training·제로샷 전이**를 하나의 모델에서 처리한다는 점이며, 이는 기존 patch/FFT/multi-scale 기반 모델들이 제공하지 못했던 generalization 패턴이다.[^1_1]

***

## 6. 향후 연구에 미치는 영향과 고려할 점

### 1) 시계열용 Foundation Model 패러다임 정립

- DAM은 “하나의 univariate foundation 모델을 다양한 도메인·데이터셋·작업(예측·보간)에 공통 사용”하는 방향을 실증함으로써, NLP/비전 분야의 foundation model 개념을 시계열 영역으로 확장한다.[^1_1]
- 이후 연구는 더 많은 데이터셋(산업·의료·금융·IoT 등)을 포함한 대규모 프리트레이닝, 더 큰 모델(파라미터 수 증가) 및 scaling laws 분석으로 이어질 가능성이 크다.[^1_1]


### 2) 모델 구조 측면에서의 후속 연구 방향

- **Cross-variable modeling**: 현재 univariate-only 구조를 multivariate attention으로 확장하되, overfitting을 피할 regularization·sparsity 기법(예: low-rank, group attention 등)을 결합하는 연구.[^1_1]
- **Sharp event / spike 대응**: basis function의 smooth 특성을 보완하기 위해,
    - wavelet-like basis,
    - local residual CNN/MLP branch,
    - event-specific token 또는 point-process 모듈을 추가하는 hybrid 구조 연구가 유망하다.
- **Neural ODE / implicit representation과의 결합**: DAM의 basis 구성은 이미 연속 시간 모델링에 가깝기 때문에, Neural ODE·Neural CDE 또는 implicit neural representation(NIR)과 결합해 더 복잡한 dynamics를 포착하는 확장도 자연스럽다.[^1_1]


### 3) 학습·추론 전략 및 실용 배치에서의 고려사항

- **학습 비용 vs 재사용성 trade-off**
    - DAM처럼 학습 비용이 큰 foundation model은, 한번 학습 후 다양한 다운스트림(새 데이터셋·새 task)에 재사용하는 전략이 필수이며, 이를 위한 효율적 fine-tuning(LoRA, prompt-tuning, adapter 등)이 중요한 연구 주제가 될 것이다.[^1_1]
- **HSR hyperparameter adaptation**
    - 실제 시스템에서는 데이터셋별·태스크별로 “최적 HSR (context 길이, $\sigma$)”을 자동 탐색·적응시키는 메타학습·AutoML 방식이 필요하다.[^1_1]
- **Uncertainty \& decision-making**
    - HSR의 random sampling을 반복함으로써 epistemic uncertainty를 추정할 수 있는 구조이므로, probabilistic forecasting·risk-sensitive decision-making(예: 전력 수급, 트래픽 제어)과의 통합 연구가 기대된다.[^1_1]


### 4) 평가·벤치마크 측면

- DAM은 장기·매우 장기 horizon, irregular sampling, zero-shot transfer, imputation 등 다양한 setting을 동시에 다룬다.[^1_1]
- 향후 연구에서는 이러한 “multi-task, multi-dataset, heterogeneous resolution”까지 포함하는 통합 시계열 벤치마크가 필요하며, DAM의 실험 설계가 그 하나의 초기 기준점 역할을 할 수 있다.[^1_1]

요약하면, DAM은 시계열 forecasting을 “dataset-specific 모델 설계”에서 “범용 foundation model 설계”로 전환하는 강한 신호를 제공하며, 향후 연구는 (1) 더 큰 스케일, (2) multivariate·event-aware 확장, (3) 효율적 adaptation·uncertainty 추정, (4) 새로운 벤치마크 설계에 초점을 둘 필요가 있다.[^1_1]

<div align="center">⁂</div>

[^1_1]: 2407.17880v1.pdf

