# Time-LLM: Time Series Forecasting by Reprogramming Large Language Models

**주요 주장 및 기여**  
Time-LLM은 대형 언어 모델(LLM)을 **수정 없이** 시계열 예측용으로 **재프로그래밍**하는 새로운 프레임워크를 제안한다.  
-  시계열 데이터를 텍스트 프로토타입(text prototypes)으로 변환해 LLM 입력으로 사용  
-  Prompt-as-Prefix(PaP)를 도입해 도메인 지식·통계 정보·과업 지침을 자연어 프롬프트로 제공  
-  소량 학습(few-shot), 무학습(zero-shot) 환경에서도 SOTA 전문 예측 모델을 능가하는 성능 달성[1]

***

## 1. 해결 문제  
기존 시계열 예측 모델은 도메인·과업별로 **전문화**되어 있으며,  
- 데이터가 부족하거나 새로운 분야에 적용할 때 **범용성 부족**  
- 모델 설계·튜닝에 **대규모 자원** 소요  
반면 NLP·CV 분야의 LLM은 **few-shot/zero-shot 전이 학습** 능력과 **추론·패턴 인식** 능력을 갖추고 있지만,  
시계열의 연속적 실수 데이터를 그대로 입력하기 어려워 응용이 제한적이었다.[1]

***

## 2. 제안 방법  
### 2.1 입력 변환(input transformation)  
1) **RevIN**으로 채널별 정규화  
2) 길이 $$T$$인 시계열을 길이 $$L_p$$ 패치로 분할해 $$\hat{X}_P\in\mathbb{R}^{P\times d_m}$$로 선형 임베딩  
3) **텍스트 프로토타입** $$E'\in\mathbb{R}^{V'\times D}$$와 크로스-어텐션(식 (1))으로 입력 패치 재프로그래밍  

$$
     Z_k = \mathrm{Softmax}\!\bigl(Q_kK_k^\top/\sqrt{d_k}\bigr)V_k,\quad Q_k=\hat{X}W^Q_k,\;K_k=E'W^K_k,\;V_k=E'W^V_k
   $$  
   
   $$\Rightarrow$$ 패치가 언어 모델의 임베딩 공간에 매핑된다.

### 2.2 모델 구조  
- **LLM 본체**(Llama-7B) 동결  
- **Prompt-as-Prefix(PaP)**:  
  - 데이터셋 설명, 과업 지침, 입력 통계(최소·최대·중앙값·추세·상위 5개 시차) 포함  
  - LLM의 시계열 추론 능력 강화  
- **출력 투영(output projection)**: LLM 출력 패치 $$\widetilde{O}$$를 평탄화 후 선형 투영해 예측값 $$\hat{Y}\in\mathbb{R}^{N\times H}$$ 생성  

### 2.3 손실 및 학습  
- **손실**: MSE  
- **학습 가능 파라미터**: 약 6.5M(LLM 대비 0.2%)  
- **효율 최적화**: 파라미터·메모리 경량화 기법(QLoRA, 양자화) 호환 가능[1]

***

## 3. 성능 향상 및 한계  
### 3.1 성능  
- **장기 예측**(8개 벤치마크, $$\!H\!=\!\{96,192,336,720\}$$)  
  - 평균 MSE 1.4%↓ vs. PatchTST, 12%↓ vs. GPT4TS[1]
- **단기 예측**(M4, $$H\in$$) SMAPE 11.98%로 8.7%↓ vs. GPT4TS, N-HiTS와 경쟁
- **Few-shot(10%,5%)**: GPT4TS 대비 MSE 5–8%↓, PatchTST·DLinear·TimesNet 대비 12–33%↓  
- **Zero-shot**: GPT4TS 대비 평균 22%↓, LLMTime 대비 75%↓  
- **모델 크기 확장**: Llama-32층 > Llama-8층 성능 14.5%↑, GPT-2 대비 14.7%↑

### 3.2 한계  
- **프롬프트 설계 의존**: 도메인 지식·통계 제공이 성능에 큰 영향  
- **수치 정밀도**: 긴 예측 시 고정 소수점 값 처리에 추가 후처리 필요  
- **대규모 LLM 의존**: 추론 비용 및 메모리 부담  

***

## 4. 일반화 성능 향상 관점  
- **소량 학습**: 5–10% 데이터만으로도 전문 모델 대비 높은 일반화  
- **무학습 전이**: 시계열 간 데이터 분포 불일치에도 예측 성능 유지  
- **프롬프트 기반 외부 지식 통합**: 새로운 도메인 지식 추가 시 재학습 없이 적용 가능  
- **패치별 프로토타입**: 불필요한 텍스트 임베딩 축소로 잡음 억제 및 중요한 패치 특징 학습[1]

***

## 5. 미래 연구 방향  
1. **리프로그래밍 표현 최적화**: 프로토타입 개수·구성 자동화  
2. **시계열 전(前)학습**: LLM 미세조정 없이 시계열 지식 강화  
3. **다중 모달 통합**: 언어, 시계열, 이미지·신호 동시 추론  
4. **고정밀 수치 처리**: 정교한 토크나이저·후처리 기법 개발  
5. **실제 애플리케이션 적용**: 금융·의료 등 고신뢰 예측 시스템 구현  

Time-LLM은 LLM을 통한 **범용적**, **데이터 효율적**, **추론력 강화**된 시계열 예측 패러다임을 제시하며, 향후 다중 모달 대형 모델의 **새로운 가능성**을 열어갈 전망이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/ec6955f6-b323-447d-b386-7fe18dab92ca/2310.01728v2.pdf)
