<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

## 논문 핵심 요약

Aliformer는 전자상거래 판매 예측에서 미래 지식(future knowledge)을 활용하는 양방향 Transformer 모델로, 기존 시계열 예측 모델들이 과거 정보만 사용하는 한계를 극복했습니다. 이 모델은 지식 기반 self-attention 메커니즘을 통해 TMS 데이터셋에서 Informer 대비 MSE 52% 개선(0.321→0.154)을 달성했으며, Tmall의 실제 상품 선정 시스템에 배포되어 4.73% AP 향상을 기록했습니다.[^1_1]

## 해결하고자 하는 문제

### 핵심 문제점

전통적인 시계열 예측 모델들은 인과성(causality) 원칙을 따라 시점 $t$를 예측할 때 $t$ 이전의 정보만 고려합니다. 하지만 전자상거래에서는 프로모션 가격, 마케팅 활동 등 **미래 지식을 사전에 알 수 있으며**, 이러한 정보가 현재 판매량에 큰 영향을 미칩니다. 예를 들어, 소비자들은 프로모션 시작 전 구매를 미루고 할인 시작 시 폭발적으로 구매하는 패턴을 보입니다.[^1_1]

### 기존 방법의 한계

LSTNet, LSTMa, LogTrans, Informer 등 기존 딥러닝 모델들은 과거 통계만으로 미래를 예측하므로, 외부 요인(프로모션 등)으로 인한 급격한 판매 변화를 예측하지 못합니다.[^1_1]

## 제안하는 방법

### 문제 정식화

과거 통계 $S = \{s_1^{(n)}, s_2^{(n)}, ..., s_T^{(n)}\}$와 지식 정보 $K = \{k_1^{(n)}, k_2^{(n)}, ..., k_{T+L}^{(n)}\}$가 주어질 때, 미래 판매량 예측은:[^1_1]

$$
\{y_{T+1}^{(n)}, ..., y_{T+L}^{(n)}\} = f_b(x_1^{(n)}, ..., x_{T+L}^{(n)})
$$

여기서 입력은 다음과 같이 정의됩니다:[^1_1]

$$
x_t^{(n)} = \begin{cases}
\text{Emb}(\{s_t^{(n)}, k_t^{(n)}\}) & 1 \leq t \leq T \\
\text{Emb}(\{u_t^{(n)}, k_t^{(n)}\}) & T+1 \leq t \leq T+L
\end{cases}
$$

$u_t^{(n)}$는 학습 가능한 토큰으로 미래의 미지 통계를 대체합니다.[^1_1]

### AliAttention Layer

**지식 기반 Attention 메커니즘**은 vanilla attention과 knowledge-guided branch를 결합합니다:[^1_1]

$$
\text{Att}(i,j) = \frac{(x_i^h W_Q^x)(x_j^h W_K^x)^T}{\sqrt{2d}}
$$

$$
\overline{\text{Att}}(i,j) = \frac{(\bar{x}_i W_{\bar{Q}}^x)(\bar{x}_j W_{\bar{K}}^x)^T}{\sqrt{2d}}
$$

$$
\text{Att}^*(i,j) = \text{Att}(i,j) + \overline{\text{Att}}(i,j)
$$

$$
x_i^{h+1} = \sum_j \text{Softmax}(\text{Att}^*(i,j))(x_i^h W_V^x)W
$$

여기서 $x^h$는 통합 정보, $\bar{x}$는 순수 지식 정보입니다. 지식 기반 branch $\overline{\text{Att}}$는 노이즈(마스킹된 토큰)의 영향을 최소화하고 최종 attention map을 보정합니다.[^1_1]

### Future-Emphasized Training Strategy

모델이 미래 지식 활용을 강조하도록, 훈련 시 미래 구간뿐 아니라 **중간 구간에도 span masking**을 적용합니다. 이는 확률 $p_1$로 기본 전략, $p_2$로 span masking을 적용하여 모델이 과거 의존도를 줄이고 미래 정보 활용을 높입니다.[^1_1]

## 모델 구조

Aliformer는 **12개의 stacked AliAttention layers**로 구성되며, 각 레이어는 12-head attention을 사용합니다. 각 AliAttention layer는:[^1_1]

1. 통합 입력 $x_t$ (통계 + 지식)
2. 원본 지식 $\bar{x}_t$ (각 레이어에 명시적 제공)

두 입력을 받아 vanilla attention과 knowledge-guided attention을 계산한 후 결합합니다. 최종 레이어의 출력은 fully connected layer를 통해 미래 15일간의 판매량을 예측합니다.[^1_1]

## 성능 향상

### 정량적 결과

**TMS 데이터셋**에서 Aliformer는 다음 성능을 달성했습니다:[^1_1]


| 모델 | MSE | MAE |
| :-- | :-- | :-- |
| Aliformer | **0.154** | **0.229** |
| Informer | 0.321 | 0.353 |
| LogTrans | 0.327 | 0.368 |
| LSTMa | 0.313 | 0.354 |
| LSTNet | 0.283 | 0.336 |

Informer 대비 **52% MSE 개선**, LogTrans 대비 **53% MSE 개선**을 달성했습니다.[^1_1]

**공개 벤치마크**에서도 일관된 우수성을 보였습니다:[^1_1]

- ETTh (336 horizon): MSE 1.604 vs Informer 2.933
- ETTm (672 horizon): MSE 0.581 vs Informer 1.134
- ECL (720 horizon): MSE 0.396 vs Informer 0.424


### 실제 배포 성과

2021년 5월부터 Tmall Industry Tablework에 배포되어, 10억 개 상품 중 상위 100만 개를 선정하는 데 사용됩니다. 2021년 618 쇼핑 페스티벌에서 Aliformer 선정 상품이 **전체 GMV의 74.96%를 커버**하여, Informer(70.23%) 대비 **4.73%p 절대 향상**을 달성했습니다.[^1_1]

### Ablation Study

미래 지식 제거 시 MSE가 0.203으로 증가(32% 악화), AliAttention 제거 시 0.163으로 증가하여 두 구성요소 모두 중요함을 입증했습니다.[^1_1]

## 모델의 일반화 성능

### 다양한 데이터셋에서의 강건성

Aliformer는 TMS 외에도 4개 공개 데이터셋(ETTh, ETTm, ECL, Kaggle-M5)에서 **일관되게 SOTA 성능**을 달성했습니다. 특히 다음 측면에서 우수한 일반화를 보입니다:[^1_1]

1. **다양한 예측 구간**: 48, 96, 168, 336, 720 타임스텝 등 모든 길이에서 최고 성능[^1_1]
2. **다변량 시계열**: 7차원(ETT), 321차원(ECL), 5차원(TMS) 등 다양한 변수 개수에서 효과적[^1_1]
3. **도메인 전이**: 전력 소비(ETT), 전기 부하(ECL), 판매(TMS, Kaggle-M5) 등 서로 다른 도메인에서 적용 가능[^1_1]

### 미래 지식의 역할

흥미롭게도, 위치 및 시간 정보만 지식으로 사용하는 공개 데이터셋에서도 성능 향상이 있었으나, 풍부한 미래 지식(가격, 프로모션 등)을 포함한 TMS에서 **훨씬 큰 성능 향상**을 보였습니다. 이는 도메인별 미래 지식의 품질이 일반화에 중요함을 시사합니다.[^1_1]

### 채널 독립성과 다변량 상관관계

Attention 메커니즘 분석 결과, 하위 레이어에서는 knowledge-guided attention이 더 큰 역할을 하고, 상위 레이어로 갈수록 vanilla attention이 충분한 정보를 포착함을 보였습니다. 이는 모델이 계층적으로 지식을 통합하며 일반화 능력을 구축함을 나타냅니다.[^1_1]

## 한계점

### 명시적 한계

1. **미래 지식 의존성**: 모델의 우수한 성능은 미래 지식 가용성에 크게 의존하며, 이러한 정보가 없는 도메인에서는 효과가 제한적입니다[^1_1]
2. **계산 비용**: 12개의 AliAttention layers는 각각 통합 정보와 지식 정보를 처리하므로, vanilla Transformer보다 계산 복잡도가 높습니다[^1_1]
3. **하이퍼파라미터 민감도**: span masking 확률 $p_2$가 50-90% 범위에서는 안정적이나, 극단값에서는 성능 저하가 발생합니다[^1_1]
4. **데이터셋 공개 보류**: TMS 데이터셋은 승인 후 공개 예정으로, 재현성이 제한적입니다[^1_1]

### 암묵적 한계

- 장기 의존성(720+ 타임스텝)에서 다른 데이터셋 대비 성능 격차가 줄어들어, 극장기 예측에서는 상대적 우위가 감소할 가능성이 있습니다[^1_1]
- 가격 변화 시뮬레이션 결과, 할인이 판매를 증가시키지만 프로모션 전 억제 효과도 예측하여, 복잡한 소비자 행동 패턴 모델링의 어려움을 시사합니다[^1_1]


## 연구의 영향과 향후 고려사항

### 학술적 영향

**1. 패러다임 전환**: Aliformer는 인과성 원칙의 엄격한 적용을 재고하게 하며, 미래 정보 활용이 가능한 시나리오에서 양방향 모델링의 유효성을 입증했습니다.[^1_1]

**2. 지식 통합 방법론**: knowledge-guided attention은 외부 지식을 attention 메커니즘에 직접 통합하는 새로운 접근법을 제시하며, multimodal learning과 시계열 예측의 교차점을 개척했습니다.[^1_1]

**3. 벤치마크 기여**: TMS 데이터셋은 대규모 전자상거래 판매 예측의 첫 공개 벤치마크로, 실무 지향 연구를 촉진할 것입니다.[^1_1]

### 산업적 영향

**실시간 배포 검증**: Alibaba의 대규모 시스템에 배포되어 실제 비즈니스 가치를 입증했으며, 전자상거래 플랫폼의 상품 선정 최적화에 직접 적용 가능함을 보였습니다.[^1_1]

### 향후 연구 시 고려사항

**1. 불확실한 미래 지식 처리**

- 현재 모델은 미래 지식이 확실하다고 가정하지만, 실제로는 계획된 프로모션이 취소되거나 가격이 변경될 수 있습니다
- 불확실성을 명시적으로 모델링하는 확률적 접근(Bayesian framework, Monte Carlo dropout)이 필요합니다[^1_2]

**2. 효율성 개선**

- 각 레이어에 원본 지식을 제공하는 현재 구조는 메모리 집약적입니다
- Parameter-efficient Transformer 기법(LoRA, adapter modules)을 적용하여 효율성을 높일 수 있습니다[^1_3]

**3. 장기 예측 강화**

- 720+ 타임스텝에서 성능 격차 감소 문제 해결
- Multi-resolution modeling 또는 decomposition 기법을 통합하여 장기 패턴 포착 능력 향상[^1_4][^1_5]

**4. 도메인 적응 및 전이 학습**

- 미래 지식이 제한적인 도메인에서도 적용 가능하도록 domain adaptation 연구 필요
- Pre-training과 fine-tuning 전략을 통한 universal time series model 구축[^1_6]

**5. 해석 가능성 향상**

- Attention 가중치 분석을 넘어 지식이 예측에 미치는 인과적 영향 정량화
- SHAP, LIME 등 설명 가능 AI 기법 통합


## 2020년 이후 관련 최신 연구 비교

### Transformer 아키텍처 혁신

**iTransformer (2023)**: 시계열 예측에서 Transformer의 차원을 반전시켜, 시간 포인트를 variate token으로 임베딩하고 multivariate 상관관계를 포착합니다. Aliformer와 달리 미래 지식을 사용하지 않지만, 변수 간 관계 모델링에 집중하여 **일반화 능력**을 크게 향상시켰습니다.[^1_7]

**PatchTST (2023)**: 시계열을 패치로 분할하여 토큰화하고 channel-independent modeling을 적용합니다. 이 접근법은 Aliformer의 variate-level attention과 상보적이며, **계산 효율성**을 크게 개선했습니다.[^1_8][^1_9]

**Timer-XL (2024)**: 장기 컨텍스트 정보를 효율적으로 관리하는 통합 시계열 예측 프레임워크로, 다차원 next token prediction을 일반화합니다. Aliformer보다 **더 긴 시퀀스**(수천 타임스텝)를 처리할 수 있습니다.[^1_10]

### 지식 기반 모델링

**ExoTST (2024)**: 과거 내생 변수와 현재 외생 변수를 효과적으로 통합하는 cross-temporal modality fusion module을 제안합니다. Aliformer의 미래 지식 개념을 확장하여 **현재 외생 정보**와 과거 컨텍스트의 결합에 집중합니다.[^1_11]

**TimeMKG (2025)**: LLM을 사용하여 변수 의미론을 해석하고 Multivariate Knowledge Graphs를 구축하는 multimodal causal reasoning 프레임워크입니다. Aliformer가 도메인 특화 지식(가격, 프로모션)을 사용한다면, TimeMKG는 **변수 이름과 설명에서 일반 지식을 추출**하여 더 넓은 적용 가능성을 제공합니다.[^1_12][^1_13]

### Pre-trained Foundation Models

**TEMPO (2023)**: Prompt-based generative pre-trained Transformer로 시계열 표현 학습에 특화되었습니다. Zero-shot 예측 능력을 통해 Aliformer의 도메인 특화 접근과 대조적입니다.[^1_14]

**Moirai (2024)**: LOTSA(Large-scale Open Time Series Archive) 27B+ 관측치로 사전 훈련된 universal forecasting model로, **zero-shot forecasting**에서 full-shot 모델과 경쟁하거나 우수한 성능을 보입니다. Aliformer가 특정 도메인(전자상거래)에 최적화된 반면, Moirai는 **범용성**을 추구합니다.[^1_6]

**LLaTA (2024)**: LLM과 시계열의 정렬을 통해 cross-modal knowledge distillation을 수행하며, input-agnostic static knowledge와 input-dependent dynamic knowledge를 모두 활용합니다. Aliformer의 지식 통합보다 **더 일반적인 지식 전이** 메커니즘을 제공합니다.[^1_15]

### 효율성 및 단순화

**PSformer (2025)**: Parameter sharing과 Spatial-Temporal Segment Attention을 결합하여 parameter-efficient한 Transformer를 구현합니다. Aliformer가 12-layer architecture로 높은 정확도를 추구한다면, PSformer는 **효율성과 성능의 균형**에 집중합니다.[^1_3]

**Future-Guided Learning (2024)**: Teacher 모델(미래 지향)이 student 모델(과거 지향)을 가이드하는 predictive coding 기반 접근법입니다. Aliformer와 유사하게 미래 정보를 활용하지만, **knowledge distillation 프레임워크**를 통해 불확실성을 명시적으로 처리합니다.[^1_2]

### 판매 예측 특화 연구

**SDPANet (2022)**: Spatiotemporal Dynamic Pattern Acquisition을 통해 multivariate 판매 시계열의 변화하는 상관 패턴을 감지합니다. Homovariance Uncertainty 문제를 해결하여 Aliformer와 유사한 도메인에서 **동적 상관관계 모델링**을 강조합니다.[^1_16]

**Deep Learning Ensemble (2024-2026)**: LSTM, CNN, MLP를 결합한 앙상블 모델이 기술 회사의 판매 예측에서 H=1 horizon에 대해 sMAPE 12.3%를 달성했습니다. Aliformer의 단일 모델 접근과 대조적으로 **앙상블의 강건성**을 활용합니다.[^1_17]

### 핵심 비교 요약

| 측면 | Aliformer (2021) | 최신 연구 방향 (2023-2025) |
| :-- | :-- | :-- |
| **지식 활용** | 도메인 특화 미래 지식 (가격, 프로모션) | 일반적 변수 의미론[^1_12], 외생 변수[^1_11] |
| **일반화** | 전자상거래 최적화, 제한적 전이 | Universal models[^1_6], zero-shot[^1_14] |
| **효율성** | 12-layer dual-branch, 높은 비용 | Parameter-efficient[^1_3], patch-based[^1_8] |
| **장기 예측** | 720 타임스텝까지 | 수천 타임스텝 지원[^1_10] |
| **불확실성** | 명시적 처리 없음 | Predictive coding[^1_2], 확률적 접근 |

Aliformer는 미래 지식 활용의 선구적 연구로 평가되지만, 이후 연구들은 **더 넓은 적용 범위, 더 높은 효율성, 더 강력한 일반화**를 추구하고 있습니다.[^1_4][^1_14][^1_7][^1_15][^1_12][^1_6]
<span style="display:none">[^1_18][^1_19][^1_20][^1_21][^1_22][^1_23][^1_24][^1_25][^1_26][^1_27][^1_28][^1_29][^1_30][^1_31][^1_32][^1_33][^1_34][^1_35][^1_36][^1_37][^1_38][^1_39]</span>

<div align="center">⁂</div>

[^1_1]: 2109.08381v2.pdf

[^1_2]: https://arxiv.org/html/2410.15217v2

[^1_3]: https://arxiv.org/html/2411.01419v1

[^1_4]: https://arxiv.org/abs/2311.04147

[^1_5]: https://arxiv.org/pdf/2202.01381.pdf

[^1_6]: https://arxiv.org/abs/2402.02592

[^1_7]: https://arxiv.org/abs/2310.06625

[^1_8]: http://arxiv.org/pdf/2211.14730v2.pdf

[^1_9]: https://pmc.ncbi.nlm.nih.gov/articles/PMC12453695/

[^1_10]: http://arxiv.org/pdf/2410.04803.pdf

[^1_11]: http://arxiv.org/pdf/2410.12184.pdf

[^1_12]: https://www.arxiv.org/abs/2508.09630?context=cs

[^1_13]: https://www.arxiv.org/abs/2508.09630

[^1_14]: https://arxiv.org/abs/2310.04948

[^1_15]: https://arxiv.org/html/2403.07300v1

[^1_16]: https://www.sciencedirect.com/science/article/abs/pii/S0306457322001029

[^1_17]: https://www.revistaespirales.com/index.php/es/article/view/901

[^1_18]: https://arxiv.org/html/2508.16641v1

[^1_19]: https://arxiv.org/abs/2305.12095

[^1_20]: https://arxiv.org/html/2507.02907v1

[^1_21]: https://arxiv.org/abs/2410.13773

[^1_22]: https://arxiv.org/abs/2204.07786

[^1_23]: https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0013395500003890

[^1_24]: https://www.jcdr.net/article_fulltext.asp?issn=0973-709x\&year=2024\&month=October\&volume=18\&issue=10\&page=LC01-LC05\&id=20182

[^1_25]: https://www.mdpi.com/2227-7390/12/23/3666

[^1_26]: https://www.semanticscholar.org/paper/41742e4ae0f3c4afcf5a90eefbd685d63134d911

[^1_27]: https://jurnal.itscience.org/index.php/dsi/article/view/3263

[^1_28]: https://jurnal.kdi.or.id/index.php/ef/article/view/1110

[^1_29]: https://ijebmr.com/uploads/pdf/archivepdf/2024/IJEBMR_1455.pdf

[^1_30]: https://link.springer.com/10.1007/s13042-024-02417-8

[^1_31]: https://ojs3.unpatti.ac.id/index.php/barekeng/article/view/9090

[^1_32]: https://ieeexplore.ieee.org/document/10394693/

[^1_33]: https://arxiv.org/pdf/2502.13721.pdf

[^1_34]: http://arxiv.org/pdf/2410.23992.pdf

[^1_35]: http://arxiv.org/pdf/2408.09723.pdf

[^1_36]: https://arxiv.org/abs/2405.16877

[^1_37]: https://www.sciencedirect.com/science/article/pii/S2665963824001040

[^1_38]: https://github.com/thuml/iTransformer

[^1_39]: https://www.reddit.com/r/MachineLearning/comments/vruyyi/p_using_transformers_for_timeseries_forecasting/

