# Large Language Models Are Zero-Shot Time Series Forecasters

## 1. 핵심 주장과 주요 기여

본 논문의 **핵심 주장**은 대규모 언어 모델(LLM)이 시계열 데이터를 수치 문자열로 인코딩하여 다음 토큰 예측 문제로 변환함으로써, 별도의 파인튜닝 없이도 제로샷(zero-shot) 방식으로 효과적인 시계열 예측을 수행할 수 있다는 것입니다.[1]

**주요 기여**는 다음과 같습니다:

- **LLMTIME 방법론 제안**: 시계열을 텍스트로 변환하여 GPT-3, LLaMA-2 등 사전 훈련된 LLM을 시계열 예측에 활용하는 간단하면서도 효과적인 방법[1]
- **효과적인 토크나이제이션 기법**: 수치 데이터를 개별 숫자로 분리하고 공백으로 구분하여 토크나이제이션 성능을 향상시키는 기법 개발[1]
- **연속 확률 밀도 변환**: 이산 토큰 분포를 연속값에 대한 유연한 확률 밀도로 변환하는 방법론[1]
- **광범위한 실험적 검증**: Darts, Monash, Informer 등 다양한 벤치마크에서 전문 시계열 모델과 비교하여 경쟁력 있는 성능 입증[1]

## 2. 해결하고자 하는 문제와 제안 방법

### 해결하고자 하는 문제

**시계열 예측의 고유한 도전과제**:
- 서로 다른 소스에서 오는 데이터들의 일관성 없는 입력 스케일과 샘플링 비율[1]
- 결측값을 포함한 불규칙한 데이터 구조[1]
- 정확한 점 예측이 거의 불가능하여 불확실성 추정이 특히 중요함[1]
- 시계열 모델링에서는 대규모 사전 훈련이 일반적이지 않으며, 전문 데이터셋 부족[1]

### 제안 방법: LLMTIME

**1. 토크나이제이션**:
시계열 데이터를 다음과 같이 전처리합니다:
```
0.123, 1.23, 12.3, 123.0 → "1 2 , 1 2 3 , 1 2 3 0 , 1 2 3 0 0"
```

데이터 정규화 및 절단: 0.123은 전체 시계열 데이터의 통계적 특성(예: 최대값)에 따라 정규화되고, 이 과정에서 소수점 이하의 특정 자릿수가 절단될 수 있습니다.  
만약 데이터 정규화 후 소수점 이하 두 번째 자리까지만 유효하다면, 0.123은 0.12가 되고, 이 숫자의 중요 부분인 12만 토큰화될 수 있습니다.

각 숫자를 개별 토큰으로 분리하여 언어 모델이 패턴을 더 쉽게 학습할 수 있도록 합니다.[1]

**2. 스케일링**:
α-백분위수 기반 스케일링을 적용하여 토큰 효율성을 높입니다:

$$ x_t \rightarrow \frac{x_t - b}{a} $$

여기서 $$b = \min_t x_t - \beta(\max_t x_t - \min_t x_t)$$이고, $$a$$는 shifted series의 α-백분위수입니다.[1]

**3. 연속 우도 변환**:
$$n$$자리 정밀도에서 각 숫자 시퀀스는 $$B^n$$개의 가능한 빈(bin) 중 하나에 대응되며, 각 빈의 폭은 $$B^{-n}$$입니다. 연속 로그 우도는 다음과 같이 계산됩니다:

$$ \log p(x) = \log p_k + n \log B $$

여기서 $$p_k$$는 해당 빈의 확률입니다.[1]

**4. 샘플링과 예측**:
LLM에서 다수의 샘플(예: 20개)을 생성하고, 각 시간 단계에서 통계량(중앙값, 분위수 등)을 계산하여 점 추정치와 확률적 예측을 구성합니다.[1]

## 3. 모델 구조와 성능

### 모델 구조
LLMTIME은 별도의 새로운 아키텍처가 아니라 **기존 사전 훈련된 LLM을 그대로 활용**하는 방법론입니다. GPT-3, LLaMA-2 70B 등 다양한 기본 모델에 적용 가능합니다.[1]

### 성능 향상
**결정론적 성능**: 
- Darts, Monash, Informer 벤치마크에서 MAE 기준으로 최고 또는 두 번째 성능 달성[1]
- 전문 시계열 모델들(ARIMA, N-BEATS, TCN 등)과 비교하여 경쟁력 있는 성능[1]

**확률적 성능**:
- 음의 로그 우도(NLL)와 CRPS 메트릭에서 기존 방법들을 크게 상회[1]
- 다중모달 분포와 불확실성 정량화에서 특히 우수한 성능[1]

**스케일링 관계**:
- 기본 모델의 추론 성능(MMLU 정확도)이 향상될수록 시계열 예측 성능도 함께 향상[1]
- 모델 크기가 증가할수록 성능이 개선되는 경향[1]

## 4. 일반화 성능 향상 가능성

### 제로샷 일반화의 핵심 메커니즘

**1. 단순성 편향 (Simplicity Bias)**:
LLM은 복잡한 생성 규칙보다 단순한 규칙을 선호하는 오컴의 면도날 원리를 따릅니다. 심볼릭 회귀 실험에서 GPT-3이 훈련 손실과 복잡성의 균형을 맞추는 해법에 높은 우도를 부여함을 확인했습니다.[1]

**2. 시계열 패턴과 LLM 능력의 정렬**:
- **반복 편향과 주기성**: LLM의 반복 시퀀스 선호는 계절성 트렌드 식별 능력과 직접 연결됩니다[1]
- **산술 및 트렌드 성분**: LLM의 덧셈/곱셈 능력이 선형/지수 트렌드 외삽에 활용됩니다[1]

**3. 다중모달 분포 표현**:
언어 모델은 가우시안 혼합 모델보다 효과적으로 비대칭적, 다중모달적, 중꼬리 분포를 처리할 수 있어 시계열의 복잡한 불확실성 구조를 자연스럽게 모델링합니다.[1]

### 데이터 효율성
LLMTIME은 **훨씬 적은 데이터로도 효과적인 성능**을 보입니다. 다른 방법들이 훈련 데이터가 줄어들면 성능이 급격히 저하되는 반면, LLMTIME은 소량의 예제만으로도 높은 우도를 할당할 수 있습니다.[1]

### 결측값 처리와 텍스트 통합
- **결측값을 'NaN' 텍스트로 처리**하여 별도의 보간 없이도 효과적으로 대응[1]
- **텍스트 부가 정보 활용** 및 **예측 설명 기능** 제공 가능[1]

## 5. 한계점

**1. 정렬 모델의 성능 저하**:
GPT-4나 LLaMA-2 Chat 모델 같이 RLHF로 정렬된 모델들은 기본 모델보다 예측 성능이 떨어지며, 특히 불확실성 보정에서 문제를 보입니다.[1]

**2. 컨텍스트 윈도우 제한**:
긴 시계열이나 다변량 문제에서는 제한된 컨텍스트 윈도우가 도전과제가 됩니다.[1]

**3. 산술 및 조합 연산의 한계**:
복잡한 패턴의 조합이나 정확한 산술 연산이 필요한 경우 성능이 제한될 수 있습니다.[1]

## 6. 향후 연구에 미치는 영향과 고려사항

### 연구에 미치는 영향

**1. 파운데이션 모델의 시계열 확장**:
시계열 예측을 언어 생성으로 프레이밍하여 단일 대형 모델 내에서 다양한 작업과 모달리티 간 이해를 공유할 수 있는 가능성을 제시합니다.[1]

**2. 제로샷 학습의 새로운 패러다임**:
도메인 전문성이나 대량의 하위 훈련 데이터 없이도 광범위하게 효과적인 성능을 달성할 수 있는 방법론을 제시합니다.[1]

**3. 크로스 모달 학습의 가능성**:
텍스트 사전 훈련이 수치 시계열 이해에도 전이될 수 있음을 보여주어, 크로스 모달 학습 연구에 새로운 방향을 제시합니다.[1]

### 향후 연구 시 고려사항

**1. 확장된 컨텍스트 윈도우 활용**:
최근 10-100K 토큰으로 확장된 LLM 컨텍스트 윈도우를 시계열 예측과 결합하는 연구가 필요합니다.[1]

**2. 시계열 특화 파인튜닝**:
제로샷 성능을 넘어서 시계열 데이터에 대한 효과적인 파인튜닝 절차 개발이 중요한 연구 방향입니다.[1]

**3. 다변량 및 장기 예측**:
현재 각 변수를 독립적으로 예측하는 방식을 넘어서 다변량 관계를 고려한 통합적 접근법 개발이 필요합니다.[1]

**4. 산술 능력 향상**:
Chain of Thought 프롬프팅, 스크래치패드, 적응적 계산 등을 통해 복잡한 조합 연산 능력을 향상시키는 연구가 유망합니다.[1]

이 연구는 **LLM의 범용성을 시계열 도메인으로 확장**하여 전통적인 전문 모델의 필요성을 재고하게 하며, **계산 효율성과 접근성을 크게 향상**시킨 혁신적인 접근법으로 평가됩니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/3bd91717-726d-493d-8ba6-3578bec1da7b/2310.07820v3.pdf)
