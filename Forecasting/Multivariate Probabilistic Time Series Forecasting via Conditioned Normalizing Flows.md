# Multivariate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows

### 1. 핵심 주장과 주요 기여[1]

이 논문의 핵심 주장은 **조건부 정규화 흐름(Conditioned Normalizing Flows)과 자기회귀 모델을 결합하여 고차원 다변량 시계열 확률적 예측을 수행할 수 있다**는 것입니다. 주요 기여는 다음과 같습니다:[1]

**기술적 혁신**: 기존의 고정된 매개변수 분포(예: 가우시안)에 의존하던 다변량 시계열 모델의 한계를 극복하고, 정규화 흐름의 유연성을 활용하여 임의의 복잡한 고차원 분포를 모델링할 수 있습니다.[1]

**확장성**: 수천 개의 상호작용하는 시계열을 동시에 처리할 수 있으며, 이는 기존의 저차원 근사(예: 저랭크 가우시안 분포)가 필요하지 않습니다.[1]

**경험적 성과**: 6개의 실제 데이터셋에서 기존 최고 성능 방법들을 능가하며, CRPS(Continuous Ranked Probability Score) 메트릭에서 현저한 개선을 달성했습니다.[1]

***

### 2. 해결하는 문제 및 제안 방법

#### 문제의식[1]

기존 시계열 예측의 주요 문제점:

1. **다변량 의존성 무시**: 개별 시계열을 독립적으로 모델링하면 상호작용 효과를 포착할 수 없습니다. 예를 들어, 교통 네트워크에서 한 도로의 혼잡이 인접 도로에 파급됩니다.[1]

2. **분포 모델링의 한계**: 다변량 가우시안을 가정하면 공분산 행렬의 크기가 $$O(D^2)$$로 증가하여 계산 복잡도가 급증합니다.[1]

3. **고차 의존성 표현 부족**: 2차 통계량만으로는 복잡한 의존성을 충분히 표현할 수 없습니다.[1]

#### 제안 방법: 조건부 정규화 흐름[1]

##### 수학적 기반

**정규화 흐름의 기본 원리**: 정규화 흐름은 복잡한 분포 $$p_X(x)$$를 간단한 기본 분포 $$p_Z(z)$$ (예: 등방성 가우시안)로 변환하는 가역함수들의 합성입니다.[1]

변수 변환 공식을 통해:

$$
\log p_X(x) = \log p_Z(z) + \sum_{i=1}^{K} \log\left|\det\frac{\partial y^i}{\partial y^{i-1}}\right|
$$

여기서 $$z = f(x)$$이고, $$K$$는 흐름 레이어 수입니다.[1]

##### Real NVP 결합층[1]

Real NVP는 결합층(coupling layer)을 사용하여 차원을 부분적으로 변환하지 않고 유지합니다:

$$
y^{1:d} = x^{1:d}
$$

$$
y^{d+1:D} = x^{d+1:D} \odot \exp(s(x^{1:d})) + t(x^{1:d})
$$

여기서 $$s$$와 $$t$$는 신경망으로 매개변수화된 스케일 및 변환 함수입니다. 야코비안 행렬식이 블록 삼각형이므로:[1]

$$
\log\left|\det\frac{\partial y}{\partial x}\right| = \sum_j s_j(x^{1:d})
$$

계산이 $$O(D)$$ 시간에 수행됩니다.[1]

##### Masked Autoregressive Flow (MAF)[1]

Real NVP와 달리, MAF는 마스킹을 사용하여 순차적 계산을 피하면서 자기회귀 변환을 구현합니다:

```math
y_i = (x_i - \mu_i(x_{ < i})) / \sigma_i(x_{ < i})
```

여기서 야코비안은 하삼각 행렬이므로 행렬식이 대각선 원소의 곱이 됩니다.[1]

##### 시간적 조건화[1]

RNN 기반 모델:

$$
h_t = \text{RNN}(\text{concat}(x_{t-1}, c_{t-1}), h_{t-1})
$$

여기서 $$h_t$$는 RNN 은닉 상태이며, 정규화 흐름의 스케일/변환 함수를 조건화합니다.[1]

조건부 결합층:

$$
s = s(\text{concat}(x^{1:d}, h_t))
$$

$$
t = t(\text{concat}(x^{1:d}, h_t))
$$

**전체 확률 모델**: 자기회귀 분해를 통해

$$
p_\theta(x_{t_0:T}|x_{1:t_0-1}, c_{1:T}) = \prod_{t=t_0}^{T} p_\theta(x_t|h_t)
$$

로그 우도 최대화:

$$
\mathcal{L} = \frac{1}{|D|T} \sum_{x_{1:T} \in D} \sum_{t=1}^{T} \log p_\theta(x_t|h_t)
$$

를 Adam 최적화기로 수행합니다.[1]

#### Transformer 기반 조건화[1]

RNN의 순차적 계산 의존성을 제거하기 위해 인코더-디코더 Transformer 구조를 제안합니다:

- **인코더**: 문맥 윈도우 $$x_{1:t_0-1}$$를 임베딩
- **디코더**: 마스크된 멀티헤드 어텐션을 통해 미래 정보 누출을 방지하면서 예측 윈도우 $$x_{t_0:T}$$에 대한 조건화 생성

계산 복잡도가 RNN의 $$O(TD^2)$$에서 $$O(T^2D)$$로 개선되며, 대규모 다변량 시계열 ($$D >> T$$)에서 우수합니다.[1]

***

### 3. 모델 구조[1]

#### 전체 아키텍처[1]

**두 가지 주요 변형**:

1. **LSTM-Real-NVP / LSTM-MAF**: RNN(LSTM/GRU)을 시간 조건화기로 사용
2. **Transformer-MAF**: Transformer 인코더-디코더를 시간 조건화기로 사용

#### 구성 요소[1]

| 요소 | 설명 | 하이퍼파라미터 |
|------|------|--------------|
| 정규화 흐름 | Real NVP 또는 MAF | K=5개 결합층 |
| 신경망 성분 | 선형 피드포워드 레이어 | 은닉차원=100 |
| 활성화 함수 | ELU | - |
| Batch Normalization | 각 결합층 출력에 적용 | 이동 평균 정규화 |
| 배치 정규화 모드 | 훈련/추론 모드 분리 | - |

#### 학습 전략[1]

- **배치 크기**: 64
- **에포크당 배치 수**: 100
- **최대 에포크**: 40
- **학습률**: 1e-3 (Adam)
- **랜덤 시간 윈도우 샘플링**: 훈련 데이터 내에서 크기 $$T$$의 랜덤 윈도우 선택, 콜드 스타트 학습 가능
- **리스케일링**: 각 시계열을 훈련 윈도우 평균으로 정규화하여 스케일 차이 해결

#### 추론[1]

**샘플링 절차**:

1. RNN 또는 인코더를 통해 워밍업 시계열 $$x_{1:t_0-1}$$ 처리
2. 각 미래 시점 $$t$$에서:
   - 등방성 가우시안에서 노이즈 $$z_t \sim \mathcal{N}(0, I)$$ 샘플링
   - 역 흐름 통과: $$x_t = f^{-1}(z_t|h_t)$$
   - RNN/디코더 업데이트: $$h_{t+1} \gets \text{RNN}(\text{concat}(x_t, c_t), h_t)$$
3. 궤적 반복 샘플링으로 예측 분포 경험적 추정

***

### 4. 성능 향상[1]

#### 벤치마크 결과

6개 실제 데이터셋에서 CRPSsum (누적 연속 순위 확률 점수) 기준 평가:

| 데이터셋 | 차원 | 기존 최고 | LSTM-Real-NVP | LSTM-MAF | **Transformer-MAF** |
|---------|------|---------|----------------|----------|-------------------|
| **Exchange** | 8 | 0.0070 | 0.0064±0.003 | 0.0050±0.003 | **0.0050±0.003** |
| **Solar** | 137 | 0.3370 | 0.3310±0.02 | 0.3150±0.023 | **0.3010±0.014** |
| **Electricity** | 370 | 0.0220 | 0.0240±0.001 | 0.0208±0.000 | **0.0207±0.000** |
| **Traffic** | 963 | 0.0780 | 0.0780±0.001 | 0.0690±0.002 | **0.0560±0.001** |
| **Taxi** | 1,214 | 0.1830 | 0.1750±0.001 | 0.1610±0.002 | **0.1790±0.002** |
| **Wikipedia** | 2,000 | 0.0860 | 0.0780±0.001 | 0.0670±0.001 | **0.0630±0.003** |

**주요 개선점**:
- **모든 데이터셋에서 우수성**: Transformer-MAF가 6개 모두에서 최고 성능
- **고차원 데이터 강점**: Wikipedia (2,000차원)에서 26.7% 개선
- **Traffic 데이터 두드러짐**: 963차원 데이터에서 기존 대비 28.2% 개선[1]

#### 추가 평가 메트릭[1]

**CRPS (개별 시계열 평균)**:

$$
\text{CRPS}(F, x) = \int_{-\infty}^{\infty} (F(z) - \mathbb{I}_{x \leq z})^2 dz
$$

Transformer-MAF가 대부분 경쟁 방법들을 능가합니다.[1]

**평균 제곱 오차 (MSE)**:
- 점 예측 성능도 우수하며, N-BEATS 같은 현대 단변량 모델과 경쟁 가능[1]

#### 의존성 구조 학습[1]

**파이프 시뮬레이션 실험**:
- 5개 센서로 구성된 액체 흐름 시스템에서 시간적 및 공간적 의존성 평가
- 모델이 지면의 진실 공분산 구조를 정확히 포착
- 교차 공분산 행렬에서 예측된 샘플과 실제 데이터 간 작은 편차 (Figure 5)

**Traffic 데이터 교차 공분산 분석**:
- Transformer-MAF가 고도로 상관된 센서들 간 의존성을 추출
- 미래 시점으로의 외삽 시 공분산 구조 보존[1]

***

### 5. 일반화 성능 향상[1]

#### 일반화 메커니즘

1. **분포 유연성**: 정규화 흐름이 고정된 분포 가정 없이 데이터의 실제 분포 학습
   - 비가우시안 한계 분포 처리 가능
   - 고차 의존성 모델링

2. **마스크된 자기회귀 흐름의 우수성**: 
   - Real NVP 대비 밀도 모델링 성능 우수 (Table 1 참고)
   - MAF로 교체 시 모든 데이터셋에서 개선[1]

3. **Transformer의 병렬 학습**:
   - RNN의 순차적 계산 의존성 제거로 더 나은 기울기 전파
   - 시간적으로 먼 의존성 효과적 포착
   - 대규모 데이터셋 학습 가능 (V-100 GPU 활용)[1]

4. **공변량 임베딩**:
   - 범주형 특성을 임베딩하여 시간 정보 풍부하게 표현
   - 라그 특성(lag features) 활용으로 자기회귀 정보 강화
   - 시간 주기성(일일, 주간) 자동 학습[1]

5. **스케일 정규화**:
   - 각 시계열을 훈련 평균으로 정규화하여 스케일 차이 해결
   - 모델이 상대적 패턴에 집중하도록 유도
   - 경험적 성능 현저한 개선[1]

#### 약한 데이터셋에서의 안정성

**Taxi 데이터 (1,214차원)**:
- 기존 GP-Copula 방법: 0.208±0.183 (높은 표준편차)
- Transformer-MAF: 0.179±0.002 (안정적)
- 개수 데이터 처리를 위한 Dequantization 전략 적용[1]

***

### 6. 한계[1]

#### 주요 제한사항

1. **이산 순서형 데이터**:
   - 현재 방법은 연속 데이터 기반
   - Dequantization으로 계수 데이터 처리 가능하나, 순서형 데이터(sales data)는 미흡[1]

2. **유변 성능 vs 다변량 성능 트레이드오프**:
   - 다변량 모델이 추가 분산을 도입하여 때로 개별 시계열 예측 성능 저하
   - 크로스 시계열 상관관계 추정의 어려움
   - 논문은 LSTNet, N-BEATS와의 비교로 이 문제 완화 입증하나 여전히 도전적[1]

3. **계산 복잡도**:
   - 역 흐름 샘플링 시 순차적 계산 필요 (추론 속도 영향)
   - Transformer 인코더는 $$O(T^2D)$$ 복잡도로, 극도로 긴 시계열에서 메모리 문제 가능[1]

4. **데이터 요구사항**:
   - 고차원 공분산 구조 학습에 충분한 데이터 필요
   - Wikipedia(2,000차원)에서 792 시간 단계가 상대적으로 적음에도 성능 양호하지만, 더 희소한 데이터에서는 과적합 위험[1]

5. **Batch Normalization의 제약**:
   - 훈련/추론 모드 분리로 배포 시프트 위험
   - 온라인 학습 시나리오에서 이동 평균 업데이트 전략 필요[1]

#### 향후 연구 제시 문제

논문에서 명시적으로 언급한 한계:
- 더 표현력 높은 흐름(예: Flow Ho et al., 2019)의 통합
- 이산 범주형 데이터 처리 방법(Tran et al., 2019)
- 이산 순서형 데이터 모델링이 여전히 미해결 문제[1]

***

### 7. 일반화 성능 향상의 핵심 인사이트[1]

#### 왜 이 모델이 잘 일반화되는가?

1. **구조적 귀납편향**: 
   - 자기회귀 시간 모델링이 장기 예측에 강점
   - 정규화 흐름의 유연성으로 데이터 특이성 적응[1]

2. **충분한 표현력**:
   - Real NVP(K=5)에서 MAF로 업그레이드 시 모든 데이터셋 개선
   - 더 표현력 높은 흐름 선택으로 추가 개선 가능성[1]

3. **정규화 전략**:
   - 스케일 정규화, Batch Norm, Dropout(0.1)의 조합
   - 공변량 임베딩으로 시간 정보 풍부화[1]

4. **크로스 시계열 의존성 포착**:
   - 공분산 구조 외삽 실험이 의존성 학습 증명
   - 전체 결합 분포 모델링으로 고차 효과 포착[1]

---

### 8. 앞으로의 연구에 미치는 영향[1]

#### 학술적 기여

**패러다임 전환**:
- 고정 분포 가정 ($$\mathcal{N}, \text{NB}$$)에서 벗어나 유연한 흐름 기반 모델로 전환
- 자기회귀 + 정규화 흐름 결합의 효과성 입증

**스케일 가능성**:
- 수천 차원 시계열 처리 가능성 열어줌 (기존: 저랭크 근사 필요)
- 대규모 실무 응용 가능성 증대[1]

#### 산업 응용

1. **소매/물류**: 
   - 수천 개 SKU 간 수요 상호작용 모델링
   - 불확실성 정량화로 재고 최적화[1]

2. **에너지**:
   - 대규모 전력망 센서의 부하 예측
   - 신재생에너지 간헐성 불확실성 포착[1]

3. **교통 시스템**:
   - Traffic 데이터셋 성과 (963차원, 28% 개선)
   - 신호 제어 최적화에 활용 가능[1]

---

### 9. 연구 시 고려할 점[1]

#### 단기 연구 방향

1. **더 표현력 높은 흐름 통합**:
   - Flow^ (Ho et al., 2019)의 변분 Dequantization 활용
   - 추가 coupling layer 또는 attention 기반 조건화[1]

2. **이산 데이터 처리**:
   - 범주형 데이터 특화 흐름(Tran et al., 2019)
   - Dequantization 전략 개선[1]

3. **극도로 고차원 데이터**:
   - 차원 축약 기법 (PCA, VAE)과의 결합
   - 스파스 공분산 구조 학습[1]

#### 장기 연구 전망

1. **이질적 샘플링 빈도**:
   - 다양한 주기를 가진 시계열의 동시 모델링
   - 동적 샘플링 간격 처리[1]

2. **외삽 정보 통합**:
   - 미래 공변량 알려진 경우의 활용
   - 정책 시뮬레이션(what-if scenarios)[1]

3. **온라인 학습**:
   - 스트리밍 데이터 환경에서 모델 업데이트
   - Batch Norm의 온라인 변형[1]

4. **인과 추론**:
   - 의존성 구조에서 인과 관계 식별
   - 개입(intervention) 시나리오 분석[1]

5. **구조 발견**:
   - 그래프 신경망과의 결합으로 시계열 간 연결 발견
   - 클러스터 구조 자동 학습[1]

***

## 결론

이 논문은 **확률적 다변량 시계열 예측에 정규화 흐름의 유연성을 처음으로 체계적으로 도입**하는 중요한 기여를 합니다. 자기회귀 모델의 예측력과 정규화 흐름의 분포 표현력을 결합함으로써, 고차원 상호작용 시계열을 다루는 새로운 기준을 제시했습니다. 특히 MAF 기반 구조와 Transformer 조건화의 조합은 계산 효율성과 성능의 우수한 균형을 보여주며, 이 연구는 후속 시계열 생성 및 예측 연구에 강력한 토대를 마련했습니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/95650aef-069f-4b48-b137-53bc94d45d65/2002.06103v3.pdf)
