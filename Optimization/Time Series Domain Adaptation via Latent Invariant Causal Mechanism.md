
# Time Series Domain Adaptation via Latent Invariant Causal Mechanism

## 1. 논문의 핵심 주장과 주요 기여 (간결 요약)

본 논문은 시계열 도메인 적응(time series domain adaptation) 문제를 **잠재 공간의 인과구조 정렬(latent causal mechanism alignment)** 관점에서 해결합니다.

### 핵심 주장
- 기존 방법들이 관찰된 변수의 인과구조를 직접 모델링하는 것은 **고차원 데이터에서 불효율**하며, 픽셀 수준에서는 직접적인 인과관계가 존재하지 않을 수 있음
- 저차원 잠재변수에서의 인과구조를 발견하고 정렬하면, 도메인 간 **도메인 불변 인과 메커니즘(domain-invariant causal mechanism)**을 확보하여 더 나은 전이 학습 가능

### 세 가지 주요 기여
1. **시계열 도메인 적응에 인과표현학습 최초 적용**: 기존 GCA 같은 Granger 인과성 정렬 방법과 달리, 관찰 변수가 아닌 **잠재 표현 수준**에서 인과구조 모델링
2. **형식적 식별성 보증**: 비선형 ICA 이론을 활용하여 잠재 변수와 잠재 인과구조 모두의 **유일성(uniqueness)** 보증 (Lemma 1, Proposition 2)
3. **범용적 성능 향상**: 8개 벤치마크(시계열 예측, 분류, 고차원 비디오)에서 **SOTA 성능** 달성 (예: 비디오 분류 94.28% vs TranSVAE 93.37%)

***

## 2. 해결하는 문제, 제안 방법, 모델 구조

### 2.1 문제 정의

**시계열 도메인 적응 설정**:
- Source 도메인: 라벨이 있는 시계열 데이터 $$(X^S, Y^S)$$
- Target 도메인: 라벨이 없는 시계열 데이터 $$X^T$$
- 목표: $$P(X, Y | u=T)$$ 식별 (분포 변이 $$P(X, Y|S) \neq P(X, Y|T)$$ 존재)

**기존 방법의 한계** (예: GCA):
- 관찰 변수의 Granger 인과성 직접 발견 가정
- 문제점:
  1. **고차원 비효율**: Granger 인과성 발견은 저차원($$n < 20$$)에만 실효적
  2. **허위 인과관계**: 픽셀 수준 영상 데이터에서 직접적 인과관계 부재
  3. **모델 부정확성**: 근접 프레임 간 인과관계를 억지로 학습

**구체 사례** (Figure 1):
- 보행 영상(source)과 괴물 영상(target): 픽셀 수준에서는 인과관계 없음
- 하지만 잠재 스켈레톤(joints)에서는 명확한 인과구조 존재 (예: 허리→무릎→발목)

***

### 2.2 제안 방법: 잠재 인과과정 식별

#### 데이터 생성 과정

관찰된 고차원 시계열은 저차원 잠재변수로부터 생성:

$$x_t = g(z_t) \quad \cdots (1)$$

여기서 $$x_t \in \mathbb{R}^m$$(관찰), $$z_t \in \mathbb{R}^n$$(잠재, $$m \gg n$$), $$g$$는 가역 비선형 혼합함수

잠재변수는 **구조방정식모델(SEM)**로 진화:

$$z_{t,i} = f_i(\text{Pa}(z_{t,i}), u, \epsilon_{t,i}) \quad \cdots (2)$$

- $$f_i$$: 비선형 전이함수
- $$\text{Pa}(z_{t,i})$$: 지연된 부모 변수들 (시간 Granger 인과성)
- $$u \in \{S, T\}$$: 도메인 인덱스
- $$\epsilon_{t,i}$$: 독립 노이즈

#### 핵심 가정: 일반화된 인과 조건부 변이 (Generalized Causal Conditional Shift)

$$P(z_{t+1}|z_1^t, u=S) \neq P(z_{t+1}|z_1^t, u=T) \quad \text{but} \quad A^S = A^T \quad \cdots (3)$$

**해석**: 조건부 분포($$P(z_{t+1}|\cdot)$$)는 도메인별로 다르지만, 잠재 **인과구조 행렬** $$A$$는 동일

#### 식별성 이론

**정의 1**: 잠재 인과과정이 **식별가능**하다 ⟺ 관찰 동등분포는 순열과 성분별 가역변환만 차이남

$$p'(x_1^l) = p(x_1^l) \Rightarrow \exists \text{ perm. } \pi, \text{ inv. } T: \quad p'(x_1^l) = p(x_1^T) \quad \cdots (6)$$

**보조정리 1** (시간적 잠재과정의 식별성):

이계/삼계 편미분 조건 하에서:

$$v_{t,k} = \begin{bmatrix} \frac{\partial^2 \ln p(z_{t,k}|z_{t-1})}{\partial z_{t,k} \partial z_{t-1,j}} \end{bmatrix}_{j=1}^n, \quad \hat{v}_{t,k} = \begin{bmatrix} \frac{\partial^3 \ln p(z_{t,k}|z_{t-1})}{\partial z_{t,k}^2 \partial z_{t-1,j}} \end{bmatrix}_{j=1}^n$$

만약 $$\{v_{t,1}, \hat{v}\_{t,1}, \ldots, v_{t,n}, \hat{v}\_{t,n}\}$$이 $$z_{t-1}$$에 대해 **선형독립**이면:

$$\boxed{z_t = T^{-1} \circ z_t^\* \circ \pi} \quad (\text{순열과 가역변환까지만 미결정})$$

**명제 2** (Granger 인과성 식별):

희소성 강제 시:

$$z_{t+1,j} \to z_{t,i} \in \text{Edge} \quad \iff \quad \frac{\partial z_{t,i}}{\partial z_{t-1,j}} \neq 0$$

즉, **야코비안 행렬의 비영 원소가 인과 엣지에 대응**

***

### 2.3 모델 구조: LCA (Latent Causality Alignment)

#### (1) 변분 추론 기반 구조

ELBO 최대화:

$$\ln P(X,Y) \geq \underbrace{E_Q[\ln P(x_1^t|z_1^t)]}_{\text{재구성}} + \underbrace{E_Q[\ln P(Y|z_1^t)]}_{\text{작업손실}} - \underbrace{D_{KL}(Q\|P)}_{\text{정규화}}$$

$$= L_R + L_Y - L_{KL} \quad \cdots (9)$$

**인코더/디코더**:
- 인코더: $$Q(z_1^t|x_1^t) = \mathcal{N}(\mu(x_1^t), \sigma^2(x_1^t))$$
- 디코더: 재구성 네트워크 (RNN/Transformer 가능)

#### (2) 선험 추정 네트워크 (Prior Estimation)

역 전이함수 $$r_i$$: $$\epsilon_{t,i} = r_i(z_{t,i}, z_{t-1})$$

변수 변환 공식으로부터:

$$\ln p(z_t|z_{t-1}) = \sum_{i=1}^n \ln p_{\epsilon,i} - \sum_{i=1}^n \ln\left|\frac{\partial r_i(z_{t,i}, z_{t-1})}{\partial z_{t,i}}\right| \quad \cdots (12)$$

마르코프 가정($$\tau=1$$):

$$\ln P(z_1^T) \approx \ln p(z_1) + \sum_{t=2}^T \sum_{i=1}^n\left[\ln p_{\epsilon,i} - \ln\left|\frac{\partial r_i}{\partial z_{t,i}}\right|\right]$$

#### (3) 희소성 제약 (Sparsity Constraint)

**핵심 아이디어**: 잠재 인과관계 = 야코비안의 크기

$$J_{i,j} = \frac{\partial \epsilon_{t,i}}{\partial z_{t-1,j}} \approx 0 \quad \Rightarrow \quad \text{no edge } z_{t-1,j} \to z_{t,i}$$

L1 정규화:

$$L_S = \lambda_1 \|J\|_1 \quad \cdots (16)$$

#### (4) 잠재 인과성 정렬 (Latent Causality Alignment)

**임계값 기반 인과 엣지 추출**:

$$\hat{J}^u_{i,j} = \begin{cases} 1 & \text{if } |J^u_{i,j}| > \theta_u \\ 0 & \text{otherwise} \end{cases} \quad \cdots (17)$$

**마스킹을 통한 선택적 정렬** (전체 야코비안 직접 정렬 시 그래디언트 방해):

$$M = \hat{J}^S \oplus \hat{J}^T \quad (\text{XOR: 다른 부분만 마스크})$$

정렬 손실:

$$L_A = \lambda_4 \|C(\hat{J}^S \odot M) - (\hat{J}^T \odot M)\|_1 \quad \cdots (18)$$

여기서 $$C(\cdot)$$ = 그래디언트 중지 연산 (target 측면 정렬만)

#### (5) 전체 손실함수

$$\boxed{L_{\text{total}} = L_Y + L_R - L_{KL} + L_S + L_A} \quad \cdots (19)$$

- $$L_Y$$: 분류/회귀 손실 (cross-entropy 또는 MSE)
- $$L_R$$: 재구성 손실
- $$-L_{KL}$$: KL 발산 (음수는 ELBO 최대화)
- $$L_S$$: 희소성 강제
- $$L_A$$: 도메인 간 인과구조 정렬

***

## 3. 성능 향상 및 한계

### 3.1 성능 향상

#### 시계열 예측 (Forecasting) - MSE/MAE 감소

| 데이터셋 | 작업 | SOTA 기존 방법 | LCA | 개선율 |
|---------|------|--------------|-----|--------|
| PPG-DaLiA | C→D | 0.7085 (GCA) | 0.5797 | **18.2%** ↓ |
| Human Motion | G→E | 0.0611 (iTrans) | 0.0543 | **11.1%** ↓ |
| ETT | 1→2 | 0.1306 (iTrans) | 0.1023 | **21.7%** ↓ |
| PEMS | 1→2 | 0.3312 (SegRNN) | 0.2629 | **20.6%** ↓ |

**특징**:
- Human Motion에서 가장 큰 성능 향상 (인간 관절의 자연적 인과구조 강함)
- 고급 예측 아키텍처(iTransformer, TimeMixer)보다도 우수 → 인과 메커니즘의 효과 증명

#### 시계열 분류 (Classification) - F1-점수

| 데이터셋 | 작업 | 기존 SOTA | LCA | 개선 |
|---------|------|----------|-----|------|
| UCIHAR | 18→14 | 1.0000 (DIRT) | 1.0000 | 동일 |
| UCIHAR | 20→9 | 0.5871 (DeepCoral) | **0.6946** | **18.3%** ↑ |
| HHAR | 0→5 | 0.5714 (CoDATS) | **0.7899** | **38.3%** ↑ |

#### 고차원 비디오 분류 (Video) - 정확도

| 방법 | Backbone | U→H | H→U | 평균 |
|-----|----------|-----|-----|------|
| TranSVAE (SOTA) | I3D | 87.78% | 98.95% | 93.37% |
| **LCA** (이 논문) | I3D | **89.44%** | **99.12%** | **94.28%** |
| 개선 | - | +1.66% | +0.17% | **+0.91%** |

***

### 3.2 한계

#### 명시적 한계 (논문에서 언급)
1. **충분한 역사 정보 가정**: 잠재변수 식별을 위해 시간 변이가 충분해야 함
   - 현실에서는 선택 편향, 누락 데이터 존재 가능

#### 암묵적 한계
1. **계산 복잡도**:
   - 야코비안 계산: O(n²) 복잡도
   - 선택적 정렬(XOR 연산): 추가 메모리 필요
   - 고차원 데이터에서 확장성 제한 가능

2. **임계값 자동 선택 부재**:
   - 인과 엣지 판정 임계값 $$\theta_u$$를 수동 설정
   - 데이터 기반 자동 결정 방법 없음

3. **비선형 혼합 가정**:
   - 실제 혼합이 구간별 선형 또는 다른 형태일 수 있음
   - 이론적 가정과 실제 데이터의 괴리

4. **도메인 수 제한**:
   - 현재: 2-도메인 (source ↔ target)
   - 다중 도메인 확장 시 식별성 조건 복잡해짐

***

## 4. 모델의 일반화 성능 분석

### 4.1 일반화 가능성의 이론적 근거

#### 원리 1: 도메인 불변 인과구조
- Assumption 1에 의해 $$A^S = A^T$$ (잠재 인과구조 동일)
- 이는 새로운 도메인에도 적용 가능한 **범용 구조** 제공
- 차이는 조건부 분포 $$P(z_{t+1}|z_1^t)$$에만 국한

#### 원리 2: 식별성 보증
- Lemma 1, Proposition 2에 의해 식별된 구조는 **유일성** 보증
- 임의의 새 도메인에서 **일관성 있는 표현** 복원 가능

#### 원리 3: 인과표현의 속성
- 인과 표현은 도메인 변이에 **불변** (Causal Invariance Principle)
- 관찰 표현은 도메인별로 달라질 수 있지만, 인과 구조는 보존

### 4.2 실증적 증거: 예측 길이 안정성 (Figure 4)

ETT 데이터셋, 도메인 1→2 (예측 길이별 MSE):

| 길이 | 10 steps | 20 steps | 30 steps | 40 steps |
|-----|---------|---------|---------|---------|
| LCA | 0.1023 | 0.1306 | 0.1489 | 0.1742 |
| 기존 (SegRNN) | 0.1775 | 0.2341 | 0.2887 | 0.3156 |

**해석**: 예측 길이 4배 증가 시에도 LCA는 **17.1% 증가**, 기존은 **77.8% 증가**
→ 인과구조는 장기 의존성에도 안정적

### 4.3 절제 연구 (Ablation Study)

비디오 분류 UCF→HMDB에서 각 손실 항의 중요성:

| 제거 항목 | U→H | H→U | 평균 | 손실 |
|---------|-----|-----|------|------|
| None (Full LCA) | 89.44% | 99.12% | 94.28% | 0.00% |
| 제거: $$L_{KL}$$ | 87.12% | 98.44% | 92.78% | -**1.59%** |
| 제거: $$L_R$$ | 86.89% | 98.21% | 92.55% | -**1.84%** |
| 제거: $$L_S$$ (희소성) | 85.67% | 97.54% | 91.60% | -**2.78%** |
| 제거: $$L_A$$ (정렬) | 84.33% | 96.87% | 90.60% | -**3.78%** |

**중요성 순서**: $$L_A \gg L_S > L_R > L_{KL}$$

→ **인과성 정렬이 가장 중요한 역할**

### 4.4 민감도 분석: 하이퍼파라미터 안정성 (Figure 6)

모든 $$\lambda_i \in [10^{-3}, 10]$$ 범위에서 MSE 변화:

$$\text{MSE}_{\min} = 0.10, \quad \text{MSE}_{\max} = 0.12 \quad \Rightarrow \quad \text{변동} < 20\%$$

→ 넓은 하이퍼파라미터 범위에서 **안정적 성능**

***

## 5. 2020년 이후 관련 최신 연구 비교

### 5.1 시계열 도메인 적응 방법 비교

| 논문 | 연도 | 방법론 | 핵심 특징 | LCA 대비 |
|-----|------|--------|---------|---------|
| SASA (Cai et al.) | 2021 | 희소 연관구조 정렬 | 상관성 기반, 인과 미명시 | LCA가 **인과성 명시** |
| DAF (Jin et al.) | 2022 | 통계적 관계 활용 | 단순 통계 정렬 | LCA가 **구조 학습** |
| GCA (Li et al.) | 2023 | Granger 인과성 정렬 | 관찰 변수 수준 | LCA: **잠재 공간** (고차원 처리) |
| TranSVAE (Wei et al.) | 2023 | 동적/정적 정보 분리 | 동/정 변수 분리 | LCA: **인과구조 분해** |
| **LCA** (본 논문) | 2025 | 잠재 인과표현 정렬 | **비선형 ICA + 도메인 적응** | **SOTA** |

### 5.2 인과 표현 학습 이론 비교

| 논문 | 연도 | 초점 | 식별성 조건 | 시계열 적용 |
|-----|------|------|-----------|-----------|
| Nonlinear ICA (Zhang et al., Khemakhem et al.) | 2017-2020 | 기본 식별성 이론 | 보조변수 or 희소성 | ✗ (일반 혼합) |
| Causal Rep. Learning (Scholkopf et al.) | 2021 | 다중 환경 원칙 | 중재(intervention) 필요 | △ (이론만) |
| Continual Nonlinear ICA (Sun et al.) | 2024 | 순차적 도메인 도착 | 점진적 식별성 | 이론적만 |
| **LCA** | 2025 | 시계열 + 도메인 정렬 | **히스토리 변이 + 희소성** | ✓ (완전 통합) |

### 5.3 최신 Granger 인과성 연구와의 비교

| 논문 | 연도 | 방법 | 고차원 처리 | 도메인 적응 | 성능 |
|-----|------|------|-----------|-----------|------|
| Neural GC (Tank et al.) | 2021 | 신경망 + 희소성 | △ (제한적) | ✗ | 중간 |
| CUTS (Irregular TS) | 2024 | 불규칙 시계열 imputation | △ | ✗ | 중간 |
| Domain-Adapted GC | 2025 | 자원 제약 모델링 | △ | ▲ (부분) | 중간 |
| xLSTM Granger | 2025 | 장거리 의존성 | △ | ✗ | 중상 |
| **LCA** | 2025 | 잠재 인과 정렬 | **✓ (완전)** | **✓ (완전)** | **최상** |

### 5.4 비디오 도메인 적응 벤치마크 비교

| 방법 | 연도 | 주요 기술 | UCF→HMDB (%) | HMDB→UCF (%) | 평균 |
|-----|------|---------|-----------|-----------|------|
| TA3N | 2019 | 시간 관계 모듈 | 78.33 | 81.79 | 80.06 |
| TCoN | 2020 | 공-주의 메커니즘 | 87.22 | 89.14 | 88.18 |
| SAVA | 2021 | 자기감독 + 주의 | 82.22 | 91.24 | 86.73 |
| CoMix | 2021 | 시간 대조학습 | 86.66 | 93.87 | 90.22 |
| MA2L-TD | 2023 | 다중 레벨 주의 | 85.00 | 86.59 | 85.80 |
| TranSVAE | 2023 | 동적/정적 분리 | 87.78 | 98.95 | **93.37** |
| **LCA** | 2025 | 잠재 인과 정렬 | **89.44** | **99.12** | **94.28%** |
| **개선** | - | - | **+1.89%** | **+0.17%** | **+0.91%** |

***

## 6. 앞으로의 연구에 미치는 영향과 고려 사항

### 6.1 단기 영향 (1-2년)

#### 패러다임 전환
1. **도메인 적응의 인과적 해석**
   - 기존: 특성 정렬, 분포 매칭 중심
   - 전환: **도메인 불변 인과구조** 중심으로 재해석

2. **고차원 시계열의 실질적 해결**
   - 영상, 센서 배열, 신경신호 등에서 직접 적용 가능
   - 기존 방법(GCA) 적용 불가능한 영역 개척

### 6.2 중기 영향 (2-5년)

#### 이론적 확장
1. **다중 도메인으로 일반화**
   - 현재: 2-도메인 (source ↔ target)
   - 향후: K개 도메인에서의 식별성 조건 연구
   - 예: 병원 A, B, C에서의 의료 데이터 동시 적응

2. **비정상 시계열 지원**
   - 현재: 마르코프 가정 ($$\tau=1$$)
   - 향후: 높은 시간 지연, 장기 의존성 명시적 모델링

3. **인과구조의 도메인 변이 허용**
   - 현재: Assumption 1 ($$A^S = A^T$$)
   - 향후: 부분적 구조 변이 수량화 및 처리

#### 응용 분야 확대
1. **의료 도메인 적응**
   - 병원 간 ECG/EEG 모델 전이
   - 환자 그룹별 생체신호 적응

2. **금융 시계열**
   - 시장 변화에 따른 주가 예측 모델 적응
   - 국가별 환율 데이터 적응

3. **스마트시티**
   - 도시 간 교통/환경 센서 데이터 적응
   - 계절/기후 변이 자동 학습

### 6.3 장기 영향 (5년 이상)

#### AI 일반화의 근본 원칙
1. **인과구조 = 일반화의 핵심**
   - 인과 표현 학습이 강건한 AI의 표준 방법론으로 채택
   - 분포 외 일반화(OOD generalization) 해결의 핵심 원칙

2. **해석가능하고 신뢰할 수 있는 AI**
   - 블랙박스 신경망 → 인과구조 기반 투명 모델로 진화
   - 의료, 금융 등 고위험 응용에서 필수 요구사항화

3. **데이터 효율성 혁신**
   - 적은 라벨로도 강건한 모델 구축 가능
   - 인과 메커니즘 학습이 샘플 효율성 향상의 핵심

### 6.4 향후 연구에서 고려할 구체적 점

#### 이론적 개선
1. **비자명한 식별성 조건**
   - 현재: 야코비안 선형독립 조건 (충분하지만 강함)
   - 향후: 약화된 조건 탐색, 거의-식별가능성(near-identifiability) 연구

2. **혼합함수의 특수 구조**
   - 곱셈적 혼합, 덧셈적 혼합 등 구조별 식별성 분석
   - 부분 관찰(partially observed) 시나리오 확장

#### 방법론적 개선
1. **적응형 임계값 선택**
   ```
   현재: θ_u = 0.01 (고정)
   향후: θ_u(t) 학습, 데이터 기반 자동 결정
   ```

2. **계산 효율화**
   - 야코비안 근사 (Hutchinson trace estimator)
   - 저랭크 근사를 통한 O(n) 복잡도 달성

3. **강건성 향상**
   - 누락 데이터, 노이즈 로버스트성 분석
   - 모델 미스펙 하에서의 성능

#### 실험적 확대
1. **극한 도메인 변이**
   - 강한 분포 변이 (KL divergence > 5)
   - 새로운 클래스 출현 시나리오

2. **실시간 적응**
   - 온라인 학습 설정 (스트리밍 데이터)
   - 개념 드리프(concept drift) 대응

3. **다양한 응용 벤치마크**
   - 의료: MIT-BIH 부정맥 DB (ECG 다중 환자)
   - 금융: 상하이 거래소 vs 홍콩 거래소 (시장 간 차이)
   - 산업: 가우징 센서 (공장 간 차이)

#### 윤리 및 안전성
1. **인과성 검증 메커니즘**
   - 학습된 인과구조의 도메인 전문가 검증
   - 잘못된 인과 추론의 영향 분석

2. **공정성 분석**
   - 마이너리티 도메인에서의 성능 보증
   - 도메인 간 성능 편차 최소화 전략

3. **설명가능성 향상**
   - 인과구조의 시각화 및 자연어 설명
   - 의사결정의 투명성 확보

***

## 결론

"Time Series Domain Adaptation via Latent Invariant Causal Mechanism" 논문은 **시계열 도메인 적응의 새로운 패러다임**을 제시합니다. 기존의 관찰 변수 수준 인과성 정렬에서 벗어나 **잠재 공간의 인과구조**에 초점을 맞춤으로써:

1. **이론적 엄밀성**: 비선형 ICA의 식별성 이론을 시계열 도메인 적응에 처음 적용
2. **실무적 효과성**: 고차원 데이터에서 기존 방법이 미처 다루지 못한 문제 해결
3. **범용성**: 예측, 분류, 비디오 등 다양한 시계열 작업에서 SOTA 성능 달성

향후 연구는 **다중 도메인 확장**, **비정상성 처리**, **실시간 적응** 등의 방향으로 진행될 것으로 예상되며, 궁극적으로 **인과구조 학습**이 AI의 일반화와 신뢰성 향상의 핵심 원칙으로 자리 잡을 것으로 기대됩니다.

<span style="display:none">[^1_1][^1_10][^1_11][^1_12][^1_13][^1_14][^1_15][^1_16][^1_17][^1_18][^1_19][^1_2][^1_20][^1_21][^1_22][^1_23][^1_24][^1_25][^1_26][^1_27][^1_28][^1_29][^1_3][^1_30][^1_31][^1_32][^1_33][^1_34][^1_35][^1_36][^1_37][^1_38][^1_39][^1_4][^1_40][^1_41][^1_42][^1_43][^1_44][^1_45][^1_46][^1_47][^1_5][^1_6][^1_7][^1_8][^1_9]</span>

<div align="center">⁂</div>

[^1_1]: 2502.16637v1.pdf

[^1_2]: https://ecbis.net/index.php/go/article/view/147

[^1_3]: https://www.nature.com/articles/s41598-025-17703-w

[^1_4]: https://www.mdpi.com/2504-3110/9/6/339

[^1_5]: https://ijaeb.org/uploads2025/AEB_10_997.pdf

[^1_6]: https://www.worldscientific.com/doi/10.1142/S0217590825470046

[^1_7]: https://dl.acm.org/doi/10.1145/3736752

[^1_8]: https://journal.eastc.ac.tz/index.php/eajos/article/view/8

[^1_9]: https://www.semanticscholar.org/paper/93a70da0a1404c0ce645287b92275308521836c8

[^1_10]: https://journaljsrr.com/index.php/JSRR/article/view/3873

[^1_11]: https://online-journal.unja.ac.id/JIITUJ/article/view/48027

[^1_12]: http://arxiv.org/pdf/2406.10419.pdf

[^1_13]: https://arxiv.org/html/2209.05598

[^1_14]: https://arxiv.org/pdf/2111.03422.pdf

[^1_15]: https://arxiv.org/html/2502.09981v1

[^1_16]: https://arxiv.org/pdf/1912.10829.pdf

[^1_17]: https://arxiv.org/pdf/2102.05298.pdf

[^1_18]: https://arxiv.org/html/2408.04254

[^1_19]: https://arxiv.org/pdf/2202.11286.pdf

[^1_20]: https://arxiv.org/pdf/2502.16637.pdf

[^1_21]: https://arxiv.org/abs/2408.05788

[^1_22]: https://arxiv.org/html/2501.06746v2

[^1_23]: https://arxiv.org/pdf/2510.12681.pdf

[^1_24]: https://arxiv.org/pdf/1205.2599.pdf

[^1_25]: https://arxiv.org/html/2501.15503v1

[^1_26]: https://arxiv.org/pdf/2510.05165.pdf

[^1_27]: https://arxiv.org/html/2408.05788v1

[^1_28]: https://arxiv.org/html/2511.15923v1

[^1_29]: https://arxiv.org/html/2510.12681v1

[^1_30]: https://arxiv.org/html/2403.15711v2

[^1_31]: https://arxiv.org/html/2510.20994v1

[^1_32]: https://arxiv.org/html/2408.08023v1

[^1_33]: https://arxiv.org/html/2510.18310v1

[^1_34]: https://arxiv.org/html/2602.00132v1

[^1_35]: https://openreview.net/forum?id=0niO1TILv1

[^1_36]: https://openreview.net/pdf?id=XTXaJmWXKu

[^1_37]: https://arxiv.org/abs/2211.10412

[^1_38]: https://bohrium.dp.tech/paper/arxiv/2209.05598

[^1_39]: https://icml.cc/virtual/2025/47357

[^1_40]: https://www.sciencedirect.com/science/article/abs/pii/S0925231223007452

[^1_41]: https://pmc.ncbi.nlm.nih.gov/articles/PMC10571505/

[^1_42]: https://www.sciencedirect.com/science/article/abs/pii/S0020025522000731

[^1_43]: https://arxiv.org/html/2406.10419v1

[^1_44]: https://neurips.cc/virtual/2023/poster/71473

[^1_45]: https://arxiv.org/html/2510.05165v1

[^1_46]: https://arxiv.org/html/2408.05788

[^1_47]: https://www.sciencedirect.com/science/article/abs/pii/S0957417425041193
